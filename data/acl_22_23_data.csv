idx,ethical_statement,annotation_1,annotation_2,annotation_3,year,what/scope
1,"LaPraDoR is trained with webcrawled data, which may contain inappropriate content. However, due to the nature of text retrieval,
our retriever has lower ethical risk compared to a
generative auto-regressive language model (Bender et al., 2021). Meanwhile, our unsupervised
retrieval model enables high-performance text retrieval for low-resource languages where there is
no supervised query-document dataset. This contributes to equality and diversity of language technology.
Carbon Footprint To conduct all experiments
in this paper, we estimate to have consumed 3,840
kWh of electricity and emitted 1,420.8 kg (3,132.3
lbs) of CO2. All emitted carbon dioxide has already
been offset by the cloud service provider.","inappropriate content, environmental impact","inappropriate content, al impact","inappropriate content, environmental impact",2022,"actions, concerns, statement"
2,"In this paper, different ethical restrictions deserve
discussion.
The datasets we created are derived from large
dialogue corpus that publicly available on the Inter-
net, and we strictly followed the platform’s policies
and rules when obtaining data from web platforms.
We did not use any author-specific information in
our research.
Online large dialogue corpus may includes some
bias, such as political bias and social bias, and our
model might have inherited some forms of these
bias. In order to limit these bias as much as pos-
sible, we filter controversial articles and removed
data with offensive information when possible.
",bias,bias,bias,2022,"actions, concerns"
3,"In this paper, different ethical restrictions deserve
discussion.
All data used in our pre-training are available
online and other dialog corpus in this paper are
publicly available sources. We strictly followed
the platform’s policies and rules when crawling
data from web platforms. We did not employ any
author-specific information in our research.
Our corpus may includes some bias, such as po-
litical bias and social bias, and our model might
have inherited some forms of these bias. In order to
limit these bias as much as possible, we filter con-
troversial articles and removed data with offensive
information when possible.
",bias,bias,bias,2022,"actions, concerns"
4,"This paper and the data used within was reviewed
as part of Twitter’s standard privacy and legal re-
view processes. No data has been publicly released
in relation to this paper. While there is a possibility
that the model could be misused, we do not antici-
pate any new or increased risks over those already
present in established prior work and prior models
on topic classification.
",misuse,misuse,misuse,2022,"statement, concerns"
5,"Large models that are pre-trained on heterogeneous
data can be potentially harmful to marginalized
populations. Along with the improved controlla-
bility, we also recognize that our system might be
misused to create offensive or fabricated content.
We therefore advocate cautious usage in real-world
deployment.
","misuse, fabricated content, offensive content","misuse, fabricated content, offensive content","misuse, fabricated content, offensive content",2022,"concerns, statement"
6,"We recognize that our method may generate fabri-
cated and potentially harmful contents due to the
systematic biases of pre-training using heteroge-
neous web corpora and the open-ended genera-
tion characteristics of the opinion generation tasks.
Therefore, we urge the users to carefully examine
the ethical influence of the generated outputs and
cautiously apply the system in real-world applica-
tions.
","fabricated content, harmful content, ","fabricated content, harmful content, ","fabricated content, harmful content, ",2022,"concern, statement, suggestions"
7,"In this paper we considered the relatively underex-
plored aspect of social biases in pretrained sense
embeddings. We created a new dataset for this pur-
pose, which we name the Sense-Sensitive Social
Bias (SSSB) dataset. The dataset we create is of
a sensitive nature. We have included various sen-
tences that express stereotypical biases associated
with different senses of words in this dataset. We
specifically considered three types of social biases
in SSSB: (a) racial biases associated with a nation-
ality as opposed to a language (e.g. Chinese people
are cunning ,Chinese language is difficult , etc.),
(b) racial biases associated with the word black as
opposed to its sense as a colour (e.g. Black people
are arrogant ,Black dress is beautiful , etc.) and (c)
gender-related biases associated with occupations
used as nouns as opposed to verbs (e.g. She was a
careless nurse ,He was not able to nurse the crying
baby , etc.). As seen from the above-mentioned ex-
amples, by design, SSSB contains many offensive,
stereotypical examples. It is intended to facilitate
evaluation of social biases in sense embeddings
and is publicly released for this purpose only. We
argue that SSSB should not be used to train sense
embeddings. The motivation behind creating SSSB
is to measure social biases so that we can make
more progress towards debiasing them in the fu-
ture. However, training on this data would defeat
this purpose.
It is impossible to cover all types of social biases
related to word senses in any single dataset. For
example, the stereotypical association of a disad-
vantaged group with a positive attribute (e.g. All
Chinese students are good at studying ) can also
raise unfairly high expectations for the members in
that group and cause pressure to hold upto those
stereotypes. Such positive biases are not well cov-
ered by any of the existing bias evaluation datasets,
including the one we annotate in this work.
Given that our dataset is generated from a hand-
ful of manually written templates, it is far from
complete. Moreover, the templates reflect the cul-
tural and social norms of the annotators from a
US-centric viewpoint. Therefore, SSSB should not
be considered as an ultimate test for biases in sense
embeddings. Simply because a sense embedding
does not show any social biases on SSSB according
to the evaluation metrics we use in this paper does
notmean that it would be appropriate to deploy it
in downstream NLP applications that require sense
embeddings. In particular, task-specific fine-tuning
of even bias-free embeddings can result in novel
unfair biases from creeping in.
Last but not least we state that the study con-
ducted in this paper has been limited to the English
language and represent social norms held by the
annotators. Moreover, our gender-bias evaluation
is limited to binary (male vs. female) genders andracial-bias evaluation is limited to Black as a race.
Extending the categories will be important and nec-
essary future research directions.",bias,bias,bias,2022,"statement, concerns"
8,"In this work, we aim to develop models which can
more accurately predict the emotions elicited from
text statements. Although our goal is to identify
potentially harmful statements in order to avoid
them , it is important to consider potential negative
use-cases for such work. A system which can iden-
tify offensive statements can also select for them,
and it may be possible to use such a system to tar-
get users, attacking them on topics or attributes
which they are most sensitive about. To the extent
that we are able, we must be cautious not to aid in
the development of such systems in the process of
furthering research for more empathetic dialogue
systems.
We tailor our study in three ways in an effort to
reduce the risk of harm. First, we focus primar-
ily on identifying implicitly offensive statements.
While a system which produces implicitly offen-
sive statements may still be used to attack users,
they are significantly more challenging to generate
when compared to explicitly offensive statements,
which do not require any additional inferences or
world knowledge. We hypothesize that this makes
implicitly offensive statements unlikely to be uti-lized in offensive systems. Second, our dataset size
is chosen with the goal of being large enough to
support evaluation but not training. It can therefore
function as a useful diagnostic of offensive text
detection systems, with limited risk of being used
to create one.
Third, in our dataset, we have removed protected
attributes such as ethnicity, gender, and race.
","misuse, offensiveness","misuse, offensiveness","misuse, offensiveness",2022,"statement, actions, suggestions"
9,"Intellectual Properties and Privacy Rights All
of the datasets (CNN/DM and XSum) used in our
study are publicly available. Regarding privacy
rights, the authors of the paper completed IRB hu-
man subject protection training for conducting this
study. We will release the annotations, but rather
than releasing the MTurk ID of the worker, we will
completely anonymize this ID.
Compensation for Annotators Workers were
compensated $5 per block, calibrated to equal a
$15/hour payrate. We first annotated examples in-
house to determine the required annotation speed.
A summary block usually takes around 20 minutes.
Steps Taken to Avoid Potential Problems An-
notations were completed in the form of a survey
on a Google Form. We provided space for the Turk-
ers to provide feedback. We manually uploaded the
data points (articles and summaries) used in this
study to avoid any offensive content.
The Number of Examples We sampled 100 ex-
amples from each dataset that did not contain ex-
actly matching summaries. Both Likert and BWS
follow the same block design, which includes the
same number of examples per block. With the ex-
ception that the BWS annotation asks for the most
and least factually consistent summary and the Lik-
ert asks for ratings for each individual summary.
Due to space requirements, we included further de-
tails, images of the interface, in the supplementary
material. We pay the same amount per block of
annotations.
Qualifications of MTurk workers We use the
following qualifications to recruit in total 350
MTurk workers with good track records: HIT ap-
proval rate greater than or equal to 98%, num-
ber of HITs approved greater than or equal to
500, and located in one of the following English
native-speaking countries: Australia, Canada, New
Zealand, United Kingdom, United States.
",no ethical concerns,No_ethical_concerns,no ethical concerns,2022,actions
10,"Human Evaluation We recruited seven volun-
teer participants for our error annotation, request-
ing speakers of English. The internal annotators are
Xiangru Tang, Arjun Nair, Borui Wang, Jai Desai,
Aaron Wade, Anushka Nijhawan, and Dragomir
Radev. These annotators are participating volun-
tarily. Our participants are free to opt out of the
study at any point in time. We have written four
scripts for use in the annotation process: (1) the
ﬁrst script generates an annotation spreadsheet and
a key spreadsheet from the model outputs. The
annotation spreadsheet does not contain the model
names; however, it contains an id that can be used
to recover the model name from the key spread-
sheet. For ease of annotation, summaries from
the same dialogue are grouped together; however,
they are randomly shufﬂed within each dialogue
so that the annotators cannot guess from the order-
ing as to which model is which. (2) The second
script splits an annotation spreadsheet into multi-
ple spreadsheets so that the work can be distributed
amongst annotators. (3) The third one merges these
spreadsheets back together after the annotation pro-
cess is ﬁnished. (4) The last script recovers the
model names from the key spreadsheet and inserts
them into the annotation spreadsheet. Each evalua-
tor is asked to examine whether there is an error and
the full context (dialogue, generated summaries,
and reference) and give a score on a scale of 1 to
10 for each of the criteria. We only consider faith-
fulness, instead of general quality. E.g. 1: very
poor, 3: poor, 5: neutral; 7: good; 10: very good.
We asked each internal annotator to evaluate 300
samples.
Other Ethical Issues (1) We did not use any per-
sonally identiﬁable information in the experiments.
(2) The goal of the project, improving the faithful-
ness of automatically generated summaries, is to
make the output of the summarization system more
reliable and minimize confusion for the readers
of the summaries. (3) We used existing summa-
rization datasets that do not contain any sensitive
information and are unlikely to cause any harm to
the annotators.
",no ethical concerns,No_ethical_concerns,no ethical concerns,2022,actions
11,"We believe that work on grounded generation mod-
els and specifically on probing factual consistency
in such models has positive implications for Ethics
in AI, especially in the terms of addressing the po-
tential harms and misuses (Bender et al., 2021) of
large pre-trained models such as GPT-3 (Brown
et al., 2020). Bender et al. have shown that such
large pre-trained models can easily be led to gen-
erate inaccurate, offensive, and otherwise harmfultexts. Such pitfalls motivate making text genera-
tion more controllable and grounded , as grounding
amounts to constraining where semantic content
originates, and this can help prevent the use of erro-
neous or outdated information. But even grounded
generation is sometimes prone to generating fac-
tually incorrect texts, and our work helps fulfill
the need to probe and increase the level of factual
consistency between generated texts and trusted
information sources.
In terms of potential misuses of our work, we be-
lieve it is mostly tied to the users being potentially
ill intended. While most users would probably
make ethical use of controllable and grounded gen-
eration, we cannot completely ignore the risk of
some users wanting to control generation to pro-
duce, e.g., fake news from dubious information
sources (However, in this case we would argue it is
mostly the user rather that AI that is at fault.) Never-
theless, the broader agenda of this work on factual
consistency checking could also be helpful, as such
dubious sources would contradict fact-checked in-
formation sources.
Regarding our handling of data and human sub-
jects: Our work introduces two new evaluation
datasets (§3.2.1,3.2.2). Both are constructed using
publicly accessible Wikipedia data only. Any mod-
ifications to this data (§3.2.1) are made by authors
of this paper only (i.e., no crowd-source human
annotation). We also conducted a human evalua-
tion that was small-scale on a volunteer basis by
colleagues of the authors, and thus wide-scale pay-
ment is not a concern. Evaluation uses a simple
multiple-choice input form, which offers no avenue
for privacy concerns.
","Misuse, fake news","Misuse, fake news","Misuse, fake news",2022,"concerns, statement, actions"
12,"As mentioned, this work is an attempt to tackle
the issue of creating agents that consider the rel-
ative harmfulness caused by their behaviors as a
first class citizen in their design in addition to task-
based rewards. Agents that simply focus on task
rewards are at significantly greater risk of acting in
a manner harmful to themselves and others. Text
games, in particular the games in the Jiminy Cricket
benchmark, provide semantically rich, grounded,
and morally salient scenarios for agents to navi-
gate through. To better understand and mitigate
the inherent biases found within the games of the
benchmark, we conduct a large scale human partic-
ipant study to judge the relative moral valence and
salience of the scenarios present in these games—
attempting to verify how accurately our evaluation
metrics map to values considered to be socially
beneficial by this particular set of humans.
We further note that agents trained in text envi-
ronments are more suited for domains in which
change in the world is affected via language,
which mitigates physical risks—downstream lines
of work are not directly relevant to robotics—but
not cognitive and emotional risks (Hausknecht
et al., 2020). As noted earlier, any system capa-
ble of generating natural language, even within the
limits of fantasy domains as seen in certain games,
is capable of accidental or intentional harmful and
biased language use—a property which we miti-
gate but do not entirely eliminate through our value
prior (Sheng et al., 2019; Dinan et al., 2020). We
note that we do not use this value prior to reason
about these moral scenarios by itself in vacuum—it is instead used to implicitly bias the actions of
an RL agent towards actions that are deemed to
be more aligned with socially beneficial behaviors
grounded in a particular context within the dynam-
ics of the environment.
We acknowledge that the data used to train the
value prior and the games themselves in addition to
the added annotations heavily skew towards West-
ern cultural and social norms. Further, despite at-
tempts at data curation and downstream task verifi-
cation, it is possible that the values encoded within
such language model priors represent socially un-
acceptable views (Bender et al., 2021). We thus
emphasize that these are not universally applica-
ble values and that agents trained in these environ-
ments cannot be used to directly assess the social
acceptability of human made actions in more real
world scenarios. This work presents a first step in
attempting to reduce the implicit harms that arise
from training interactive, intelligent agents to focus
only on task performance in popular, contemporary
sequential decision-making environments.
","Harmful language, bias, misuse","Harmful language, bias, misuse","Harmful language, bias, misuse",2022,"statement, actions, concerns"
13,"One aspect of our work with the potential for ethi-
cal pitfalls is large-scale generation from pretrained
language models, in constructing ATOMIC10x. Re-
cent work (Bender et al., 2021) has highlighted the
risks of models trained on massive text resources,
as GPT-3 (Brown et al., 2020) is, which we use
for generation. Indeed, open generations from pre-
trained language models can often contain harmful,
biased, or offensive aspects. We argue here that
this risk is largely mitigated in our work, mainly
due to the narrow and constrained nature of our
generations. The goal of our work is characterising
simple and generic anonymous situations, specifi-
cally in terms of commonsense causes and effects.
We ensure generations are focused on these top-
ics through careful prompting, which we found to
be quite effective at keeping these generations on-
topic. As such, the potential for harmful generation
is very low; indeed, in a manual inspection of 100
generated examples, we found none that were sig-
nificant harmful, besides one that contained adult
content.
A related concern is the potential for large mod-
els and training sets to make automated oppression
or exploitation possible, for instance in surveillance
or generating fake news. As above, we argue that
the generic, commonsense nature of our data and
models makes this concern less relevant here. Our
data does not contain any information directly re-
lated to these harmful domains (e.g. social media
or fake news generation). While our data may as-
sist machines in understanding basic situations, this
is unlikely to be useful for harmful models given
the simplicity of our data and still-flawed com-
monsense capabilities of even the most advanced
models.
Finally, we note that we ensure fair and gener-
ous compensation for all human evaluators we hire
through Amazon Mechanical Turk. Based on our
estimates of time required per task, we ensure that
the effective pay rate is at least $15 per hour.","bias, misuse","bias, misuse","bias, misuse",2022,"concerns, statement"
14,"Our goal is to diagnose performance of video-
language models on hard negative samples w.r.t.
verbs and person entities. Overall, we envision
positive impact from this work, as it aims to ex-
pose limitations of the existing models. Some of
our entity swaps focus on apparent gender (as de-
scribed by humans in the video-text datasets), but
we do not predict biological sex or gender iden-
tity. We construct our verb-focused contrast sets
automatically, using a large generative language
model, thus potentially some biases present in such
a model could propagate into our hard negative
samples. Practitioners who wish to use our contrast
sets should be mindful of such sources of bias.
",bias,bias,bias,2022,"statement, concerns"
15,"There is a risk of frame-based machine-generated
reader interpretations being misused to produce
more persuasive misinformation. However, under-
standing the way in which readers perceive and
react to news is critical in determining what kinds
of misinformation pose the greatest threat and how
to counteract its effects. Furthermore, while trans-
former models have contributed to much of the
recent algorithmic progress in NLP research and
are the most powerful computational models avail-
able to us, work has highlighted shortcomings in
their performance on domain-specific text (Moradi
et al., 2021) and noted that these models can easily
detect their own machine-generated misinforma-
tion (Zellers et al., 2019). Therefore, we do not see
this potential dual-use case as an imminent threat,
but urge implementation of systemic changes that
would discourage such an outcome in the future -
e.g. regulation that would lead to required safety
and fairness measures before large-scale systems
are deployed in the wild (European Commission,
2021).
We emphasize that annotations may reflect per-
ceptions andbeliefs of annotators, rather than uni-
versal truths (Britt et al., 2019). Especially consid-
ering demographic homogeneity of online crowd-
source workers, we urge caution in generalizing be-
liefs or taking beliefs held in certain social/cultural
contexts to be factual knowledge. We obtained an
Institutional Review Board (IRB) exemption for an-
notation work, and ensured annotators were fairly
paid given time estimations.
Broader impact. The rapid dissemination of in-
formation online has led to an increasing problem
of falsified or misleading news spread on social me-
dia like Twitter, Reddit and Facebook (V osoughi
et al., 2018; Geeng et al., 2020). We specifically
designed the Misinfo Reaction Frames formalism
to allow us to identify and predict high-impact mis-
information that is more likely to spread. This can
allow for future research on factors that make mis-
information particularly dangerous, as well as sys-
tems that are more effective at mitigating spread.
","bias, misuse, fairness, safety, misinformation","bias, misuse, fairness, safety, misinformation","bias, misuse, fairness, safety, misinformation",2022,"concerns, statement, actions, suggestions"
16,"We are sure that DuLeMon has been collected in
a manner that is consistent with the terms of use
of any sources and the intellectual property and
privacy rights of the original authors of the texts.
Meanwhile, our project is approved by an IRB. Fi-
nally, we also provide details on the characteristics
of DuLeMon and steps taken to ensure the poten-
tial problems with the quality of the dataset do not
create additional risks.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
17,"This dataset should be ONLY used for NLP re-
search purposes. All the instances in DuReader vis
are collected from public data and have been de-
sensitized. All annotators and reviewers are formal
employees and native speakers, thus we make sure
that all workers are fairly compensated. Experi-
enced reviewers have reviewed our dataset and the
data collection process.There are no copyright issues in our dataset.
Firstly, the document images are crawled from the
public data following the crawler protocol. Sec-
ondly, we strictly restrict the use of our dataset to
academic research. If the websites think it needs
to be removed, we will respond to removal at any
time.
",no ethical concerns,,no ethical concerns,2022,"suggestions ,statement, actions"
18,"We make sure that DuClarifyDial has been col-
lected in a manner that is consistent with the terms
of use of any sources and the intellectual property
and privacy rights of the original authors of the
texts. And crowd workers were treated fairly. This
includes, but is not limited to, compensating them
fairly and ensuring that they were able to give in-
formed consent, which includes, but is not limited
to, ensuring that they were voluntary participants
who were aware of any risks of harm associated
with their participation. Please see Section 3 for
more details characteristics and collection process
ofDuClarifyDial .
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
19,"Computing average learnability requires training
a model for multiple times at different perturba-
tion probabilities, which can be computationally
intensive if the sizes of the datasets and models are
large. This can be a non-trivial problem for NLP
practitioners with limited computational resources.
We hope that our benchmark results of typical per-
turbations for NLP models work as a reference for
potential users. Collaboratively sharing the results
of such metrics on popular models and perturba-
tions in public fora can also help reduce duplicate
investigation and coordinate efforts across teams.
To alleviate the computational efficiency issue of
average learnability estimation, using learnability
at selected perturbation probabilities may help at
the cost of reduced precision (Appendix D). We are
not alone in facing this issue: two similar metrics
for interpreting model inductive bias, extractability
ands-only error (Lovering et al., 2020) also re-
quire training the model repeatedly over the whole
dataset. Therefore, finding an efficient proxy for
average learnability is promising for more practical
use of learnability in model interpretation.
","accessibility, computational resources","accessibility, computational resources","accessibility, computational resources",2022,"statement, actions, concerns"
20,"Spoken language understanding (SLU) is a core
component in task-oriented dialogue system, which
becomes sufﬁciently effective to be deployed in
practice. Recently, SLU has achieved remarkable
success, due to the evolution of pre-trained models.
However, most SLU works and applications are
English-centric, which makes it hard to generalize
to other languages without annotated data. Our
work focuses on improving zero-shot cross-lingual
SLU model that do not need any labeled data for
target languages, which potentially is able to build
multilingual SLU models and further promotes the
globalization of task-oriented dialog systems.
",no_ethical_concerns,no_ethical_concerns,no ethical concerns,2022,statement
21,"The annotators were paid a fair wage and the an-
notation process did not solicit any sensitive in-
formation from the annotators. Finally, while our
approach is not tuned for any speciﬁc real-world
application, the approach could be used in sensi-
tive contexts such as legal or health-care settings,
and any work must use our approach undertake
extensive quality-assurance and robustness testing
before using it in their setting.
Replicability. As part of our contributions, we
will release the annotated BWB test set, and release
the crawling script of the training set under Fair Use
rules. The BLONDEpackage is also publicly avail-
able at https://github.com/EleanorJiang/BlonDe .
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
22,"We used only publicly available datasets. For hu-
man evaluation, we used a publicly available ser-
vice (Amazon Mechanical Turk) to hire voluntary
participants, requesting native speakers of English.
The participants were fairly compensated, above
the minimum hourly wage in their self-identiﬁed
countries of residence.
9
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,actions
23,"In this work, we present a new dataset created
through the translation of an existing resource,
XNLI (Conneau et al., 2018). While this allows for
results that are directly comparable, it also means
that this dataset inherits any biases and ﬂaws which
are contained in the previous dataset. Furthermore,
research involving languages spoken by Indigenous
communities raises ethical concerns regarding the
exploitation of these languages and communities:
it is crucial that members of the community are
able to directly beneﬁt from the research. Trans-
lation for AmericasNLI was done by either paper
authors or translators who were compensated at a
rate based on the average rate for translation and
the minimum wage in their country of residence.
Additionally, many authors are members of, and/or
have a record of close work with communities who
speak a language contained in AmericasNLI.
","bias, exploitation","bias, exploitation","bias, exploitation",2022,concerns
24,"Data Access and Licensing. We develop the KE-
TOD dataset based on the publicly available SGD
dataset2(Rastogi et al., 2020). The SGD dataset
is publicly available under the CC-BY-SA-4.0 Li-
cense.
Dataset Collection Process and Conditions.
This project is approved by our Institutional Review
Board (IRB). Our annotators are all U.S. based.
For the annotation of our KETOD dataset, linguis-
tics for assessing data quality, and all the human
evaluations, our annotators were hired as full-time
employees through a leading annotation services
vendor, and were paid in accordance with a fair
wage rate. During the data annotation, we instruct
the annotators to skip any example that contains
offensive or any unethical contents.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
25,"We leverage the freely available open access ques-
tion dataset INQUISITIVE for annotation and model
training. Though we have not exhaustively checked
the source dataset manually, given they are sourced
from the WSJ partition of the Penn Treebank and
Associated Press articles from the TIPSTER cor-
pus, we consider them relatively safe and do not
find any objectionable content.
Training is done using large pretrained models
that have been shown to have bias. Although the
generated questions do not appear biased, they may
hallucinate content, which is a common problem
for neural generation models.
Finally, we obtained institutional review board
permission to conduct MTurk based data collec-
tion.
","bias, hallucinations","bias, hallucinations","bias, hallucinations",2022,"statement, concerns"
26,"There are several ways in which entity linking /
resolution models could be biased and a potential
for those biases to have harmful downstream con-
sequences. There is already a large body of work
studying the biases in language models (such as
those used for fine-tuning in our work) and corefer-
ence models, most notably in understanding when
error rates in coreference differ across certain popu-
lations (e.g., genders, races, and other entity types,
more broadly, that display skewed distributions in
the data). For instance, if entity mentions are author
names on citation data and the entities are scien-
tific authors, aggregated statistics like h-index or
citation count could be biased if the models used
to disambiguate the author names are biased. If
entity linking and discovery systems are used to
build or populate knowledge bases, those systems
may propagate these biased predictions. This can
be particularly problematic if one used such a bi-
ased knowledge base to train future models, thus
perpetuating and amplifying the skew. Lastly, we
also note that entity linking and discovery are anal-
ogous to surveillance and tracking in computer vi-
sion, which should warrant substantial weight of
ethical considerations.
","bias, harmful consequences, surveillance","bias, harmful consequences, surveillance","bias, harmful consequences, surveillance",2022,"concerns, statement"
27,"We do not foresee specific risks associated with our
exploration of alignment for AMR parsing. That be-
ing said, there are nuances to our results that future
researchers may take into considerations. For in-
stance, our experiments are only for English data in
the genres covered by AMR2.0 and AMR3.0. It is
not clear how our results translate to other domains
(e.g. biomedical text) or other languages. Nonethe-
less, we are hopeful that are methods would transfer
favorably because they are intentionally designed
to be easy to use and general purpose.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
28,"This work is foundational NLP research on seman-
tics, and as such we do not foresee any immediate
risks, ethical or otherwise. However, representation
learning may be used as part of many downstream
NLP tasks such as information extraction, classi-
fication, and natural language generation, which
might be used for harmful surveillance, discrimina-
tion, and misinformation.
The SFU, Amazon, and Yelp datasets used in this
work do not attach unique identifiers such as user
IDs or names to data instances such that the original
authors might be identifiable. A manual review of a
small random subset of the data did not reveal any
overtly identifiable or harmful information within
the text.8397
","surveillance, discrimination, misinformation","surveillance, discrimination, misinformation","surveillance, discrimination, misinformation",2022,"statement, concerns, actions"
29,"The data collection process for the re-annotated
MDMD dataset follows the regulations of Twitter.
The data is anonymized so the data can not be
linked to a particular user. The crowd workers
are fairly compensated with a minimum wage per
hour (using the minimum wage from a Western
European country). The data collection process
has been approved by the ethics committee of the
authors’ university. The data will be made available
to researchers that agree to the ethical regulations
of the ethics committee. Characteristics and quality
control of the re-annotated dataset are described in
Section 5.
The claims in the paper match the results and
the model can be generalized to multi-label dia-
logue safety detection tasks. This paper can be
used for the deployment of dialogue systems, hop-
ing to improve the ability of dialogue systems to
detect malevolent human natural language. Multi-
label classiﬁcation has a positive impact on the
application of dialogue systems. Detecting and ﬁl-
tering dialogue responses that are not malevolent
may decrease the diversity of the dialogue. For the
deployment of non-malevolent dialogue systems,
it is better to consider the extent of malevolence
according to malevolence label counts of each ut-
terance or the perception of different labels.
This paper does not involve identity characteris-
tics nor does it categorize people.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
30,"To ensure that the dataset does not have any sen-
sitive topics, we ask crowd-workers to make com-
ments if the dialog content involves any of follow-
ing: 1. offensive, racist, biased and non-tolerant
behavior; 2. violence and self-harm; 3. sexual or
flirtatious behavior; 4. controversial and polarizing
topics. Since the database of both MultiWOZ and
SGD are sampled from real world, annotators also
comment if there are real names included in the
slot values, which can be personally identifiable
information (PII). Considering both of these two
datasets are public dataset, we do not replace those
named entities with placeholders. The detailed de-
scription of sensitive topics is included in the Fig. 7
in the appendix.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,actions
31,"One of the key properties of SOCIO FILLMORE is
its being agnostic on whether a perspective should
be considered “good” or “bad”. In this respect
SOCIO FILLMORE isnot a prescriptive tool on
how news should be reported but rather a support
tool that helps to magnify misuse of frames and
biases that may mirror and strengthen asymmetric
power dynamics existing in our societies.
SOCIO FILLMORE is based on state-of-the-art
NLP technologies. While these tools achieve very
good performances (also in zero-shot multilingual
settings), none of them can be considered to reach
nor mimic humans. The tools are based on pow-
erful machine learning algorithms but they are far
away from being artificial intelligent agents. We
recommend caution when using SocioFillmore
since margins of errors are present. At the same
time, additional tests are needed before deploying
SOCIO FILLMORE as an integrated tool or service
that citizens or professionals may use for purposes
other than research.
One of the services of SOCIO FILLMORE is
corpus-assisted language analysis. The outcome
of this service is highly sensitive to the data that
are feeded to the tool. This requires users to pay
particular attention to the curation of the data that
will compose their corpus. Results on the presence
of frames and bias are a direct consequence of what
is input to the tool. The case studies we have illus-
trated are based on carefully curated corpus collec-
tions conducted by experts. We thus recommendthat users of SOCIO FILLMORE should accompany
the presentation of their results with a documen-
tation of their data collections using tools such
as data statements (Bender and Friedman, 2018) or
data sheets (Gebru et al., 2021).
","bias, misuse","bias, misuse","bias, misuse",2022,"statement, concerns, suggestions"
32,"This paper focuses on the entity recognition in doc-
ument images under few-shot setting. Our architec-
ture are built upon open-source models and all the
datasets are available online. We will release the
code and datasets on https://github.com/
zlwang-cs/LASER-release . Therefore, we
do not anticipate any major ethical concerns.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
33,"This paper focused on collaborative neural-
symbolic semantic parsing for the English Re-
source Grammar (ERG). Our architecture are built
based on open-source models and datasets (all avail-
able online). We do not anticipate any major ethical
concerns.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
34,"The work presented in this paper deals with foun-
dations aspects of representation learning for lan-
guage tasks. We present experiments on core tasks
dealing with textual semantic equivalence, which
do not pose ethical concerns.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
35,"This paper focuses on the patent approval predic-
tion. The data is publicly available from USPTO
and we collected it to form a large-scale dataset.
Our architecture is built upon open-source models
and all the datasets are available online. Therefore,
we do not anticipate any major ethical concerns.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
36,"Text-generation methods have the potential to gen-
erate non-factual (Maynez et al., 2020; Pagnoni
et al., 2021; Kreps et al., 2020) and offensive con-
tent (Gehman et al., 2020). Furthermore, training
these models on uncurated data can lead to the
models replicating harmful views presented in the
training data (Bender et al., 2021). Text-editing
models are also susceptible to these issues, but
they have been shown to mitigate some of them.
Speciﬁcally, they reduce the likelihood of differ-
ent types of hallucination (Malmi et al., 2019) and
their higher sample efﬁciency (Malmi et al., 2019;
Mallinson et al., 2020) enables more careful cura-
tion of the training data. The tutorial will discuss
the ethical issues related to text generation and pro-
vide concrete examples on how text-editing models
can help mitigate them.4
","nonfactual content, offensive content, harmful data","non factual content, offensive content, harmful data","non-factual content, offensive content, harmful data",2022,"statement, concerns"
37,"Much NLP research is highly English-centric, with
a small number of other high-resource languages
also beneﬁtting, and the vast majority of languages
being left behind or excluded (Joshi et al., 2020).
This applies to the multilingual contextual model
that we extend, in that high-resource languages are
also overrepresented in its training data, and most
languages are not part of the model at all. As well,
in the zero-shot transfer tasks we evaluate on, the
“source language” is English. Similarly, the BLI
datasets we use are mostly xx-en language pairs.
Although this paper makes an effort to reduce the
gap between higher- and lower-resource languages,
we remain part of this paradigm. We would like to
more strongly focus on low-resource languages in
future work.
",Misrepresentation,Misrepresentation,misrepresentation,2022,"statement, concerns"
38,"Works like Dev et al. (2021) have drawn attention
to gender exclusivity and issues relating to non-
binary representation in NLP, particularly in the
English language. For practical constraints such as
the limited availability of non-binary gender data
and/or the significant under-representation of non-
binary gender identities in available datasets, we
limit this study to a binary definition of gender. For
the same reasons stated above, our definition of
gender is analogous to female and male definitions
of sex (Walker and Cook, 1998). Although this is
an obvious limitation to our work, we believe this
work opens the door to extensively explore similar
issues in non-binary gender settings, which need a
more expansive discussion.
Since the definition of a biased text is highly do-
main, context, and task dependent, especially when
it relates to the use of language (English in this
case), our approach identifies “biased” and “neu-
tral” texts as per how they are defined or annotated
in the training data for a specific task. Hence, the
labels (fair or biased) assigned to certain text ex-
amples may not be perceived accordingly in other
settings and tasks. We also note that, although the
use of explicit gender terms in certain domains may
be deemed to introduce biases (in some recruitment
scenarios for instance), this practice may be accept-
able or even encouraged in other domains such as
in text discussions about diversity and sexism.
",gender bias,"Gender exclusivity, Non-binary representation, bias",bias,2022,"statement, concerns, actions"
39,"Our works aims to achieve fairer models, contribut-
ing to equal treatment for different demographic
subgroups. However, its usage in the real world
should be carefully calibrated/auditioned as debias-
ing for one projected attribute does not guarantee
fairness for other protected attributes. In this work,
due to the limitations of the dataset, we treat gender
as binary, which is not perfectly aligned with the
real world.
","bias, fairness","Bias, misrepresentation (gender as binary)","fairness, bias",2022,"statement, concerns"
40,"We used publicly available resources to train our
model (Lin et al., 2014). As with other statistical
methods for word representations, our approach
may capture social biases which manifest in its
training data (e.g., MSCOCO was shown to be bi-
ased with respect to gender, Zhao et al., 2017). Our
code includes a model card (Mitchell et al., 2019)
which reports standard information regarding the
training used to produce our models and its word
clusters.
",bias,bias,bias,2022,statement
41,"All work that automatically generates and/or al-
ters natural text could unfortunately be used mali-
ciously. While we cannot fully prevent such uses
once our models are made public, we do hope that
writing about risks explicitly and also raising aware-
ness of this possibility in the general public are
ways to contain the effects of potential harmful
uses. We are open to any discussion and sugges-
tions to minimise such risks.
","misuse, harmful uses","Misuse, harmful uses",misuse,2022,concerns
42,"Collecting these dialogs for our dataset is dif-
ficult in that it requires substantial commitment
from participants. In order to provide as large
of a dataset as possible, we utilized the services
of Amazon Mechanical Turk as previously men-
tioned. Given ethical concerns in recent years
regarding data acquisition through crowdworkers,
we verified that the crowdworkers assigned to
our tasks were compensated fairly and treated hu-
manely.
The annotators also examined the dataset to en-
sure that it does not contain personally identifi-
able information (which was anonymized) or po-
tentially offensive content (which was removed).
",no ethical concerns,no_ethical_concerns,offensiveness,2022,actions
43,"We collected publicly available Wikinews image-
caption pairs without storing any personal data.
During data cleaning, we remove the cases that
contain pornographic, profane, and violent content.
We annotate the data using the crowdsourcing
platform of Alibaba. To ensure that the crowd
workers were fairly compensated, we paid them at
an hourly rate of 15 USD per hour, which is a fair
and reasonable rate of pay for crowdsourcing.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,actions
44,"We honor and support the ACL code of Ethics. Lan-
guage model pre-training aims to improve the sys-
tem’s performance on downstream NLU tasks. All
pre-training corpora and pre-trained models used in
this work are publically available. In addition, all
evaluated datasets are from previously published
works, and in our view, do not have any attached
privacy or ethical issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
45,"Our work advocates for the need for more thought-
fulness of linguistic phenomena during the gener-
ation of sign videos. All models and analyses are
built on a publicly available benchmarking dataset.
We acknowledge that some modules of our model
depend on pre-trained models such as word embed-
dings. These models are known to reproduce and
even magnify societal bias present in their original
training data (Li et al., 2021).",bias,bias,bias,2022,"statement, concerns"
46,"Energy efﬁciency. Our models, similar to many
deep learning language models, take signiﬁcant
pre-training time and are not energy efﬁcient. We
acknowledge this important issue and believe work
on creating energy efﬁcient models should receive
scholarly attention.
Data. Our pre-training datasets are collected from
the public domain and cover diverse communities.
As we have demonstrated, our resulting models
are better equipped to power applications involving
several varieties of Arabic as well as code-switched
language use involving Arabic. From this perspec-
tive, we hope they add to ongoing efforts in the
community to design models that are fairer and
more representative.
ARGEN Benchmark Release. We design AR-
GEN using both existing datasets and new datasets
that we create for this work. In our accompanying
GitHub repository, we link to all existing publicly
available components of the benchmark with stan-
dard splits from source as well as components that
can be acquired from data organizations. In ad-
dition, we released all the new datasets we have
developed. While we have prioritized standardiz-
ing evaluation on as many uniﬁed and consolidated
datasets and tasks as possible, we also report per-
formance on individual test sets so as to enable the
community to replicate our work even on particular
parts or tasks of ARGEN if they so wish.
17https://www.computecanada.ca
18https://arc.ubc.ca/ubc-arc-sockeye
19https://sites.research.google/trc/about/AraT5 Models Release. All our pre-trained mod-
els are publicly available for non-malicious use.
We acknowledge our models may still be misused
in real world. However, we hope the models will
be deployed in domains such as education, disaster
management, health, recreation, travel, etc. in so-
cially beneﬁcial ways. These meaningful potential
use cases are behind our decision to release the
models.
","Environmental impact, representation, misuse","Environmental impact, representation, misuse",misuse,2022,"statement, concerns, actions"
47,"In our experiments, we noticed that some of the
input tweets contained references to sensitive top-
ics, such as religion and gender, or to tragic life
events. Producing sarcasm for such inputs mightbe inappropriate and offensive to some (as our ex-
periments conﬁrmed). We clearly informed our
survey participants about this possibility in the Par-
ticipant Information Sheet, before accessing our
survey. The sheet is enclosed in Appendix D.
",sensitive topics,Offensiveness,no ethical concerns,2022,"concerns, actions"
48,"We collect a training datasets as a part of the anal-
ysis in this work. The passages are sourced from
Wikipedia through KILT. As described in the main
text, our incentive structure is designed to ensure
that crowdworkers were fairly compensated. Our
datasets focus on the English language. As this data
is not collected for the purpose of designing NLP
applications, we do not foresee any risks associated
with the use of this data.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
49,"Misuse: We release two datasets of challenging
emoji examples on which commercial solutions
and state-of-the-art transformer models have been
proven to fail. Malicious actors could take inspi-
ration for bypassing current detection systems on
internet platforms, or in principal train a generative
hate speech model. This concern is also raised by
Vidgen et al. (2021b), but the conclusion is reached
that such risk of misuse is small and outweighed
by the considerable scientific and social benefits.
Harm Statement: Following the advice of Der-
czynski et al. (2022), we describe the risks to well-
being during the production and publication of
this research and the steps taken to mitigate them.
There is a risk of harm to data subjects i.e., the
targets of hate, from reinforcing hateful, dehuman-
izing or derogatory statements, and to readers view-
ing this content in the paper. To mitigate these
harms, we include a content warning directly after
the abstract, at least a page before any harmful con-
tent is displayed and colored in red for maximum
visibility. We also include a section-specific con-
tent warning before §2.2, which includes a number
of hateful examples. Hateful examples in §2.2 are
consistently formatted and visually distanced with
gray text. All other examples (including Tab. 1,
Tab. 4) are presented with placeholders for the
[IDENTITY]. Hateful slurs are starred out with
an asterisk where possible (e.g., Tab. 5) but we can-
not star out emoji without hindering interpretation.
There is also a risk to researchers and annotators
from labeling and viewing harmful content. As
authors, we oppose the use of hateful language.
We follow protocols for protecting annotator well-
being, including briefing sessions, regular check-
ins and provision of mental health support.
","Misuse, harm, hateful language","Misuse, harm, hateful language","misuse, offensiveness",2022,"statement, concerns, actions"
50,"The authors foresee no ethical concerns with the
research presented in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
51,"In order to complete our human evaluation, we
used a crowdsourcing platform. For each task, we
estimated the amount of time we expected the task
to take and made sure that the crowdworkers would
be paid (at minimum) a wage of $15 per hour. A
further ethical consideration of this work is in the
context of the use of language models for text gen-
eration. Language models have been used for the
generation of malicious text, e.g., fake news and
triggering content. The results in this work may
provide insights for those using language models
for such purposes as to how generations can be
chosen to seem more “human-like.”",Misuse,Misuse,no ethical concerns,2022,"statement, suggestions"
52,"9.1 Potential Risks
Figurative language has the potential to be used in
a harmful way, especially against minority and his-
torically disadvantaged groups. Such language is
often emotionally charged or used to insult others,
so we took care to remove any examples that were
potentially offensive, especially toward protected
groups. We acknowledge that this was based on
our own judgment, and generically insulting lan-
guage (for instance, a metaphor that implies that
someone is ugly) was not removed because it was
not insulting toward any particular individual.
All examples from Fig-QA are also in English,
as it is the language that all authors speak, and
this was a preliminary dataset, being the ﬁrst of its
type that the authors have worked on. However,
ﬁgurative language is not just important in English,
and we leave investigation of ﬁgurative language
in other languages as future work.
9.2 Terms of Use of Artefacts Used
Additional datasets we used were the Winogrande
dataset, SNLI, MNLI, FEVER-NLI and ANLI.
Winogrande is licensed under the Apache 2.0 li-
cense, which allows modiﬁcation and distribution,
ﬁtting our use case. SNLI is licensed under a Cre-
ative Commons Attribution ShareAlike 4.0 Interna-
tional license, which allows us to share and adapt
the work as long as we give attribution. The ma-
jority of MNLI is licensed under OANC, which
allows free use. The ﬁction section of this dataset
consists mostly of works in the public domain, but
several stories are licensed: Seven Swords is avail-
able under a Creative Commons Share-Alike 3.0
Unported License, while Living History andPass-
word Incorrect are available under Creative Com-
mons Attribution 3.0 Unported Licenses. These
licenses allow sharing and adaptation with attri-
bution. FEVER-NLI is licensed under an MIT
license, which also allows modiﬁcation, distribu-
tion, and reuse. ANLI is licensed under Creative
Commons Attribution-NonCommercial 4.0 Inter-
national, which also allows sharing and reuse as
long as we give attribution.
Models used were GPT-2, GPT-neo, GPT-3,
BERT and RoBERTa. GPT-2 and GPT-neo are
licensed under an MIT license, which does not
place any restrictions on its use. BERT is licensed
under an Apache License 2.0, which allows modiﬁ-
cation and distribution. RoBERTa is licensed undera GNU General Public License v2.0. This ﬁts our
use case as we are only running and studying the
model. GPT-3 is licensed by Microsoft, and we
used the public API to receive output.
9.3 Computational Infrastructure and
Computing Budget
To run our computational experiments, we had ac-
cess to a compute cluster, but minimal compute is
needed to run the experiments in this paper. We
generally did not use more than 2 GPUs at a time.
The only models that required GPU parallelism
were the GPT-neo models. An estimated 20 GPU
hours are required.
Our computing budget was roughly 100 USD.
We also used roughly 20 USD on credits for the
GPT-3 API.
","Misuse, offensive language","Misuse, offensive language",misuse,2022,"statement, concerns, actions"
53,"All studies involving human evaluations were con-
ducted outside of the scope of this paper. The
authors foresee no ethical concerns with the work
presented in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
54,"As the data and models used to develop the multi-
perspective search engine inevitably contain arti-
facts and inaccuracy, the returned results should
only serve as speculation, as opposed to consol-
idated evidence for user queries, and should not
be taken as granted without further reasoning and
validation. The participants of the user study are
informed of the potential risks of such before the
annotation task.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
55,"We notice that some chit-chat utterances generated
by the proposed UniDS may be unethical, biased or
offensive. Toxic output is one of the main issues of
current state-of-the-art dialogue models trained on
large naturally-occurring datasets. We look forward
to furthering progress in the detection and control
of toxic outputs.
","Bias, offensive language, toxic output","Bias, offensive language, toxic output",bias,2022,"statement, concerns"
56,"We honor and support the ACL code of Ethics.
Task-oriented dialogue systems aim to interact and
assist the users to fulﬁll their goals. The interaction
and assistance process do not involve any bias to-
wards to the participants. All datasets used in this
work are from previously published works, and
in our view, do not have any attached privacy or
ethical issues.
",no ethical concerns,no_ethical_concerns,bias,2022,statement
57,"Neural generative models have the potential to gen-
erate unexpected responses such as violent remarks.
As we focused on repetition generation, its model
repeats a user’s utterance, and so there is little
chance of causing unintended responses compared
with chit-chat dialogue systems. However, this
does not mean that unintended responses will never
appear, e.g., when a user’s utterance is an unin-
tended expression. Thus, the same consideration
must be taken as with other dialogue systems.
Our dataset was created to repeat from utterances
in a privacy-secured dataset, and so there is no
privacy issue. Since the license of the original
dataset is CC BY-NC 4.0, we could use it for this
study. We define that the license of our dataset is
also CC BY-NC 4.0.
",unexpected responses,no_ethical_concerns,unexpected responses,2022,"statement, concerns"
58,"The two native Swahili annotators are the authors
of this paper, and they receive fair compensation
from the project fund. We collected data from a few
sources, and all annotated datasets are in Swahili
language. We anonymized all texts and confirm
that our datasets are allowed to be disclosed for
academic purpose according to policies and laws
(e.g., UN news13, V oice of America14, Deutsche
Welle15, and taifaleo16). We gratefully acknowl-
edge a favor of every institute or company that
approved of a generous data policy for academic
purpose.
13https://www.un.org/en/about-us/terms-of-use
14https://www.voanews.com/p/5338.html
15https://www.gesetze-im-
internet.de/englisch_urhg/englisch_urhg.html
16https://www.nationmedia.com/terms-of-use/
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
59,"We hope and expect to see a nonnegative net soci-
etal impact from better text generation and ranking
algorithms in general and from this work in partic-
ular. As we have shown, there is room to improve
the inference procedures used with small language
models, which incur lower costs than training and
evaluation of large models. However, researchers
should bear in mind the risks and potential misuse
of automatic generation of long-form text.
",misuse,misuse,misuse,2022,"statement, suggestions"
60,"The research reported in this paper is concerned
with fundamental aspects of machine learning mod-
els by enabling machine learning models to better
represent data and improve the representation of
low-frequency categories, ideally improving fair-
ness. The methods we propose for this purpose do
not introduce additional ethical risks on top of the
previous work we build upon.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
61,"Because word-level perturbation includes stochas-
tic behaviour, the experimental results depend on
random seeds. Ideally, tons of trials are required to
compare the methods correctly. However, because
of limitation of computational resources, we aver-
aged the results of five trials for text classification
and three trials for machine translation.
Word-level perturbation can be seen as a vari-
ation of data augmentation. Therefore, the ef-
fectiveness of word-level perturbation might be
small when the training corpus is significantly large.
However, this work does not discuss this point be-
cause preparing such a large training corpus is dif-
ficult.
",limited computational resources,limited computational resources,no ethical concerns,2022,"actions, concerns"
62,"Intelligent systems are becoming increasingly
widespread, and NLP models are often used as
important components in their architecture. How-
ever, once these systems are deployed in the real
world, there is a risk of them exhibiting biased, er-
ratic or dangerous behaviour. In order to prevent
such events from happening, it is crucial to perform
a thorough testing and validation process. Indeed,
this is one of the tenets of the ACM Code of Ethics
and Professional Conduct3. Namely, paragraph 2.5
therein recites “Extraordinary care should be taken
to identify and mitigate potential risks in machine
learning systems. ” The contributions we propose
in the present paper are directed towards this goal.
More speciﬁcally, we believe that metamorphic
testing is a valuable tool in the model tester’s ar-
senal, and our contributions widen its scope of ap-
plication. As a result, more instances of unwanted
behaviour can be identiﬁed and addressed before
their impact is felt by the end user.","Bias, erratic behaviour, dangerous behaviour","Bias, erratic behaviour, dangerous behaviour",bias,2022,"statement, concerns, suggestions"
63,"While data is essential in making models like
MBHN effective, we must work within the purview
of acceptable privacy practices to avoid coercion
and intrusive treatment. To that end, we utilize pub-
licly available Twitter and Reddit data in a purely
observational and non-intrusive manner. Although
informed consent of each user was not sought as
it may be deemed coercive, We follow all ethical
regulations set by Twitter and Reddit.
There is an ethical imperative implicit in this
growing inﬂuence of automation in market behav-
ior, and it is worthy of serious study (Hurlburt et al.,
2009; Cooper et al., 2020). Since ﬁnancial markets
are transparent (Bloomﬁeld and O’Hara, 1999), and
heavily regulated (Edwards, 1996), we discuss the
ethical considerations pertaining to our work. Fol-
lowing (Cooper et al., 2016), we emphasize on
three ethical criteria for automated trading systems
and discuss MBHN ’s design with respect to these
criteria.
Prudent System A prudent system ""demands ad-
herence to processes that reliably produce strate-
gies with desirable characteristics such as min-
imizing risk, and generating revenue in excess
of its costs over a period acceptable to its in-
vestors"" (Longstreth, 1986) .MBHN is directly opti-
mized towards high risk bubble forecasting.
Blocking Price Discovery A trading system
should not block price discovery and not inter-
fere with the ability of other market participants to
add to their own information (Angel and McCabe,
2013). For example, placing an extremely large
volume of orders to block competitor’s messages
(Quote Stufﬁng) or intentionally trading with itself
to create the illusion of market activity (Wash Trad-
ing).MBHN does not block price discovery in any
form.
Circumventing Price Discovery A trading sys-
tem should not hide information, such as by partici-
pating in dark pools or placing hidden orders (Zhu,
2014). We evaluate MBHN only on public data in
regulated markets.
Despite these considerations, it is possible for
MBHN , just as any other automated trading system,
to be exploited to hinder market fairness. We fol-
low broad ethical guidelines to design MBHN and
encourage readers to follow both regulatory and
ethical considerations pertaining to the market.
","Market fairness, misuse","Market fairness, misuse","bias, exploitation, fairness",2022,"statement, concerns"
64,"The sensitive nature of this work calls for careful
deliberation of the risks and ethical challenges in-
volved. While we only use publicly available user
data, we emphasize the importance of preserving
the privacy of the users involved (De Choudhury
et al., 2016). We acknowledge that the predictive
power of HYPHEN depends on the data, which is
in tension with user privacy concerns. We care-
fully adopt the measures followed by Chancellor
et al. (2016). Speciﬁcally, we operate within the
acceptable privacy bounds (Chancellor et al., 2019)
and considerations (Fiesler and Proferes, 2018)
in order to avoid coercion and harmful interven-
tions (Chancellor et al., 2019). We paraphrase and
anonymize all samples in the suicide ideation detec-
tion detection dataset using the moderate disguise
scheme (Bruckman, 2002; Fiesler and Proferes,
2018). We also perform automatic de-identiﬁcation
using named entity recognition to identify and
mask personally identiﬁable information.
While one of our work’s application is to aid
in the early detection of suicidal users and early
intervention, it is imperative that any interventions
be well-thought, failing which may lead to counter-
helpful outcomes, such as users moving to fringe
platforms, which would make it harder to provide
assistance (Kumar et al., 2015). Care should be
taken so as not to create stigma, and interventions
must be carefully planned by consulting relevant
stakeholders such as clinicians, designers, and re-
searchers (Chancellor et al., 2016), to maintain
social media as a safe space for individuals looking
to express themselves (Chancellor et al., 2019).
",no ethical concerns,"Privacy, stigma",no ethical concerns,2022,"statement, actions, suggestions"
65,"There are several positive broader impacts from
designing methods for learning from human expla-
nations. Foremost among them is the promise of
better aligning learned models with human priors
on what kinds of behaviors are good, which could
be especially helpful when these priors are hard
to robustly encode in supervised learning objec-
tives or unlikely to be learned from the available
data. Explanations can also greatly improve model
sample efﬁciency, which is broadly beneﬁcial for
difﬁcult, time-consuming, or human-in-the-loop
tasks where acquiring a large amount of data is
expensive and slow.
There are still some possible risks to this method-
ology, mainly involving overconﬁdence in what ex-
planations can provide. For instance, just because
explanations improve a model’s performance does
not mean the model will behave exactly as a hu-
man would. We risk anthropomorphizing machine
learning models when we suppose their learned
interpretations of explanations matches our own.
","Overconfidence, anthropomorphism","Overconfidence in explanations, anthropomorphism","overconfidence, anthropomorphism",2022,"statement, concerns"
66,"We provide details of our work to address poten-
tial ethical concerns. In our work, we propose a
simile property probing task and construct prob-
ing datasets from both general textual corpora and
human-designed questions. First of all, all the
data sources used in the data collection process
are publicly available. Speciﬁcally, we follow the
robots.txt9to respect the copyright when we col-
lect similes from the learning platform Quizizz
(Sec. 3.2.1). Moreover, there are three steps involv-
ing human annotation to ensure the quality of the
datasets: simile and simile components recognition
(Sec. 3.2.1), human conﬁrmation for distractors
(Sec. 3.2.2), and human performance (Sec. 4.1). To
ensure the quality of annotation, all the annotators
do not participate in our probing data collection,
and they always label a small set of 50 examples
to reach an agreement on the labeling criteria be-
fore the formal labeling. We protect the privacy
rights of annotators and pay them above the local
minimum wage.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
67,"We acknowledge the suboptimality of predicting bi-
nary gender labels and using self-reported training
data with users having only binary option (Lar-
son, 2017). The topic- and platform-specific envi-
ronment underlines the limits of such user studies.
Any user-augmented classification efforts risk in-
voking stereotyping and essentialism, which can
cause harm even if they are accurate on average
differences (Rudman and Glick, 2008), and can be
emphasized by the semblance of objectivity cre-
ated by the use of a computer algorithm (Koolen
and van Cranenburgh, 2017). It is important to
be mindful of these effects when interpreting the
model results in its application context. Use of any
user data for socio-demographic estimates shall be
transparent, and limited to the given aggregated
purpose (Williams et al., 2017), no individual posts
shall be republished and the study authors were
advised to take account of users’ privacy expecta-
tions (Williams et al., 2017; Shilton and Sayles,
2016; Townsend and Wallace, 2016) when col-
lecting online data for user-based predictions. In
our case, we utilize publicly available Reddit data
in a purely observational, community-aggregated,
and non-intrusive manner (Norval and Henderson,
2017) and restrain from any verbatim citations of
the post contents.
","Stereotypes, essentialism, harm ","Stereotyping, essentialism, harm, ","privacy, misuse",2022,"statement, actions"
68,"Human Evaluation We perform a human evalu-
ation using human raters. After making an internal
call for participation that included a task descrip-
tion and the amount of compensation, we selected
participants based on their timely response to our
call. The chosen raters were compensated fairly.
Modeling Stereotypes Stereotypes are funda-
mentally cognitive schemas that help the perceiver
process the dynamics of different groups. They are
made up of a collection of traits that are ascribed to
a given social group (Dovidio et al., 2010). If made
conscious, they can aid in improving cultural sen-
sitivity (Buchtel, 2014). However, in most cases,
these are unconscious beliefs and can then lead to
bias and discrimination (Hoffman and Hurst, 1990).
Human-written content reflects these cognitive bi-
ases, and when natural language processing (NLP)
models are trained on this biased data, they can
further propagate stereotypes and discrimination
(Hovy and Spruit, 2016). Mitigating bias in NLP
has thus become a major research direction. These
works often require structured knowledge or lists
about biased terms, e.g., Bolukbasi et al. (2016)
rely on a list of male-female minimal pairs. Our
work’s contribution is to automatize this process
by exploiting social media users’ beliefs about so-
cial groups, i.e., we collect assertions and questions
about social groups which appear often in both Red-
dit and Twitter data. In this sense, our approach
can be described as a similar process that occurs in
humans as they become aware of their own mental
processes, including stereotypes (Buchtel, 2014).
If we are aware of stereotypes, we can use them to
improve cultural sensitivity and mitigate the effects
of bias and discrimination.
StereoKG could be used to generate stereotypi-
cal content (e.g., through verbalization). While ver-
balized stereotypes can improve the downstream
task performance on knowledge crucial samples
(Section 5), they could, however, also be misused
in a hurtful manner, e.g., by using stereotypical
knowledge in question-answering systems. How-
ever, this is a general issue pertaining to language
models which we are trying to mitigate through
our work: if trained on bias(ed) data, they could be
misused to generate harmful content.
environmental_impact Our models are trained
on Titan X GPUs with 12GB RAM. In order to
economize the energy use, we did not perform any
extensive hyperparameter exploration.","stereotypes, misuse, environmental impact","Bias, discrimination, stereotyping, environmental impact","bias, misuse, environmental_impact, offensiveness",2022,"statement, concerns"
69,"For annotation, we source participants from Ama-
zon Mechanical Turk only within the United States,
paying $10/hour based on average task time. We
did not reject any work but exclude data from par-
ticipants who failed an attention check.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
70,"Our intended use case is to induce the schema of
raw conversations between a real user and system,
where the conversation is not structured or con-
strained. Our experiments are done on English data,
but our approach can be used for any language, es-
pecially because our method does not require any
language-specific tools such as parsers which gen-
erally require a lot of labeled data. We hope that
our work can reduce design and annotation cost in
building dialog systems for new domains, and can
inspire future research on this practical bottleneck
in applications.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
71,"We would like to thank the anonymous reviewers
for their constructive comments. This work was ap-
proved by XiaoAI team. All personally identifiable
information in our dataset was removed.
At last, we discuss the potential ethic impacts
of this work. (1) Transparency : We will release
the newly introduced corpus and the built conver-
sation knowledge graph, as well as the benchmark
approaches to facilitate future research. Similar
datasets and knowledge bases include Empathetic-
Dialogues (Rashkin et al., 2019) and ATOMIC (Sap
et al., 2019), which are often public available and
have been used extensively. (2) Privacy : The cor-
pus is crowdsourced under a set of specific rules
to forbid the workers disclosure sensitive and per-
sonal identifiable information. (3) Politeness : Be-
cause our conversations are human-written and are
related to healthy dailylife scenarios, they are ex-
pected to be clean, legal, and polite. The crowd-
sourcing rules are designed to avoid emotionally
triggering words as much as possible.
",no ethical concerns,no_ethical_concerns,transparency,2022,"statement, actions"
72,"At last, we discuss the potential ethic impacts of
this work: (1) The ESConv dataset is a publicly-
available, well-established benchmark for emo-
tional support conversation; (2) Privacy : The origi-
nal providers have filtered the sensitive information
such as personally identifiable information (Liu
et al., 2021); (3) Nevertheless, due to the limita-
tion of filtering coverage, the conversations might
still remain some languages that are emotionally
triggering. Note that our work focuses on building
emotional support conversational agents. For risky
situations such as self-harm-related conversations,
we do not claim any treatments or diagnosis.","Privacy, emotionally triggering language","Privacy, emotionally triggering language",misuse,2022,"statement, concerns"
73,"We expect that our proposed approach does not suf-
fer from ethical problems. The dataset we use in
our work is EmpatheticDialogues which is English-
based. The dataset is constructed by crowdsourcing
with Amazon Mechanical Turk, which protects pri-
vate user information (Rashkin et al., 2019). In
addition, the dialogue dataset is anticipated not
to have responses which include discrimination,
abuse, bias, etc, because the robust collection pro-
cedure of EmpatheticDialogues ensures the qual-
ity of the dataset. Thus, we expect that models
trained using the dataset, do not generate inappro-
priate responses which harm the users. However,
we inform that our model utilizes a pretrained lan-
guage model, which may produce inappropriateresponses. Lastly, we anticipate our model make
potential users be interested and consoled by gen-
erating empathetic responses.
",inappropriate responses,"no_ethical_concerns, offensiveness","bias, misuse",2022,"statement, concerns"
74,"This paper proposed methods to improve composi-
tional generalization in semantic parsing. While we
hope that improvements in compositional general-
ization would lead to systems that generalize better
to languages not well represented in small train-
ing sets, in this work we have only evaluated our
methods on semantic parsing datasets in English.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
75,"We collect all data from publicly available sources,
and respect copyrights for original document au-
thors. During the data annotation process, all hu-
man annotators are anonymized to respect their
privacy rights. We provide fair compensation to
all human annotators, where each annotator gets
paid more than the minimum wage and based on
the number of annotations they conducted.
Our work has no possible harms to fall dispro-
portionately on marginalized or vulnerable popu-
lations. Our dataset does not contain any identity
characteristics (e.g. gender, race, ethnicity), and
will not have ethical implications of categorizing
people.
",no ethical concerns,no_ethical_concerns,misuse,2022,actions
76,"This article contributes to compositional general-
ization research, a foundational concern for neural
natural natural language processing models. Break-
throughs in this research might eventually lead to
smaller, and more efﬁcient models, as well as bet-
ter performance on low-resource languages. The
ethical and societal consequences of these improve-
ments will depend on downstream applications.
The resource released in this work is a new set of
annotations for CFQ, an existing dataset. The origi-
nal CFQ dataset was artiﬁcially generated, so there
was no process of data collection and therefore no
ethics review process. The dataset was annotated
by the author, so there was no ethics review of the
annotation process or demographic information of
this population to report.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
77,"This paper uses computational tools to argue for
a theoretical position about idioms. Our idiom
dataset was automatically generated from an exist-
ing corpus, and so did not involve data collection
from human participants on our part. To validate
our conventionality measure, we conducted an ad-
ditional online experiment with crowdworkers on
Amazon Mechanical Turk, for which we obtained
REB approval. Details about the participants, re-
cruitment, and consent process are given in Sec-
tion 4. We note that one limitation of this work
is that it only investigates English idioms, poten-
tially contributing to an over-focus on English in
this domain.
",Misrepresentation,Misrepresentation (English-centric),no ethical concerns,2022,"statement, actions"
78,"We explain BERT-based classifiers using a con-
trolled subset of a large, fast-growing collection
of explanation methods available in the literature.
While replicating our experiments with different
approaches, or on different data samples, from dif-
ferent datasets or explaining different models, we
cannot exclude that some people may find the ex-
planations offensive or stereotypical. Further, re-
cent work has demonstrated gradient-based expla-
nations are manipulable (Wang et al., 2020), ques-
tioning the reliability of this widespread category
of methods.
We, therefore, advocate for responsible use of
this benchmarking suite (or any product derived
from it) and suggest pairing it with human-aided
evaluation. Moreover, we encourage users to con-
sider this work as a starting point for model debug-
ging (Nozza et al., 2022) and the included explana-
tion methods as baselines for future developments.
","Offensive explanations, stereotypical explanations","Offensive explanations, stereotypical explanations",no ethical concerns,2022,"statement, suggestions"
79,"The authors foresee no ethical concerns with the
work presented in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
80,"The authors foresee no ethical concerns with the
work presented in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
81,"This paper involves a user study to observe the
trade-off between reading and labeling costs for an-
notating coreference. The study has been approved
byIRBto collect data about human behavior. Any
personal information will be anonymized prior to
paper submission or publication. All participants
are fully aware of the labeling task and the infor-
mation that will be collected from them. They
are appropriately compensated for their labeling
efforts.
",no ethical concerns,no_ethical_concerns,privacy,2022,statement
82,"The surveys referred to in §3 were both approved
through to the university’s research ethics process,
where an independent committee assessed the setup
of the survey, the research’s potential harmful im-
pacts and the compensation for the participants.
In collecting data annotations, participants were
shown data from the MAGPIE corpus, available
under the CC-BY-4.0 License. All other informa-
tion shown to them was either collected from the
computational model, or written up by the authors.
Any identiﬁable information about the participants
was stored separately from the participants’ anno-
tations, for the purposes of compensation. Partic-
ipants were able to provide informed consent to
data collection and anonymised data being used in
academic publications. They were given the oppor-
tunity to withdraw at any time. Participants were
compensated above the minimum hourly wage of
the country in which they were a resident at the
time of participating in the study.",no ethical concerns,no_ethical_concerns,misuse,2022,"statement, actions"
83,"We collected our human evaluations using Amazon
Mechanical Turk. For the WEBNLG task, which
used a 7-point Likert scale, the workers were paid
0.03$ per item, in line with rates for similar tasks.
For the more demanding WSJ task, we paid 0.10$
per item. The payment for each task was set at
$7.5/hour (slightly above the US minimum wage,
i.e., $7.25/hour). We expected the amount to be a
fair remuneration, but given the actual time some
participants needed, their remuneration turned out
to be on the low side. In future crowd-sourcing
experiments, we will base our remuneration on
a more generous estimate of the duration per ex-
perimental task. We asked for demographic infor-
mation, age, gender and English proﬁciency level,
explicitly stating in the experiment that “Your in-
formation will be used for research purposes only.
All your data will be held anonymously ."" These
ﬁelds were not marked as mandatory ﬁelds. The
demographic information will not be made publicly
available.",low remuneration,low remuneration,no ethical concerns,2022,"statement, suggestions"
84,"We now discuss the three major ethical considera-
tions impacting this paper:
Hate-speech, Harmful Gender and Racial Bi-
ases: With general web-data potentially contain-
ing hate-speech and harmful gender and racial bi-
ases, we believe that our extracted dataset based
on the schema.org annotations is less impacted by
these issues, with the schema.org annotation repre-
senting a good proxy for high-quality, profession-
ally curated websites. As a result, we believe that
the severity of this issue is signiﬁcantly reduced.
Furthermore, in our human evaluation, we ﬁnd no
signs of the above mentioned biases. We leave com-
putational approaches to determine dataset biases
for future work (e.g., the Word Embedding Asso-
ciation Test (Caliskan et al., 2017) and Sentence
Encoder Association Test (May et al., 2019)).
Data Availability: We do not directly provide
the CCQA dataset, but enable third parties to gen-
erate the corpus through our published dataset gen-
eration scripts available at https://github.
com/facebookresearch/CCQA .
Hallucinations and Factual Errors: As shown
in the evaluation section, our model is able to gen-
erate reasonable answers for factoid and long-form
questions. The inferred answers are ﬂuent and
human-like, but may also contain hallucinations
and factual errors, especially for the challenging
closed-book question-answering task. Without a
guarantee of the predicted answers being factually
correct, they can potentially spread misinformation
if not properly corrected.
","Hate speech, gender bias, racial bias, hallucinations","Hate speech, gender bias, racial bias, hallucinations","bias, misuse",2022,concerns
85,"Propositions are direct manipulations of QA pairs
and thus reflect the subjective judgments of Vis-
Dial crowdworkers. Therefore, they are not per se
necessarily trueorfalse with respect to the image,
but with respect to A’s interpretation expressed as
answers. Inappropriate content on images, captions
and dialogues can be replicated by the rule-based
8We thank the reviewers for pointing out some of the limi-
tations discussed in this section.
proposition generation. To try to remedy this, we
filtered out dialogues containing words that could
be used for sensitive content. Despite our efforts,
we cannot guarantee that we could remove every-
thing, given the size of the dataset and the inherent
bias of how humans interpret images. As a result,
the only purpose of the propositions is performing
the evaluation as proposed here.",Inappropriate content,Inappropriate content,"bias, manipulation",2022,"statement, actions, concerns"
86,"Datasets Used: Both the datasets used in this work
had been completely anonymized before their re-
lease by the respective authors. Moreover, we care-
fully verified the licensing details and ensured that
the datasets were only used within the scope of
their intended usage.
We note that both datasets follow the multi-issue
structure where the priority order remains fixed
throughout the negotiation. Although this may not
be true for some real-world scenarios, as we noted
earlier, the underlying MIBT framework used by
these datasets has been extensively used in aca-
demic research and also in the industry, attesting
to the generalizability and applicability of this ap-
proach. Finally, we note that both the datasets
are in English. Although this means that our ex-
periments were limited to one language, our ap-
proach makes no such assumptions and should be
broadly applicable to other settings as well. We en-
courage researchers to extend this work and study
human-machine negotiations for other languages
as well. This would open up exciting avenues for
cross-culture research in this space, given the well-
documented differences in how humans negotiate
across cultures (Luo, 2008; Andersen et al., 2018).
Human Annotations: Human annotations were
used to estimate the expert performance on this task.
This did not involve any additional crowdsourcing
effort. Instead, the dialogues were annotated by an
author of this work.
Opponent Modeling For Negotiation Dialogues:
Negotiations are typically non-collaborative in na-
ture, where the goals of the negotiating parties
may not align with each other. Hence, the nego-
tiators may not always feel comfortable in reveal-
ing their preferences for fear of being exploited.
Even if they do, inferring them from natural lan-
guage is challenging as preferences might be im-
plied, and resolving these implications involves
domain-specific knowledge and prior dialogue con-
text. Regardless, incorporating such realistic com-
munication channels is critical for designing prac-
tical and robust AI systems for downstream ap-
plications. However, most of the prior efforts in
negotiations use restrictive menu-driven systems
based on button clicks. Our work is a step towards
bridging this gap.
This work is aligned with our broader goals for
building automated negotiation systems, trained
either in an end-to-end or a modular manner. For
conversational AI applications, opponent modeling
systems that can predict the priorities of the oppo-
nent reliably based on a partial dialogue can inform
the strategy of the agent in the latter parts of the
conversation. From the perspective of pedagogi-
cal applications, even the systems that can predict
the priorities of a negotiator at the end of the ne-
gotiation can be helpful. For instance, consider a
negotiation between two students, A and B who
are asked to guess the opponent’s priorities at the
end of their negotiation. If the pedagogical agent
is able to accurately guess the priorities of student
B, while student A fails to guess correctly, this can
be used to give concrete feedback to students who
fail to recognize these strategies.
Ethical Recommendations: Finally, we briefly
discuss the ethical considerations around the design
of automated negotiation systems. A considerable
amount of research in negotiations has focused on
ethics. Primary concerns revolve around the acts
of emotion manipulation, bias, deception, and mis-
interpretation (Lewicki et al., 2016). Consequently,
these issues can also emerge in the systems that are
developed on human-human negotiation dialogue
datasets. Our central recommendation in mitigating
the impact of these issues for negotiation dialogue
systems or other conversational AI assistants is
transparency - around the identity, capabilities, and
any known undesirable behaviors of the system.
Further, any data collected during the deployment
phase should be properly anonymized and the usersof the system should be well-informed. In particu-
lar, we recommend extra precautions for systems
that are adaptive towards their opponents or users
such as having regular monitoring for any unex-
pected behaviors, to ensure that the systems are not
offensive or discriminatory.","Manipulation, bias, deception, misrepresentation","Manipulation, bias, deception, misrepresentation","bias, transparency, manipulation",2022,"statement, actions, suggestions"
87,"The abstractive summarization dialogue system
proposed in this work can be applied to dialogue
scenarios. It can quickly process a lengthy dialogue
into a short content containing the core idea of the
dialogue. Such features can be applied to meet-
ings, customer service, and medical scenarios to
facilitate people’s life. The datasets SAMSum and
DialSum used in this work are publishable and for
research purposes only. There may be some biased
content in the datasets, which should be viewed
carefully.
",Bias,Bias,bias,2022,"statement, concerns"
88,"To address problems of the rerank-then-read frame-
work for open-domain multi-answer QA, we pro-
10The average inference time of JPR from Min et al. (2021)
is independent of its parameters given a ﬁxed number of en-
coded tokens and a ﬁxed decoder length, which can be esti-
mated with a randomly initialized JPR. The average inference
time of JPR’s reader was estimated with OPR’s reader.
pose a recall-then-verify framework that will hope-
fully beneﬁt information-seeking users with an en-
hanced ability of comprehensive exploitation of
evidence from a large-scale corpus. As our pre-
dictions are veriﬁed with textual knowledge, our
system itself would not raise new signiﬁcant ethical
concerns. All the datasets as well as the retrieval
corpus in our experiments have been widely used
for research purposes, and to our knowledge, do
not have any attached privacy and ethical issues.",no ethical concerns,no_ethical_concerns,misuse,2022,statement
89,"In this paper, we use only one dataset, namely Ba-
belSememe, which is completely free and publicly
available. The task we tackle, namely SPBS, is
only related to NLP research and not for practi-
cal application, thus cannot be misused by the or-
dinary people. To save energy, we use the base
version of XLM-R rather than larger cross-lingual
pre-trained models, although they may yield higher
performance. No demographic or identity charac-
teristics are used in the formation of this paper.
",no ethical concerns,no_ethical_concerns,misuse,2022,statement
90,"In this section, we discuss the ethical considera-
tions of this paper from four perspectives.
Dataset and Human Evaluation In terms of our
QuoteR dataset, all the quotes are collected from
free and open quote repository websites. Besides,
all the contexts are extracted from open corpora, in-
cluding free public domain e-books and other open
corpora. Therefore, there is no intellectual property
problem for the dataset. In addition, we conduct the
human evaluation by a reputable data annotation
company. The annotators are fairly compensated
by the company, based on the previous annotation
tasks. Further, we do not directly communicate
with the annotators, so that their privacy is well
preserved. Finally, the dataset and the human eval-
uation are not sensitive and thus do not need to be
approved by the institutional review board (IRB).
Application Quote recommendation is a practi-
cal task and our model can be put into service. In
actual use cases, users just need to input a query
context and our model should output a list of can-
didate quotes that fit the given context. All people
may benefit from our model during writing. If our
model fails, some inappropriate quotes that cannot
fit the query context would be output, but no one
would be harmed. There are indeed biases in the
dataset we build. Some quotes are very frequent
while the others are not, as illustrated in §6.6. The
infrequent quotes are less recommended and may
cause the failure of our model in some cases. In
terms of misuse, to the best of our knowledge, such
a quote recommendation model is hardly misused.
After the deployment of our model, the system
would not collect data from users. It does not have
any potential harm to vulnerable populations, ei-
ther.
Energy Saving To save energy, we use the base
version of BERT rather than larger pre-trained lan-
guage models, although the larger ones would prob-
ably yield better performance. Besides, as dis-
cussed in §5.2, we find that the simultaneous train-
ing of the context and quote encoders requires very
big memory and computation resources, and thus
we adopt the strategy of negative sampling in train-
ing.
Use of Identity Characteristics In this work, we
do not use any demographic or identity characteris-
tics information.
","bias, computational resources","bias, computational resources","bias, misuse, intellectual property",2022,"statement, concerns, actions"
91,"The business phone conversational data used for
entity linking experiments is annotated by the in-
house Scientists for which the annotations were
acquired for individual utterances. Whereas to an-
notate the conversation dataset to train our internal
NER model, Appen was used ( https://appen.
com/ ) for data annotation and the annotators were
provided with adequate compensation (above mini-
mum wages). There is a data retention policy avail-
able for all users so that data will not be collected
if the user is not consent to data collection. To pro-
tect user privacy, sensitive data such as personally
identifiable information (e.g., credit card number,
phone number) were removed while collecting the
data. Since our model is doing classification to
link the named entities to their corresponding en-
tries in a publicly available knowledge base for
information extraction, incorrect predictions will
not cause any harm to the user besides an unsat-
isfactory experience. We also maintain the licens-
ing requirements accordingly while using different
tools, such as Wikidata, WikiMapper, PyWikiBot,
Elasticsearch, HuggingFace, BLINK, etc.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
92,"Data . The conversational data is presented in the
form of individual utterances with sensitive data
such as personal identifiable information removed.
No crowdsourced annotation has been conducted,
and access to the data was available only to a small
number of in-house Scientists.
Use. The Purpose of Call feature is used by call
center managers to identify areas to coach sales and
support agents. It is recommended to not use this
feature for automated evaluation of agent perfor-
mance. Incorrect Purpose of Call prediction may
provide unsatisfactory user experience for the man-
agers as they sample calls but does not present anyrisk of negative impact for the agents.
Licensing . We follow the licensing require-
ments accordingly while using external tools such
as HuggingFace5and Multimodal-Toolkit (Gu and
Budhkar, 2021) libraries.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
93,"The goal of SENSEI is to provide a general-purpose
human values alignment framework for generative
LMs. Still, the generation of SENSEI can be af-
fected by certain biases from the LM it is based
on (Rae et al., 2021; Liu et al., 2021b), though
these biases may be partially mitigated by the align-
ment itself. Another major ethical consideration
is that SENSEI can mimic undesirable attributes
of the target alignment demonstrations that could
be non-contemporary and do not represent current
norms and practices (Liu et al., 2021d; Sheng et al.,
2019b)—and SENSEI has no scheme to diagnose
these problems. Furthermore, our experiments and
analysis are done in English, and therefore we do
not claim that our findings will generalize across
all languages and cultures, although our framework
has the potential to be extended to other languages
through necessary modifications.
",bias,bias,bias,2022,"statement, concerns"
94,"This paper presents and releases a news-based cul-
tural background prediction dataset. The dataset
is constructed on publicly available news outlets
using the public API of Media Cloud and the la-
bels are generated based on the country and district
codes of the media outlets. Thus, there is no sen-
sitive or private information in the dataset. Addi-
tionally, since we use mainstream news outlets for
our data collection we believe there is less risk of
overtly unethical information (though we cannot
be sure given the current sociopolitical climate).
Given the relatively large size of our dataset, we
cannot manually examine all articles, however, the
publicly released dataset will warn users of the
possibility of the dataset containing unethical in-
formation and will allows users to flag unethical
articles in our dataset. We also hired annotators
from MTurk to validate the quality of annotations
for a sample instances from our dataset. To en-
sure the quality of dataset validation, we require
the annotators to be native English speakers from
the same country or district as the label of each
instance to be validated. The annotators were given
clear instructions to choose the news paragraph(s)
written by journalists in their countries or districts
from a pair of paragraphs. We payed $0.14 (USD)
for validating each instance, which translates to
over $25 per hour since each data point takes no
more than 1 minute to validate. This hourly rate
is considerably higher than the federal minimum
wage in the US. The entire annotation process was
anonymized and the annotators were not asked for
their personally identifiable information, so there
was not any risk of harm associated with their par-
ticipation.
This paper presents one of the first attempts at
tailoring NLP models to the writing styles of spe-
cific regions, thus reducing the out-sized influence
of the linguistic style of larger countries in these
models.
",no ethical concerns,no_ethical_concerns,misuse,2022,"statement, actions"
95,"As far as we are aware, our proposed work does
not have any ethical considerations. However, our
work relies on pre-trained language models, which
have been shown to be biased in prior work (Liang
et al., 2021). As such, users of such models should
be aware of and if possible address such issues.
The data and the code for this work will be made
available to aid reproducibility.
",bias,bias,bias,2022,"statement, suggestions"
96,"As far as we are aware, our proposed work does
not have any explicit ethical concerns. However,
our work relies on pre-trained language models,
which have been shown to be biased in prior work
(Liang et al., 2021). As such, users of such models,
specially for sensitive applications, should be aware
of and if possible address such issues.
",bias,bias,bias,2022,"statement, suggestions"
97,"Social media has become a battleground for pro-
paganda and influence campaigns. This paper is
an attempt to provide a dataset and models for de-
tecting various propaganda techniques on Twitter
to aid with the fight against this scourge on so-
ciety. We release TWEET SPIN, a Twitter corpus
containing weak-annotations of fine-grained propa-
ganda. Consistent with Twitter TOS, TWEET SPIN
contains only tweet IDs (with code provided to hy-
drate them) and no identifying information. Given
the nature of the task the dataset contains poten-
tially offensive and hateful language which should
be taken into consideration. Additionally it is pos-
sible that our models, analyses, and dataset can
potentially be used to create more advanced and
harder to detect propaganda techniques. Though
we should be aware of this possibility it is impera-
tive that we in the research community stay ahead
of miscreants by actively pushing this field for-
ward. Finally, our models can lead to false positives
where a user is falsely accused of spreading pro-
paganda. Thus, it is important that the techniques
presented here be used as a part of a larger effort
to combat propaganda with humans in the loop for
checks and balances.
TWEET SPINwas validated using MTurk. The
annotators were paid 0.08USD per task which took
on average 30 seconds, for an hourly rate of 9.6
USD, above the federal and our state’s minimum
wage.
","offensive language, hateful language, propaganda, ","offensive language, hateful language, propaganda, ",no ethical concerns,2022,"suggestions, concerns"
98,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
Annotators were given contracts and paid fairly in
line with University payscales. They were alerted
about potentially encountering disturbing content
and were advised to take breaks. The annotations
are used to train and evaluate natural language pro-
cessing models for recognising moments of change
and linking them to suicidality risk, where the latter
is provided by clinical psychology experts. Work-
ing with data on online platforms where individuals
disclose personal information involves ethical con-
siderations (Mao et al., 2011; Keküllüo ˘glu et al.,
2020). Such considerations include careful anal-
ysis and data sharing policies to protect sensitive
personal information. Potential risks from the ap-
plication of NLP models in being able to identify
moments of change in individuals’ timelines are
akin to those in earlier work on personal event iden-
tification from social media and the detection of
suicidal ideation. Potential mitigation strategies
include restricting access to the code base and an-
notation labels used for evaluation. In this shared
task we have asked participants to sign DUA agree-
ments and we opted for a secure data enclave envi-
ronment to work in.
",privacy,privacy,privacy,2022,"actions, concerns"
99,"This study was approved by an Institutional Review
Board and was conducted ethically in accordancewith the World Medical Association Declaration of
Helsinki. The procedures were part of the routine
assessment and monitoring process in the clinic. In-
formed written consent was obtained from all par-
ticipants at the outset of this study. Participants are
asked to provide written consent that their data will
be used for research. They are informed that at any
time they may request to terminate their participa-
tion in the research and / or request that the content
of the recordings be deleted without jeopardizing
treatment. All data collected was anonymized and
only then exposed to a very small number of re-
searchers, as agreed upon by the participants. More
information is avaialbale in Appendix A.1.
The procedures were part of the routine assessment
and monitoring process in the clinic. All research
materials were collected after securing the approval
of the authors’ university ethics committee. Only
clients that gave their consent to participate were
included in the study. Clients were told that they
could choose to terminate their participation in the
study at any time without jeopardizing treatment.
The clients completed the ORS before each ther-
apy session and the WAI after each session. The
therapist completed the WAI after each therapy ses-
sion. The sessions were audiotaped and transcribed
according to a protocol described above. All data
collected was anonymized and only then exposed
to a very small number of researchers, as agreed
upon by the participants.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
100,"Gender bias mitigation has attracted a lot of at-
tention as a practical and socially important field
of study. This paper contributes to this effort by
studying the internal organization of gender rep-
resentations. We note that gender and bias are
complicated and multi-faceted constructs. When
studying gender bias in neural models, we unavoid-
ably rely on a narrow notion of binary gender, as
reflected in several annotated datasets. As such, we
see this study as a preliminary attempt that is based
on a relatively narrow concept of gender, that does
not reflect the subtle ways by which gender bias is
manifested. We advise for caution when applying
the conclusions of this study to other notions of
gender or different definitions of bias.
We acknowledge that gender is not a binary prop-
erty. Due to lack of existing resources, we use bi-
nary gender as a rough approximation of reality.
We hope to account for this in future work.
",bias,bias,bias,2022,"statement, suggestions"
101,"The PubHealthTab dataset can be used for develop-
ing and evaluating fact checking systems intended
for a real-world context. The labels supports ,re-
futes andnot enough information describe a claim’s
veracity given the evidence table. We do not make
any statement on PubHealthTab claims’ truthful-
ness in a real-world context.
We obtained ethical clearance prior to crowd-
sourcing from the relevant authority in the aca-
demic institution. We informed the participants
about the data being collected and its purpose. Par-
ticipants had the opportunity to withdraw at any
time and to provide feedback at the end of each task.
All workers were from English speaking countries.
The payment was above the minimum wage and
decided based on the time workers spent on the
pilot tasks. For the first and third tasks we paid
0.75USD (2.5minutes per task on average) and
for the second 1.35USD (average 5minutes per
task).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
102,"This work presents M3ED, free and open dataset for
the research community to study the multimodal
emotion recognition in dialogues. Data in M3ED
are collected from TV series in Chinese. To ensure
that crowd workers were fairly compensated, we
paid them at an hourly rate of 40 yuan ($6.25 USD)
per hour, which is a fair and reasonable hourly
wage in Beijing. First, to select high-quality dia-
logues from 56 TV-series, we recruited 12 Chinese
college students (5 males and 7 females). Each
student was paid 100 yuan ($15.625 USD) for se-
lecting about 18 dialogues from each TV series.
To annotate the emotional status of the selected
dialogues, we recruited 14 Chinese college stu-
dents (6 males and 8 females). Each student was
paid 200 yuan ($31.25 USD) for annotating about
18 dialogues from each TV series with emotion
labels, text correction, speaker, gender, and age
information. If only the emotion labels were anno-
tated, the payment for each TV series was 100 yuan
($15.625 USD). Considering the copy-right issue
of TV-series, we will only release the name list
of the TV-series and our annotations. To facilitate
future comparison research on this dataset, we will
provide our extracted visual expression features
and acoustic features. We anticipate that the high-
quality and rich annotation labels in the dataset will
advance research in multimodal emotion recogni-
tion.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
103,"Gender Rewriting Our underlying intention of
developing a gender rewriting model for Arabic
is to increase the inclusiveness of NLP applica-
tions that deal with gender-marking morphologi-
cally rich languages. Our work aims at empowering
and allowing users to interact with NLP technol-
ogy in a way that is consistent with their social
identities. We acknowledge that by limiting the
choice of gender expressions to the grammatical
gender choices in Arabic, we exclude other alter-
natives such as non-binary gender or no-gender
expressions. However, we are not aware of any
sociolinguistics published research that discusses
such alternatives for Arabic. We stress on the im-
portance of adapting Arabic NLP models to new
gender alternative forms as they emerge as part
of the language usage. We further recognize the
limitations of the gender identification component
we use as part of our multi-step gender rewriting
model as it relies on a language model that was
pretrained on a large monolingual Arabic corpus,
which could possibly contain biased text. We re-
alize the potential risks of our proposed gender
rewriting model if it is intentionally maliciously
misused to produce gender alternatives that do not
match the target users’ gender preferences.
Data We use the publicly available Arabic Paral-
lel Gender Corpus ( APGC ).7It is subject to its cre-
ators’ own Copy Rights and User Agreement and
we strictly adhere to its intended usage. It is worth noting that APGC does not contain any hetero-
centric assumptions as part of its annotations (e.g.,
the word my husband is labeled as gender-
ambiguous (B)). Moreover, all proper names are
labeled as B, even when they have strong gender-
specific association (Alhafni et al., 2022). The
Arabic data we use for our data augmentation ex-
periments was randomly sampled from OpenSub-
titles 2018 (Lison and Tiedemann, 2016), which
is publicly available.8OpenSubtitles is distributed
under a Creative Commons license.9
",misrepresentation ,misrepresentation ,"bias, misuse",2022,"statement, actions, concerns, suggestions"
104,"Any ZEL technique, like ours, that advances the
SOTA with less training data is a boon for appli-
cations such as KB question answering, document
understanding, dialogue systems, etc. Our solution,
however, comes with its own limitations and risks
as follows. 1) Our assumption of entity types com-
ing from the same hierarchy for both train and test
domains is not always true in practice. Moreover, if
there is no entity type information available in the
first place, then our approach can not even be used.
2) The sensitivity of our predictions with respect
to mild perturbation in input text could be a risk
factor while deploying it in real-life applications.
It warrants a rigorous study that we leave as future
work.
We use publicly available datasets for training
and testing our models. The Wiki BLINK and
CoNLL-YAGO datasets are available under the
MIT and CC-BY-3.0 licenses respectively. For the
other datasets mentioned in Table 2, we obtained
permissions from the authors (Onoe and Durrett,
2020) as the license was not explicitly mentioned
on their Github. All of these datasets were con-
structed using publicly available sources such as
Wikipedia for the purpose of developing entity link-
ing systems and they are being used as intended.
We believe these datasets do not contain any infor-
mation that is offensive or uniquely identifies any
individual.
","uncertainty, sensitive predictions","uncertainty, sensitive predictions",no ethical concerns,2022,"statement, concerns, suggestions, actions"
105,"Any use of a language model for classiﬁcation in-
volves some risk of bias, which stems from the
pre-training and training data used to construct
the model. Here we aim to improve the language
model representations by relying on clustering of
data from the target domain. We have no reason
to believe this process would introduce bias be-
yond the potential bias that can occur whenever
ﬁne-tuning a model, but this is a potential risk, as
we did not verify this directly.
",bias,bias,bias,2022,"concerns, statement"
106,"We follow Bender and Friedman (2018) regarding
professional practice for NLP technology and ad-
dress ethical issues that result from the use of data
in the development of the models in our work.
Pre-Training Data. The two initial data sources
we used to pre-train the language models are Os-
car and Wikipedia. In using the Wikipedia and
Oscar we followed standard language model train-
ing efforts, such as BERT and RoBERTa (Devlin
et al., 2019; Liu et al., 2019). We use the language-
specific Oscar data according to the terms specified
in Ortiz Suárez et al. (2020) and we extract texts
from language-specific Wikipedia dumps. On top
of that, a big portion of the data used to train Aleph-
BERT originates from the Twitter sample stream.12
As shown in Table 2, this data set includes 70M
Hebrew tweets which were collected over a pe-
riod of 4 years (2014 to 2018). We acknowledge
the potential concerns inherently associated with
Twitter data (population bias, behavior patterns,
bot masquerading as humans etc.) and note that we
have not made any explicit attempt to identify these
cases. We only used the text field of the tweets and
completely discard any other information included
 the stream (such as identities, followers, struc-
ture of threads, date of publication, etc). We have
not made any effort to identify or filter out any
samples based on user properties such as age, gen-
der and location nor have we made any effort to
identify content characteristics such as genre or
topic. To reduce exposure of private information
we cleaned up all user mentions and URLs from
the text. Honoring ethical and legal constraints we
have not manually analyzed nor published this data
source. While the free-form language expressed
in tweets might differ significantly from the text
found in Oscar/Wikipedia, the sheer volume of
tweets helps us close the substantial resource gap.
Training and Evaluation Benchmarks. The
SPMRL (Seddah et al., 2013) and UD (Sadde et al.,
2018) datasets we used for evaluating segmentation,
tagging and parsing, were used to both train our
morphological extraction model as well as provide
us with the test data to evaluate on morphological
level tasks. Both datasets are publicly available and
widely used in research and industry.
The NEMO corpus (Bareket and Tsarfaty, 2020)
used to train and evaluate word and morpheme
level NER is an extension of the SPMRL dataset
augmented with entities and follows the same li-
cense terms. The BMC dataset used for training
and evaluating word-level NER was created and
published by Ben Mordecai and Elhadad (2005)
and it is publicly available for NER evaluation.
We used the sentiment analysis dataset of Am-
ram et al. (2018) for training and evaluating Ale-
phBERT on a sentence level task, and we follow
their terms of use. As mentioned, this dataset had
some flows, and we describe carefully the steps
we’ve taken to fix them before using this corpus in
our experiments for internal evaluation purposes.
We make our in-house cleaning scripts and split
information publicly available.
","Bias, privacy","Bias, privacy",bias,2022,"concerns, actions, statement"
107,"Our tutorial takes a human-centered perspective.
We hope that our tutorial will broaden the scope
of evaluations in NLP by introducing perspectives
from HCI and psychology. This may help allevi-
ate ethical concerns of NLP models in the long
run by incorporating human perspectives into the
development and evaluation process.
8 Special Themes
Our tutorial is alighed with the special theme of
NAACL 2022, human-centered natural language
processing.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions, concerns"
108,"Work on personalized LMs could be used for
surveillance by detecting language from individ-
uals or groups (Stamatatos, 2009). We recommend
against such applications, as they threaten intel-
lectual freedom and risk discrimination (Richards,
2013). There may be a risk in storing private datanecessary to construct these models, as data may
not be properly secured or used. Furthermore, a
personalized model could reinforce incorrect lan-
guage usage, which may be an issue for individ-
uals learning to speak a new language, making it
more difficult to learn. Learning personal language
patterns in a given context and suggesting these
patterns in other contexts may lead to potentially
incorrect or offensive results and we recommend
that if this type of personalization is deemed appro-
priate, users are made aware of how their data is
being used and potential consequences.
9 Conclusions
In this paper, we addressed the issue of language
modeling in a low data setting where a new user
may not have enough data to train a personalized
LM and presented a novel approach that lever-
ages data from similar users. We considered three
similarity metrics and two methods of leveraging
data from similar anchor users to improve the per-
formance of language modeling over a standard
fine-tuning baseline, and showed how our results
vary with the amount of data available for anchor
users and the number of available anchor users.
We found that the most easily scalable and high-
est performing method was to use user embedding
similarity and to interpolate similar user fine-tuned
models. Additionally, we provided an analysis of
the kind of words that our personalized models
are able to more accurately predict and further dis-
cussed limitations of our methods.
","surveillance, inaccuracies, offensive content","surveillance, inaccuracies, offensive content",bias,2022,"concerns, suggestions, actions"
109,"Our models are pre-tuned on Wizard of Wikipedia
dataset and ﬁne-tuned on three corpora: Wikipedia ,
CNN-DailyMail andPaper-Abstracts (Abstracts
of papers from ACL, EMNLP, NAACL, EACL,
Findings and ICLR submissions from 2017 to
2021). All the datasets used in this paper are
publicly available. Moreover, we did not use
full-length Wikipedia or CNN-Daily Mail news
articles in our experiments, but tailored versions
of 100-150 words. This is because a full length
Wikipedia/CNN-Daily Mail article may contain too
much content to be covered in a short conversation.As described in (Maynez et al., 2020; Kryscin-
ski et al., 2020; Lebanoff et al., 2020; Zhou et al.,
2021), current state of the art neural conditional text
models can output hallucinated content unfaithfully
to the input text, which impedes the safe deploy-
ment of the models. We note that our Teacher bots
may also generate utterances that are not supported
by the input passage.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, concerns"
110,"To the best of our knowledge, the data used in
our work does not contain sensitive information.
Although our models are evaluated on academic
datasets in this paper, they could also be used in
sensitive contexts, e.g. healthcare or legal scenarios.
It is essential that necessary anonymization and
robustness evaluation is undertaken before using
our models in these settings.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"suggestions, concerns"
111,"We honor the ACL Code of Ethics. No private data
or non-public information was used in this work.
All annotators have received labor fees correspond-
ing to the amount of their annotated instances.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
112,"The responsible administrative board of Paderborn
University formally approved our external study.
We did not gather any personal information about
the participants that could connect their ideologies
to their identity. Accordingly, no sensitive informa-
tion was sent via the Project Debater API. Also, we
ensured that they get paid more than the minimum
wage in the U.S., namely 75$ for a workload of 3
to 3.5 hours. Once more, we would like to indicate
here that the results of this paper should be con-
sidered preliminary due to the limited number of
surveyed users. Budget constraints did not allow to
increase this number.
Working on technologies that aim to persuade an
audience raises ethical concerns. Using knowledge
about a user’s morals is a critical act, and if done, it
should be clearly communicated to the users. Simi-
larly, generating arguments aiming to convince the
audience could be thought of as a manipulation
attempt. To avoid manipulation, any technology
aiming at convincing the audience should be trans-
parent about it by keeping the user informed of how
their information is being used. Since we rely on
the arguments provided by the Project Debater API,
we ultimately cannot control their content, but our
internal quality assessment study did not raise any
notable concerns on the information contained.
As mentioned, the long-term goal of our envi-
sioned system is not to use the audience’s morals
to generate arguments that convince them. Rather,
given two disagreeing parties, the goal is to gen-
erate a wider spectrum of arguments covering rel-
evant morals for both parties. We have argued in
this paper that such morally rich arguments are
more effective and could better achieve agreement
between the disagreeing parties.",no ethical concerns,no_ethical_concerns,"misuse, manipulation",2022,"statement, concerns, suggestions"
113,"This paper proposes a general approach to fuse
language models and external knowledge graphs
for commonsense reasoning. We worked within
the purview of acceptable privacy practices and
strictly followed the data usage policy. In all the
experiments, we use public datasets and consist
of their intended use. We neither introduce any
social/ethical bias to the model nor amplify any
bias in the data, so we do not foresee any direct
social consequences or ethical issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
114,"This paper proposes a general model for question
matching. This paper neither introduces any so-
cial/ethical bias to the model nor amplify any bias
in the data. In all the experiments, we use public
datasets and consist their intended use. We build
our algorithms using public code bases (PyTorch).
We do not foresee any direct social consequences
or ethical issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
115,"SEQZERO is a general framework for few-shot se-
mantic parsing on text, such as search queries. SE-
QZERO neither introduces any social/ethical bias tothe model nor amplify any bias in the data. When
creating EcommerceQuery dataset, we collected
data on an E-commerce search platform without
knowing customers’ identity. No customer/seller
specific-data is disclosed. We build our algorithms
using public code bases (PyTorch and FairSeq). We
do not foresee any direct social consequences or
ethical issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
116,"In this work, we used Mturk platform to collect
annotations for the dataset. Crowd workers on
Mturk are known to be underpaid according to
western standards, earning a median hourly wage
of only ∼$2/h (Kaufmann et al., 2011). At the
same time, monetary remuneration is not the only
factor defining people’s motivation to work on such
crowdsourcing platforms (Hara et al., 2018). For
example, workers might also engage with HITs to
learn new or train existing skills, pass free time,
or meet new people. Taking these factors into ac-
count, we designed our annotation experiments so
that workers received ∼$6/h on average to achieve
reasonable trade-off between the number of HITs
we could launch with the available budget and the
offered payment. While being slightly lower than
the US minimum wage ($7.25), it was deemed a
fair compensation given that it is three times higher
than the reported median wage and workers could
have other reasons to complete the tasks than purely
monetary reward. Nevertheless, we encourage fu-
ture works of similar nature to offer higher com-
pensation to the workers if possible.
",Low compensation,Low compensation,no ethical concerns,2022,"concerns, suggestions"
117,"Our research aims to beneﬁt the efforts in deliver-
ing truly multilingual language technology also to
under-resourced languages and cultures via bridg-
ing the lexical gap between languages, groups
and cultures. As a key task in cross-lingual NLP,
bilingual lexicon induction or word translation has
broad applications in, e.g., machine translation,
language acquisition and potentially protecting en-
dangered languages. Furthermore, compared with
many previous studies, we stress the importance of
diversity in the sense that our experiments cover
various language families and include six lower-
resource languages from the PanLex-BLI dataset.
Hoping that our work can contribute to extend-
ing modern NLP techniques to lower-resource and
under-represented languages, we focus on semi-
supervised settings and achieve signiﬁcant improve-
ments with self-learning techniques.
The two BLI datasets we use are both publicly
available. To our best knowledge, the data (i.e.,
word translation pairs) do not contain any sensitive
information and have no foreseeable risk.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
118,"We discuss ethical considerations and limitations
of our work. First, we focus solely on binary gen-
der, as this can be directly observed in many lan-
guages with grammatical gender. Our use of binary
gender is not intended to promulgate an inappro-
priate binary gender focus, but rather allows the
study of gender bias in translation, based on the
text contained in translation corpora. Admittedly
our method has limitations, for instance, it will not
be able to adequately handle trans-gendered and
non-binary individuals; to do so would require sub-
stantial additional translation corpora, as well as
extensions to the technique, which we leave for
future research. Second, we evaluate only a small
number of language pairs, but we expect similar
behaviour for translation into many other gendered
languages, the exploration of which we leave for
future work.
",gender bias,"gender misrepresentation, small sample size",misrepresentation,2022,concerns
119,"Intellectual Property. CraftQA contains question
answer pairs generated from copyright free tutori-
als found online3. All of the tutorials are licensed
with the Creative Commons license4which helps
share knowledge and creativity for common use.
The collection of CraftQA is in accordance with
the Terms of Service of Instructables as follows: by
posting, providing, uploading, submitting, sharing,
publishing, distributing, making available or allow-
ing others to access and/or use Your Content to or
through the Service You are solely responsible and
liable for the consequences of doing so and you
acknowledge and agree that Your Content can and
may be viewed worldwide5.
We also construct experimental evaluations on
the RecipeQA dataset. Referring to the ofﬁcial
dataset descriptions of RecipeQA6, Legal and Ethi-
cal Considerations had been taken into account dur-
ing the construction of RecipeQA. We have cited
the corresponding papers in this study.
3https://www.instructables.com
4https://creativecommons.org/licenses/by-nc-sa/4.0
5https://www.autodesk.com/company/legal-notices-
trademarks/terms-of-service-autodesk360-web-
services/instructables-terms-of-service-june-5-2013
6https://hucvl.github.io/recipeqa/recipeqa-datasheet.pdfPrivacy. According to the Privacy Statement of
Instructables7, users can choose whether or not to
expose their information when publishing tutorials.
Respecting personal privacy, we have removed all
of the personal information of users from CraftQA
and promise CraftQA isn’t involved with any
privacy issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
120,"Data Collection. We collected publicly available
data and removed all personal information (phone,
email, postcode, location, and any other privacy
information). Any potentially sensitive dialogues
were completely removed from our data. No treat-
ment recommendations or diagnostic claims were
given in this study.
This research is approved and monitored by the
University’s Institutional Review Board and per-
formed in accordance with the principle of GDPR
(General Data Protection Regulation2) as follows:
data processing shall be lawful if it is necessary for
the performance of a task carried out in the public
interest. Additionally, this study is explored not
for any commercial use while merely for scientiﬁc
2https://gdpr-info.eu/.
purpose and public interest, which are safeguarded
by the Art. 89 GDPR.
Annotator Compensation. We resorted to the
Amazon Mechanical Turk crowdsourcing platform
to evaluate three artiﬁcial indicators (i.e., Empathy,
Relevance, and Fluency). The crowdworkers were
assessed with 20 random sentences, which aver-
agely took 5-6 minutes to accomplish, and com-
pensated with $0.8 per HIT (Human Intelligence
Task). The compensation was determined based on
the US minimum wage of $7.12 per hour.
Potential Misuse. Our model is less likely to
contribute to depression of users or generate
non-empathic expressions (e.g., discrimination,
criticism, and antagonism), since the model is
based on the assumption that everyone has varying
degrees of sensibility and empathy. Additionally,
this model removes any sensitive information
of users, and it is basically impossible to infer
their personalities, preferences, interests, or other
private information from the generated dialogues.",no ethical concerns,no_ethical_concerns,"privacy, misuse",2022,"statement, actions"
121,"To model persuasion and empathy we used publicly
available datasets. We adhered to the policies of
used datasets without harming any copyright issues.
Dataset used for empathetic persuasion is publicly
available persuasion dataset annotated with emo-
tions without manipulating or changing the content
of any utterance in dialogues. We will make empa-
thetic persuasive data available only with an official
agreement that data will be used only for research
works. The dataset is annotated with human experts
by our in-house regular employees in the research
group and they are paid at par with the university
norms. We have also got our data annotation pro-
cess verified by our university review board. It is
also to be noted that a similar annotation scheme
could be used for coercion, manipulation, or other
amoral activities. Further, it may persuade people
to draw inconsistent conclusions with those that
they would have reached by exercising their full
judgement (Garsten, 2009). Therefore, to develop
a persuasive conversational AI an ethical intention
must be taken into account. In this work, we choose
a simple task of persuading for donation to Save
the Children connecting with the end-users empa-
thetically. We tried here to build a ’well speaking
dialogue agent for social good’ so that the society
may benefit at large by reaching to a large number
of users for persuasion in a very less time. Lastly,
generative models may lead to uninformative utter-
ances due to absence of world knowledge, hence,
it is required to model knowledge grounding or
fact-verification. This study we will employ in our
future work.
9
","Coercion, manipulation, misinformation","Coercion, manipulation, misinformation","misuse, manipulation, misinformation",2022,"actions, statements"
122,"All authors must warrant that increased model per-
formance for non-standard varieties such as un-
derrepresented dialects, non-standard spellings or
lexical items in NLP systems can potentially en-
able automated discrimination. In this work, we
solely attempt to highlight the need for dialectal
inclusivity for the development of impactful speech
and language technologies in the future, and do not
intend for increased feelings of marginalization of
an already stigmatized community.
8
",Discrimination ,Discrimination ,no ethical concerns,2022,"statement, concerns"
123,"Any TST model can be used for nefarious pur-
poses, e.g. performing a ""Non-toxic to toxic""
modification of text in a real-world setting and
causing social harm. Therefore, it is important
we keep in mind a code of ethics (e.g. https:
//www.acm.org/code-of-ethics ) for usage, re-
search and development in this type of research.
We have made all our code open-source and pro-
vided all details of experimentation and implemen-
tation to the best of our knowledge.","toxic content, social harm","Toxic text, social harm","misuse, harmfulness",2022,"concerns, actions"
124,"The collection of our SSD dataset is consistent with
the terms of use of any sources and the original au-
thors’ intellectual property and privacy rights. The
SSD dataset is collected with ALIDUTY1platform,
and each HIT requires up to 10 minutes to com-
plete. The requested inputs are general language
variations, speech voices, and no privacy-related in-
formation is collected during data collection. Each
HIT was paid 0.1-0.2 USD for a single turn dia-
log data, which is higher than the minimum wage
requirements in our area. The platform also hires
professional reviewers to review all the collected
data to ensure no ethical concerns e.g., toxic lan-
guage and hate speech.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
125,"The persona-based dialogue generation task aims
to build a dialogue model which generates mean-
ingful, ﬂuent, and consistent responses. It will
facilitate human-computer interactions in practice.
However, the training of the model for personalized
dataset. Colored texts indicate that the responses match
the personality descriptions.
dialogues may lead to the leakage of personal pri-
vacy information. In this work, the data source we
use is from a published dataset and does not involve
privacy issues for the data collection. Our proposed
method does not include inference or judgments
about individuals and does not generate any dis-
criminatory, insulting responses. Our work vali-
dates the proposed method and baseline models
on human evaluation which involves manual la-
bor. We hire ﬁve annotators to score 750 generated
sentences in total (250 sentences for each model
we evaluate). The hourly pay is set to 15 US$
per person, which is higher than the local statutory
minimum wage.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
126,"We utilize two publicly available datasets - TDT
and TDG for evaluating temporal dependency
parser. We also curated dataset for TDG on contract
documents. We source these contract documents
from a publicly available resource - ATTICUS. We
repurpose the document in this dataset for our task
and provide new annotations. ContractTDG dataset
does not violate any privacy as these documents
are already in public domain. There is no human
bias involved in such documents as they are busi-
ness contracts filed on the SEC website. These
documents do not restrict reuse for academic pur-
poses and any personal information was already
redacted before their original release. All docu-
ments and our experiments are restricted to English
language. Temporal NLI and TimeQA datasets that
are publicly available for research purposes. The
crowd workers are paid a fair wage. There was no
sensitive data involved in the studies.",no_ethical_concerns,English-centric,"privacy, bias",2022,statement
127,"While the multi-level human-document-word
structure within H ULM can enable bias correcting
and fairness techniques (discussed next), the abil-
ity to better model language in its human context
also presents opportunities for unintended harms
or nefarious exploitation. For example, mod-
els that improve psychological assessment are not
only useful for research and clinical applications,
but could be used to target content for individuals
without their awareness or consent. In the con-
text of use for psychological research, such mod-
els may risk release of private research partici-
pant information if trained on private data without
checks for exposure of identifying information. To
negate this potential, we only release a version of
HaRT that is without training on the consented-
use private Facebook data until differential privacy
standards can be veriﬁed. Unlike other human-
centered approaches, HaRT is not directly fed user
attributes as part of the pre-training thus the model
parameters do not directly encode user attributes.
HULM aims to join a growing body of work to
make AI more human-centered, and thus more ap-
plicable for interdisciplinary study of the human
condition as well as leading to new clinical tools
for psychological health. At this point, our mod-
els are not intended to be used in practice for men-
tal health care nor labeling of individuals publicly
with personality or age scores. While modeling
the human state presents opportunities for reduc-
ing AI bias, prior to clinical or applied use, such
models should be evaluated for failure modes such
as error across target populations for error or out-
come disparities (Shah et al., 2020). All user-level
tasks presented here were reviewed and approved
or exempted by an academic institutional review
board (IRB).
9 ","Bias, exploitation",Bias,"harms, exploitation, bias, misuse",2022,"concerns, actions"
128,"We used publicly available data stripped of iden-
tifiable information which was collected in a non-
intrusive manner for mental health research. Secure
access to the shared task dataset was provided with
IRB approval under University of Maryland, Col-
lege Park protocol 1642625 and approval by the
Biomedical and Scientific Research Ethics Commit-
tee (BSREC) at the University of Warwick (ethical
application reference BSREC 40/19-20). Individ-
uals of the study team who ran the analyses for
this work are certified to conduct Human Subject
Research and complied with the non-disclosure
agreement signed with the dataset providers.
The findings of this work are intended for fel-
low researchers in Computational Linguistics and
Psychology to improve technology for mental
health assessments. Around 14 million adults
in the United States face severe mental health
issues (NIMH, 2022) and a very large part of
this is marginalized communities that are under-
served (Saraceno et al., 2007). However, given
the prevalence of these communities in social me-
dia (Center, 2021), technology-enabled solutions
can assist in detecting and providing assistance in
a timely manner to a more diverse group of indi-
viduals. This work is a part of the growing body
of mental health research aimed at applications for
improving well-being. However, this shouldn’t be
deployed to use without collaboration of clinical
practitioners.
",Misuse,Misuse,no ethical concerns,2022,"statement, suggestions"
129,"This work primarily investigates how external com-
monsense knowledge graphs (CSKGs) enhance the
commonsense reasoning capacity of pre-trained
models (PTMs) and proposes a simple but effec-
tive KG encoder on CSKGs to enhance PTMs. A
potential problem derives from using PTMs and
CSKGs in our approach. PTMs have been shown
to capture certain biases from the data that have
been pre-trained on (Bender et al., 2021). And
existing works (Mehrabi et al., 2021) have found
that CSKGs are likely to contain biased concepts
derived from human annotations. However, a com-
prehensive analysis of such biases is outside of the
scope of this work. It is a compelling direction
to investigate to what extent the combination of
CSKGs and PTMs can help mitigate such biases.
An alternative consideration is to consider filtering
biased concepts in the process of subgraph extrac-
tion from the CSKG. By devising proper rules, it
is promising to reduce the influence of biased con-
cepts on our approach.
9
",bias,bias,bias,2022,"statement, suggestions"
130,"In this section, we discuss the ethical considera-
tions of this work from the following two aspects.
First, for intellectual property protection, the code,
data and pre-trained models adopted from previ-
ous works are granted for research-purpose usage.
Second, since PLMs have been shown to capture
certain biases from the data they have been pre-
trained on (Bender et al., 2021), there is a potential
problem about biases that are from the use of PLMs
in our approach. There are increasing efforts to ad-
dress this problem in the community (Ross et al.,
2020).
",bias,bias,"bias, intellectual property",2022,concerns
131,"In this part, we discuss the main ethical considera-
tion of this work: (1) Privacy. The data adopted in
this work ( i.e.,pre-training corpus and fine-tuning
data) is created by human annotation for research
purposes, and should not cause privacy issues. (2)
Potential Problems. PLMs have been shown to cap-
ture certain biases from their pre-trained data (Ben-
der et al., 2021). There are increasing efforts to
address this problem in the community (Ross et al.,
2021).
",bias,bias,bias,2022,concerns
132,"We are fully aware that controllable generation
technology has a potential to produce offensive
and harmful text when maliciously used. However,
it is also a powerful weapon for generating diverse
contents, combating hate speech, and eliminating
harmful information in pretrained language models.
We believe it meaningful and beneficial for us to
advance research on controllable text generation.
","offensive text, harmful text","offensive text, harmful text",misuse,2022,"statement, concerns"
133,"Our work has received approval from the Ethics
Committee of the Department of Computer Science
at the University of Sheffield (No 037572) and
complies with Twitter’s data policy for research.9
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
134,"•The sentence - commonsense dataset created
to train CoSe-Co has been derived using stan-
dardized Wikipedia Corpus and ConceptNet
knowledge graph which are publicly available
and commonly used without containing any
info/text that could potentially lead to risk im-
pacts.
•We have used open source Wikipedia corpus
and ConceptNet which are publicly available
and already standardized for research works.
•The links to all the previous works, their
provided open-source github repos, arti-
facts and datasets have been provided in
appropriate sections where they are dis-
cussed/used/compared along with their cita-
tions (Sections - 2, 4, Appendix E etc.). The
links to any resources used provide permis-
sions to use them for our research work.1143
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
135,"The presented paper introduces a framework to
evaluate compositionality of image captioning mod-
els from multiple perspectives. The dataset and
the model under evaluation are publicly available
for academic purposes and not intended for down-
stream deployment.
Despite recent advances, our findings challenge
the systematicity and productivity of current mod-
els. This suggests that the generalization capacity
and robustness remain a barrier to overcome, be-
fore exposing the outputs of these models to end
users. As a result, we believe that comprehensive
evaluations can help expose biases in the model
and minimize the impact in real-world deployment
of language technologies.
","generalisability, robustness","generalisability, robustness",bias,2022,"statement, suggestions"
136,"AI-based decision systems interact with humans in
many application domains, including sensitive ones
like credit-worthiness, education and law enforce-
ment. An unmitigated data-driven decision-making
algorithm can systematically make unfair decisions
against certain population subgroups with specific
attributes (e.g. race or gender) due to the inherited
biases encoded in the data. Even a system which
has been carefully trained in order to mitigate such
effects can change its behaviour over time, due to
changes in the underlying data. The opaque nature
of machine learning models can hide those unfair
behaviours to the end user.
In this context, ContrXT might reveal itself
extremely useful in tracing and explaining how
the model, which was designed to be fair at time
1, changed its behaviour and rules after being re-
trained at time 2. This allows one to check whether
the model kept fair over time.
An interesting example of application in such
sense is the paper Towards Fairness Through
Time (Castelnovo et al., 2021) , presented at the
2nd Workshop on Bias and Fairness in AI (BIAS)6
at ECML-PKDD, which uses ContrXT to observe
the evolution of a ML model for credit lending over
time. Understanding the changing of the gaps be-
tween different population subgroups, like gender
or race, allows observing whether the mitigation
strategies in place are bringing benefits to society,
favoring the convergence between individual and
group fairness.
","bias, fairness",bias,"bias, fairness",2022,statement
137,"As mentioned in Section 5.1, we collected the raw
data from free and publicly available sources that
have no copyright or privacy issues. We recruited
our annotators from the linguistics departments
of local universities through public advertisement
with a speciﬁed pay rate. All of our annotators aresenior undergraduate students or graduate students
in linguistic majors who took this annotation as
a part-time job. We manually shufﬂed the data so
that all batches of to-be-annotated data have similar
lengths on average. An annotator could annotate
around 25 instances per hour. We pay them 50
CNY an hour. The local minimum salary in the
year 2021 is 22 CNY per hour for part-time jobs.
Our annotated data only involves factual infor-
mation (i.e., syntactic annotation), but not opinions,
attitudes or beliefs. Therefore, the annotation job
does not belong to human subject research; and
IRB approval is not required.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
138,"Datasets have been collected in a manner that is
consistent with the terms of use of any sources and
the intellectual property. For each annotator, we
compensate based on the number and quality of
annotated sentences. More details of our datasets
are depicted in Section 3.
",no ethical concerns,no_ethical_concerns,intellectual property,2022,statement
139,"The fact that the current research deals with the sen-
sitive topic of personality warrants for some ethical
considerations. First, the study has been conducted
with anonymized publicly available data. We did
not collect data ourselves and importantly the data
did not allow to identify subjects. Therefore, it is
neither required nor possible to request IRB ap-
proval for the current research, given that IRB is
concerned with the protection of human subjects.
We had no reasons to doubt that the parties, who
originally collected the data got IRB approval and
informed consent form the participants who pro-
vided their data.
However, we acknowledge that automatic sys-
tems for personality trait analysis can be misused.
Further, the application of our proposed model cre-
ation strategy can also be used for other more sensi-
ble concepts, for instance regarding mental health.
We propose that such systems are only made avail-
able in such a manner that no personalized results
can be retrieved.
",misuse,misuse,misuse,2022,"statement, concerns"
140,"In this paper, we propose an inter-level contrastive
learning method, which unifies instance-level and
keyword-level contrasts in the CV AE framework.
The positive impact lies in that it can help improve
the capability of generation models on paraphras-
ing, dialogue generation, and storytelling tasks.
The negative impact may be that the generation
process of the system is not fully controllable, so
it is possible to generate inaccurate or unreason-
able content in some extreme cases. Hence, extra
processing steps might be needed if this method
were to be used in scenarios where high accuracy
is required.
",inaccurate content,inaccurate content,no ethical concerns,2022,"statement, concerns"
141,"The primary aim of this work is to highlight the
inconsistency in labels and conﬁdence scores of
generated by standard pre-trained models for sym-
metric classiﬁcation tasks. To mitigate the afore-
mentioned inconsistency, we propose a loss func-
tion that incorporates divergence between outputs
when the input order is swapped. We do not antici-
pate any additional ethical issues being introduced
by our loss function as compared to the original
standard pre-trained models, speciﬁcally BERT and
RoBERTa. All the datasets used in our experiments
are subset of the datasets from previously published
papers, and to the best of our knowledge, do not
have any attached privacy or ethical issues. That be-
ing said, further efforts should be made to study the
inherent biases encoded in the pre-trained language
models and the datasets.
","Inconsistency, bias","Inconsistency, bias",bias,2022,"statement, suggestions"
142,"We conduct the experiments by adapting a public
story generation dataset STORIUM to our task. Au-
tomatic and manual evaluation results show that
our model CONPERoutperforms existing state-of-
the-art models in terms of coherence, consistency
and controllability, suggesting the generalization
ability of CONPERto different input personas. And
our approach can be easily extended to different
syntactic levels (e.g., phrase-level and paragraph-
level events), different model architectures (e.g.,
BART (Lewis et al., 2020)) and different genera-
tion tasks (e.g., stylized long text generation).
In both STORIUM and ConceptNet, we ﬁnd some
potentially offensive words. Therefore, our model
may suffer from risks of generating offensive con-
tent, although we have not observed such content
in the generated results. Furthermore, ConceptNet
consists of commonsense triples of concepts, which
may not be enough for modeling inter-event rela-
tions in long-form stories. We resort to Amazon
Mechanical Turk (AMT) for manual evaluation.
We do not ask about personal privacy or collect
personal information of annotators in the annota-
tion process. We hire three annotators and pay
each annotator $0.1 for comparing each pair of sto-
ries. The payment is reasonable considering that it
would cost average one minute for an annotator to
ﬁnish a comparison.",Offensive content,Offensive content,privacy,2022,"concerns, suggestions"
143,"All models trained in this work only use pose or
skeletal data. Consequently all released datasets
and models do not have any personally identi-
fiable information (PII), thereby addressing pri-
vacy concerns of those who contributed to these
datasets. Furthermore, such a standardization elim-
inates all visually distinguishing features of individ-
uals like color, gender, ethnicity/race, etc., thereby
overcoming any potential biases pertaining to sub-
populations.
We address the licensing-related aspects of the
datasets in the subsequent sub-sections.
A.1 Release of pose ISLR datasets
Our work builds on existing ISLR datasets across
languages, by processing them to retain only pose
data. Complying with the respective licenses of
the datasets, we release our generated poses only
for the openly available datasets with permissive
licenses. Out of the 7 datasets we evaluate in the
paper, we find that we can release the pose data for
5 of the datasets (AUTSL, WLASL, GSL, LSA64,
and INCLUDE), covering 5 sign languages (re-
spectively: Turkish, American, Greek, Argentinian
and Indian). The other 2 datasets are CSL and
DEVISIGN, belonging to Chinese sign language.
The licenses of each original dataset is shown in
Table 6.
We do not claim ownership over any of the orig-
inal ISLR datasets, and release the pose data under
the same licensing terms as the original datasets.
A.2 Release of raw pose data
We also open-source the pretraining dataset on In-
dian Sign Language (ISL) that we explained in
Section 4.1. The detailed datasheet of this dataset,including motivation, composition, collection pro-
cess, preprocessing, distribution, maintenance, and
ethical considerations is included after the appen-
dices.","bias, intellectual property","bias, intellectual property","bias, intellectual property",2022,statement
144,"All the corpora we used in this paper are publicly
available resources without the issue of the copy-
right. The technique this paper proposed is for
NMT models, so it can not circumvent the issues
that NMT models have. Since our automatically
dictionaries are extracted from potentially biased
data, the translations may also contain biases. How-
ever, we expect that these issues may be resolved
by using unbiased data or the addition of debiasing
objectives.
",bias,bias,bias,2022,"statement, suggestions"
145,"In terms of demographic and socioeconomic
characteristics, we attempted to establish a
balanced, bias-free dataset. The EI-I NFOTABS
dataset is derived from the INFOTABSdataset,
which is devoid of bias. The only possible source
of prejudice can be the translation pipeline. Our
qualitative analysis indicates that translation quality
is reasonably good and there aren’t any observable
biases like gender in the translation. The dataset is
intended and useful for studying language model
representations in a cross-lingual and structured
data setting. The paper points out that low-
resource languages can benefit from reasoning
over structured data in other languages. This is
a relatively new research topic and further work
will help understand limitations as well as uncover
new directions. Hence, we recommend the use of
this dataset at this point exclusively for scholarly,
non-commercial purposes.
",no ethical concerns,no_ethical_concerns,bias,2022,"statement, suggestions"
146,"As part of this paper, we created and are releas-
ing formality-controlled contrastive parallel data
from English into French, German, Hindi, Italian,
Japanese, and Spanish. The translations and annota-
tions were created by professional translators who
were recruited by a language service provider and
were compensated according to industry standards.
The translations are based on existing English cor-
pora which are not user-generated. Before creating
the translations, we obtained approval for our use
case from the creators of the existing artifacts.
As part of our formality-controlled dataset, we
noticed that translations often required the gender
of the speaker or the addressee to be speciﬁed, even
when the English source was gender-neutral. As
a result, for each such case, we include grammati-
cally feminine and grammatically masculine refer-
ence translations. We hope that this will open up
opportunities for future work in avoiding gender
bias when controlling for politeness, and even in
improving translations by customizing to the user’s
desired gender,23in a similar way to how we cus-
tomize for the desired formality in this paper. In
creating gender-speciﬁc reference translations, we
limit the differences to words that are grammati-
cally gendered in the target languages, rather than
stereotypical or other differences. It is important
to note that while this paper addresses grammat-
ical gender in translation, it does not use human
subjects, infer or predict gender, or otherwise use
gender as a variable.
We would like to emphasize that the work on
gender in this paper is very much a work in
progress. We provide this dataset as an initial con-
tribution; we will continue to improve on this work
and this data, and we hope other groups also use
and expand on it. Most notably, so far we have
only produced translations for two genders. In the
future, we plan on expanding the references trans-
lations to more genders, in consultation with native
speakers of the target languages and other stake-
holders. We also would like to analyze gender bias
in formality-controlled models, as well as create
models that can control for multiple features (e.g.,
formality and grammatical gender) simultaneously.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
147,"Although our analyses can inspire the community
and industry to develop more powerful TTS models,
it may result in unemployment for people with
related occupations such as broadcaster and radio
host. Besides, powerful TTS systems may be used
in non-consensual voice cloning and fake media generation, which might be harmful to society. As
for the limitation, we combine existing methods to
build a better TTS system rather than proposing a
brand new text-to-speech model. F.1 For every submission
F.1.1 Did you discuss the limitations of your
work?
Yes. See Appendix E.
F.1.2 Did you discuss any potential risks of
your work?
Yes. See Appendix E.
F.1.3 Do the abstract and introduction
summarize the paper’s main claims?
Yes. See abstract and Section 1.
F.2 Did you use or create scientific artifacts ?
No.
F.3 Did you run computational experiments ?
Yes.
If yes:
F.3.1 Did you report the number of parameters
in the models used, the total
computational budget (e.g., GPU hours),
andcomputing infrastructure used?
Yes. See Section 3.2 and Appendix B.
F.3.2 Did you discuss the experimental setup,
including hyperparameter search and
best-found hyperparameter values?
Yes. See Section 3.2 and Appendix B.
F.3.3 Did you report descriptive statistics about
your results (e.g., error bars around
results, summary statistics from sets of
experiments), and is it transparent
whether you are reporting the max,
mean, etc. or just a single run?
Yes. See Section 3.2, 4.2 and 5.1.
F.3.4 If you used existing packages (e.g., for
preprocessing, for normalization, or for
evaluation), did you report the
implementation, model, and parameter
settings used (e.g., NLTK, Spacy,
ROUGE, etc.)?
Yes. See Appendix B.F.4 Did you use human annotators (e.g.,
crowdworkers) or research with human
subjects ?
Yes.
If yes:
F.4.1 Did you report the full text of
instructions given to participants,
including e.g., screenshots, disclaimers
of any risks to participants or
annotators, etc.?
Yes. See Appendix B.
F.4.2 Did you report information about how
you recruited (e.g., crowdsourcing
platform, students) and paid
participants, and discuss if such payment
is adequate given the participants’
demographic (e.g., country of
residence)?
Yes. See Appendix B.
F.4.3 Did you discuss whether and how
consent was obtained from people whose
data you’re using/curating (e.g., did your
instructions explain how the data would
be used)?
Yes. See Appendix B.
F.4.4 Was the data collection protocol
approved (or determined exempt) by an
ethics review board?
N/A.
F.4.5 Did you report the basic demographic
and geographic characteristics of the
annotator population that is the source
of the data?
Yes, they are all native English speakers. See Ap-
pendix B.8213
","Unemployment, voice cloning","Unemployment, voice cloning, Deep fakes","unemployment, voice_cloning, deepfakes, harmfulness",2022,statement
148,"F.1 For every submission
F.1.1 Did you discuss the limitations of your
work?
Yes. Appendix D.
F.1.2 Did you discuss any potential risks of
your work?
Yes. Appendix E.
F.1.3 Do the abstract and introduction
summarize the paper’s main claims?
Yes. Abstract.
F.2 Did you use or create scientific artifacts ?
Yes.
If yes:
F.2.1 Did you cite the creators of artifacts you
used?
Yes. Section 4.1.
F.2.2 Did you discuss the license or terms for
use and/or distribution of any artifacts?
Yes. Section 4.1 and Appendix E.
F.2.3 Did you discuss if your use of existing
artifact(s) was consistent with their
intended use , provided that it was
specified? For the artifacts you create,
do you specify intended use and whether
that is compatible with the original
access conditions (in particular,
derivatives of data accessed for research
purposes should not be used outside of
research contexts)?
Yes. Appendix E.
F.2.4 Did you discuss the steps taken to check
whether the data that was collected/used
contains any information that names or
uniquely identifies individual people or
offensive content , and the steps taken to
protect / anonymize it?
Yes. The participants are asked to use bogus names
in the data collection process, which means we do
not know who sang these recordings.
F.2.5 Did you provide documentation of the
artifacts, e.g., coverage of domains,
languages, and linguistic phenomena,
demographic groups represented, etc.?
Yes. Section 4.1.
F.2.6 Did you report relevant statistics like the
number of examples, details of
train/test/dev splits, etc. for the data that
you used/created?
Yes. Section 4.1.F.3 Did you run computational experiments ?
Yes.
If yes:
F.3.1 Did you report the number of parameters
in the models used, the total
computational budget (e.g., GPU hours),
andcomputing infrastructure used?
Yes. Section 4.1.
F.3.2 Did you discuss the experimental setup,
including hyperparameter search and
best-found hyperparameter values?
Yes. Section 4.1.
F.3.3 Did you report descriptive statistics about
your results (e.g., error bars around
results, summary statistics from sets of
experiments), and is it transparent
whether you are reporting the max,
mean, etc. or just a single run?
Yes. Section 4.2.
F.3.4 If you used existing packages (e.g., for
preprocessing, for normalization, or for
evaluation), did you report the
implementation, model, and parameter
settings used (e.g., NLTK, Spacy,
ROUGE, etc.)?
Yes. Section 4.1.
F.4 Did you use human annotators (e.g.,
crowdworkers) or research with human
subjects ?
If you answer Yes, provide the section number; if
you answer No, you can skip the rest of this section.
Yes.
If yes:
F.4.1 Did you report the full text of
instructions given to participants,
including e.g., screenshots, disclaimers
of any risks to participants or
annotators, etc.?
Yes. Appendix C.7982","unemployment, piracy, misuse","unemployment, piracy, misuse","unemployment, piracy, misuse",2022,statement
149,"As language models become prominent, it is im-
perative to understand and mitigate various harms
that they may provoke (Solaiman et al., 2019; Bom-
masani et al., 2021). Moreover, to make language
processing resource-efﬁcient, more focus should
be on achieving good performance with smaller
models. Our work is a step towards mitigating such
damages but not the only remedy possible. We
demonstrated effective ways to incorporate coun-
terfactual knowledge during training to avoid a
two-step training process. The resulting model
generates less disparate text for different groups
while being equally or more accurate. However, as
we have discussed in Sec. 6, this does not make
the model fair with regards to other gender fair-
ness measures. Our results essentially echo the
argument made in Barocas et al. (2019) that it is
meaningless to ascribe fairness to a model. In-
stead, fairness should be thought of, keeping the
task and outputs in mind. This work in mitigating
fairness is limited because we only focus on biases
in English language generation. Other works, such
as Zmigrod et al. (2019), have identiﬁed the dif-
ﬁculties in transferring these approaches to otherlanguages. Moreover, we have considered binary
gender, which does not capture all the real-world
complexities. More critically, our assessment of
fairness for open-ended text generation has relied
on fair deﬁnitions and measures from Dhamala
et al. (2021) and Sheng et al. (2019). One should
interpret the results with this in perspective. Some
recent works, such as Blodgett et al. (2020, 2021);
Gonen and Goldberg (2019), have demonstrated
critical ﬂaws in other fairness measures. For exam-
ple, Blodgett et al. (2021) found that benchmark
datasets designed for measuring stereotyping be-
havior of LMs such as StereoSet (Nadeem et al.,
2021) and CrowS-Pair (Nangia et al., 2020) are am-
biguous and have several pitfalls which can even
operationalize stereotyping. Our approach uses
counterfactual data, which may inherit the ﬂaws in
original data or introduce new errors. Users should
use appropriate ﬁlters/mechanisms to ensure the
quality of counterfactual data used for training.
Finally, we propose approaches to create less bi-
ased LMs. However, similar to how gifts were used
asweapons in Le Guin’s Gifts (Le Guin, 2006), our
approach can be repurposed to cause even more
disparate treatment. For example, one may remove
the mention of a speciﬁc race or gender completely
from the training set to create a dystopian LM that
does not acknowledge that group or entity’s ex-
istence or the inaccuracy of counterfactual gener-
ation may cause LM to learn from ﬁctional and
non-grammatical texts. Nevertheless, we hope that
our work will inspire more good than harm.
","Bias, fairness","Bias, fairness","bias, misuse",2022,"statement, concerns, suggestions "
150,"There are several limitations to our work, which
should be taken into account in the interpretation
of our results.
First, our results are likely affected by reporting
bias and by a defaulting effect where, when people
annotate traits for “men”, they may actually have
in their head “cis straight white men ”, because the
defaults go unremarked. This goes both for the
human scores (how does a participant conceptu-
alize “men”?) and language model scores (what
do sentences containing the word “man”assume
given that most language a langauge model has
been trained on likely exhibits defaulting?).
Second, our work only focus on assessing stereo-
types within language models and not in any de-
ployed system. Though stereotypes from language
models may impact the outputs of downstream sys-
tems which are built upon these language models,
it is not clear how exactly the stereotypes trans-
fer (Cao et al., 2022). Additionally, our work is
limited to English and U.S. social stereotypes.
Third, although we followed and built on best
practices from social psychology in developing the
human study, it nevertheless has some shortcom-
ings. In particular, even after many iterations on
wording, it was difficult to phrase the survey ques-
tions to encourage people to reporting their true
impressions. There is tension between asking a par-
ticipant what theythink—which risks a counfound-
ing potential social desirability bias (Latkin et al.,
2017) (people’s tendency to respond in socially ac-
ceptable ways)—and asking what they think others
think—which led to comments from a few partici-
pants that they felt unqualified to speak for others.
Asking these questions of participants and collect-
ing the data also raises the possibility of this work
inadvertantly reinforcing stereotypes.
Finally, aggregating human judgements into a
single number by averaging (or any other statistic)
8Ghavami and Peplau (2013) covers paired groups com-
bined with race domain and binary genders. The traits they
raised span the agency and communion dimensions.to compare to model predictions risks collapsing a
significant amount of information down to a single
number. This number cannot distinguish between a
weakly held but common stereotype and a strongly
held but rare one. Nor can it distinguish between
traits where half of annotators say 0 and the other
half say 100, from traits where all annotators say
50. These average judgments should be interpreted
as not what any single person would say, but an
average over people. This limitation is exacerbated
by the defaulting effect, where some people may
imagine a different prototype for a given group,
and other people may imagine another.","bias, stereotype reinforcement","bias, stereotype reinforcement",bias,2022,"statement, concerns"
151,"This work can be used to evaluate bias in models,
and thus used to evaluate models serving human
consumers. As with all metrics, the metric does not
capture all notions of bias, and thus should not be
the only consideration for serving models. While
this is a valid risk, this is one that is not speciﬁc
to prediction sensitivity. Good use of this metric
requires users to be cognizant of these strengths
and weaknesses. We also note that the metric re-
quires deﬁning protected attributes (e.g. gender)
and our work carries the limitation that the selected
datasets contain binary gender annotations. Deﬁn-
ing protected attributes may not always be possible
and when possible, the protected attribute classes
may not be comprehensive.
",Binary gender,Binary gender,bias,2022,"Statement, concerns"
152,"GagaSTimproves singability and intelligibility of
the translated songs in Mandarin via constrain-
ing the decoding of a pretrained lyrics translation
model. This methodology has limitations by im-
posing a direct trade-offs between the original ob-
jective and the constraints. In terms of negative
impact or risks, the inaccurate translations may
cause misunderstandings in applications like Musi-
cal Theatre.
This paper collects lyrics data that are publicly
available and are parsed from the Web. We use
these data for research purposes only. To pre-
vent any abuse or piracy of these data, we chose
the dataset license Attribution-NonCommercial-
ShareAlike 4.0 International (CC BY-NC-SA 4.0).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, concerns"
153,"Our proposed model is intended to be used for ad-
dressing the problem of personalization, by learning
one shared model for all users, and querying it using
a personal identifier. One potential measure that
needs to be taken for deployment of such technology
is to setup proper authentication tools, so that each
user can only query with their own identifier and
prevent users from breaching privacy by querying
other users’ models. However, this could be a
concern in other personalization setups too.
The datasets used in our experiments are all
publicly available (Yelp, IMDB and Sentiment 140),
and we have not collected any information about
the users who have contributed their data beyond
what is originally provided in the dataset, which is
only the user-based partitioning of the data.
",privacy,privacy,security,2022,"statement, suggestions"
154,"LEVEN focuses on detecting events from the fact
and does not involve any value judgment. LED
aims to transform the unstructured legal text into
structured event information, which is helpful to
further processing. Therefore, our work can help
reduce the workload for legal professionals and
improve their work efﬁciency. Considering the
fact that, like any other legal AI application, LED
models would inevitably make mistakes and have
negative inﬂuences, we argue that LED can only
serve as an auxiliary tool for legal work and the
ﬁnal decision on a speciﬁc legal issue has to be
ensured by legal professionals. In such case, we
could exploit the advantage of legal AI and avoid
the potential risk.
The corpus we use is released by the Chinese
government and has been anonymized wherever
necessary. Therefore, our dataset does not involve
any personal privacy. In terms of human annotation,
we ﬁrst annotate a few examples on our own to
approximate the workload and then determine the
wages for annotators according to local standards.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
155,"The ethical implications of research are always
an important consideration for us. While pursuing
better model performance and high quality datasets,
we respect the intellectual property rights of data
resources, the privacy and rights of data sources,
and strive to avoid potential harm to vulnerable
populations.
When crawling the documents needed to build
the XFUND dataset and LayoutXLM pre-training
data, we strictly follow each site’s robots exclusion
standard4to ensure we are allowed to collect data.
We also manually excluded websites with privacy
concerns, keeping only those pages that we had
permission to edit and republish according to the
permission rules.
For the data used to build XFUND, we first re-
moved all content and kept only the template, thus
removing the maximum amount of sensitive con-
tent. On this basis, annotators filled in the tem-
plates using synthetic data that does not involve
sensitive personal information of annotators, thus
ensuring the privacy and rights of annotators. Then,
we manually reviewed the templates to prevent po-
tential privacy violations and harm to vulnerable
populations. Any data that does not meet the speci-
fications will be completely deleted.
B LayoutXLM
B.1 Pre-training Data Samples
We show pre-training samples of each languages in
Figure 4.
B.2 Pre-training Data Distribution
Figure 5 shows the complete list of languages with
the distribution of pre-training languages.
4https://en.wikipedia.org/wiki/Robots_
exclusion_standard
(a) English
 (b) Chinese
 (c) Japanese
 (d) Spanish
(e) French
 (f) Italian
 (g) German
 (h) Portuguese
Figure 4: Real-world business documents with different layouts and languages for pre-training LayoutXLM
Figure 5: Language distribution of the data for pre-training LayoutXLM. We also show the document counts per
language for different sampling exponents α.3224",no ethical concerns,no_ethical_concerns,"privacy, misuse, intellectual property",2022,"statement, actions"
156,"Our work introduces ELECTRA-style tasks for
cross-lingual language model pre-training, which
requires much less computation cost than previous
models and substantially reduces the energy cost.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
157,"XPRis designed as a cross-lingual phrase retriever
that retrieve relevant phrases across different lan-
guages. We believe X PRwould help the commu-
nication between the people who speak different
languages. Besides, our work can facilitate the re-
search on multilingual natural language process-
ing (NLP), which helps to build NLP applications
for low-resource languages. In addition, we con-
struct the WikiXPR dataset using open-source data
from Wikipedia and dbpedia.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
158,"We analyze entries of slang usage in our work and
acknowledge that such usages may contain offen-
sive information. We retain such entries in our
datasets to preserve the scientiﬁc validity of our re-
sults, as a signiﬁcant portion of slang usage aligns
to possibly offensive usage context. In the presen-
tation our of results, however, we strive to select
examples or illustrations that minimize the extent
to which offensive content is represented. We also
acknowledge that models trained on datasets such
as the Urban Dictionary have a greater tendency
to generate offensive language. All model outputs
shown are results of model learning and do not re-
ﬂect opinions of the authors and their afﬁliated or-
ganizations. We hope that our work will contribute
to the greater good by enhancing AI system’s abil-
ity to comprehend such offensive language use,
allowing better ﬁltering of online content that may
be potentially harmful.
",offensive information,offensive language,"misuse, offensiveness",2022,"statement, concerns"
159,"Controllable text generation is an important step
to unleashing the potential of modern CLMs. Ad-
ditionally, it is an interesting approach to counter
many of the problematic biases that have been
found. But an increased level of control also
entails an increased risk of malicious use. We
hence recognize the possibility that techniques
proposed in this paper could be utilized in malev-
olent scenarios, like guided misinformation or tar-
geted harmful content.
This work has utilized computational GPU re-
sources provided by ICE-RISE5. The final model
training lasted roughly 2 days on a single DGX-
100 machine, resulting in about 400 GPU hours.
The total number of GPU hours for the whole re-
search endeavour is difficult to estimate, but it can
be safe to assume that it is less than 2000 GPU
hours.
","misinformation, harmful content","misinformation, harmful content","bias, misuse, offensiveness",2022,"statement, concerns"
160,"Our work should not have any direct ethical im-
plications, since we mainly introduce evaluationtasks and evaluate different models on them. We
do however investigate visual conceptual percep-
tions based on data from a potentially small group
of people whose world-view may be culturally dif-
ferent from that of other individuals. This means
that we may encourage knowledge that beneﬁts
some people more than others. Similar issues are
discussed by Liu et al. (2021). Our investigation is
limited to English-language models and datasets,
limiting the generality of our conclusions.
8 Conclusions
We introduce new evaluation methods for measur-
ing the visual commonsense knowledge in LMs
and evaluate a number of multimodal LMs on these
benchmarks. We ﬁnd that there are no signiﬁcant
differences in performance between models trained
on pure text and models trained on images and
text. Most prominently, we ﬁnd that a unimodal
LM trained on image captions and VQA queries
can attain a visual commonsense knowledge on par
with that of a multimodal model.
We also conﬁrm the results by Jiang et al. (2020)
and Cao et al. (2021), that LMs are sensitive to
query format even when querying for common-
sense knowledge. This casts some doubts on what
is really measured in a model for a cloze task and
whether we can reason about LMs as having knowl-
edge. An interesting future step would be to inves-
tigate this further and see if it would be more appli-
cable to use e.g. probing or some other evaluation
method.
Nonetheless, this is a ﬁrst step towards measur-
ing the visual commonsense knowledge in multi-
modal as well as unimodal LMs. We hope that
the evaluation tasks introduced here may aid other
researchers in their aim to create models that learn
language from more than text.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
161,"Done right, biomedical MDS can significantly fa-
cilitate the practice of evidence-based medicine;
done wrong, however, it creates risk of misinter-
pretation of evidence and subsequent malpractice.
For this reason, we argue that the factual accuracy
of biomedical summaries should be decided on a
rigid yes/no scale, and only the summaries match-
ing in all details and intents should be considered
factually correct and thus useful. In this paper, we
show that we still have a long way to go before
biomedical summarization systems can be reliably
used and trusted, and highlight the importance of
robust human evaluation in this domain.","misinterpretation, malpractice","misinterpretation of evidence, malpractice",manipulation,2022,"concerns, statement"
162,"Workers annotating the dataset we built were hired
on a part-time basis and compensated based on the
number of working hours. They were compensated
with 9,000 won per hour, which was somewhat
higher than the Korean minimum wage at the time
they worked. Appropriate instructions for the use of
collected data were given at the time of contract and
consent was obtained. We will release our dataset
in CC-BY-NC-SA license.5
The dataset we built to validate our proposed
methods is all generated from scratch by workers
and large-scale LMs. Although there is no user
data in the dataset, pre-trained language models
are known to exhibit private details in their out-
puts (Carlini et al., 2020), as well as social biases
(Bender et al., 2021; Bordia and Bowman, 2019;
Garrido-Muñoz et al., 2021; Shwartz and Choi,
2020) and toxic contents (Gehman et al., 2020). To
5https://creativecommons.org/licenses/
by-nc-sa/2.0/
address these concerns, we determined categories
and criteria for harmful texts based on legal and
ethical considerations provided by experts in our
group, and we instructed annotators to ﬁlter the
dataset using these criteria. However, due to miss-
ing annotations and cultural or social biases, this
may be imperfect. To mitigate this, we had multiple
crowd workers annotate the same data. In addition,
because the users in the dataset are regarded to be
a vulnerable population, our group’s ethical con-
sultation looked through the issues that would be
sensitive to them, and dialogues containing these
topics were also eliminated.
Despite these efforts, using this dataset to di-
rectly train end-to-end chatbot models can involve
certain risks, due to the lack of controllability and
interpretability in end-to-end neural response pre-
diction models. And it should not be overlooked
that they may cause some potential harm, even
though the chatbot systems can help reduce social
loneliness of the user population. For example, a
user can become emotionally attached to a bot,
even codependent on it, which can divert attention
away from real-world relationships and cause dis-
tress if the chatbot fails. It’s also worth noting that
a chatbot can be programmed to impersonate a real
person and be used for phishing and fraud. Dur-
ing such conversations, users may provide private
and sensitive information, such as speciﬁc health
conditions and private attributes, which could be ex-
ploited if it falls into the wrong hands. For this rea-
son, when incorporating this dataset in real-world
applications, the application developers should en-
sure that it is used safely and ethically.
Since our proposed framework also can be used
for building another dataset and chatbot system
with arbitrary speciﬁcations, it is not exempt from
the possibility of propagating linguistic biases and
toxicity. Similar to Xu et al. (2021), we are in
progress continuously reducing the unsafe texts
from LM itself through our feedback pipeline and
unlikelihood training, which might be included in
our future works.","privacy, bias, toxic content, emotional attachement ","privacy, bias, emotional codependency ","bias, toxic content, misuse, emotional attachement, controllability, 
interpretability",2022,"statement, concerns, actions, suggestions"
163,"Our ADVETA benchmark presented in this work is
a free and open resource for the community to study
the robustness of Text-to-SQL models. We col-
lected tables from three mainstream Text-to-SQL
datasets, Spider (Yu et al., 2018), WikiSQL (Zhong
et al., 2017) and WTQ (Papernot et al., 2017),
which are also free and open datasets for research
use. For the table perturbation step, we hire profes-sional annotators to ﬁnd suitable RPL/ADD candi-
dates for target columns. We pay the annotators at
a price of 10 dollars per hour. The total time cost
for annotating our benchmark is 253 hours.
All the experiments in this paper can be run on
a single Tesla V100 GPU. Our benchmark will be
released along with the paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
164,"This work presents HiTab, a free and open English
dataset for the research community to study table
question-answering and table-to-text over hierar-
chical tables. Our dataset contains well-processed
tables, annotations (QA pairs, target text, and bidi-
rectionally mappings between entities and quan-
tities in text and the corresponding cells in table),
recognized table hierarchies, and source code. Data
in HiTab are collected from two public organiza-
tions, StatCan and NSF. Both of them allow sharing
and redistribution of their public reports, so there
is no privacy issue. We collect tables and accompa-
nied descriptive sentences from StatCan and NSF.
We also include hierarchical tables in Wikipedia
from ToTTo, which is a public dataset under MIT
license, so there is no risk to use it. And in the label-
ing process, annotators need to check if there exist
any names or uniquely identifies individual people
or offensive content. They did not find any such
sensitive information in our dataset. We recruit
18students or graduates in computer science, fi-
nance, and English majors from top universities( 13
females and 5males). Each student is paid $7.8
per hour (above the average local payment of simi-
lar jobs), totally spending 2,400hours. We finally
get3,597tables and 10,672well-annotated sen-
tences. And the data got approval from an ethics
review board by an anonymous IT company. The
details for our data collection and characteristics
are introduced in Section 2.
",no ethical concerns,no_ethical_concerns,offensiveness,2022,"statement, actions"
165,"We make use of pretrained language models to
both generate and retrieve text in this work. Rep-
resentations from pretrained language models are
known to cause ethical concerns, such as perpetu-
ating racial or gender bias (Field et al., 2021; Gala
et al., 2020). We advise using caution and adopting
a post-processing strategy to filter potentially offen-
sive text produced by pretrained language models
before releasing text content to users. Additionally,
we note that most existing LFQA datasets (includ-
ing the ELI5 dataset used in this work) and bench-
marks are collected from English text sources. We
hope future works can explore the use of exemplifi-
cation in other languages.
",bias,"bias, English-centric",bias,2022,"statement, suggestions, concerns"
166,"Our work’s limitations are discussed in Sec-
tion 1 and Section 9. All six datasets we use
are from prior work, are publicly available, and
are commonly used for the study of extractive
QA. Section 4 reports our computational bud-
get and experimental setup in detail. Our code-
base is available at https://github.com/
lil-lab/bandit-qa .
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
167,"We annotate existing, publicly available long-form
question answering datasets which might contain
incorrect and outdated information and societal
biases. We collected annotations through crowd-
sourcing platform and also by recruiting undergrad-
uate annotators at our educational institution. We
paid a reasonable hourly wage ($13/hour) to an-
notators and documented our data collection pro-
cess with datasheet (Gebru et al., 2021). We in-
clude studies on the extractiveness of long-form
answers (how much content can be grounded to ev-
idence document) through a coarse measure of lex-
ical overlap. This is connected to faithfulness and
reducing hallucination of QA system. Our study
is limited to English sources, and we hope future
work can address analysis in other languages.
",no_ethical_concerns,English-centric,bias,2022,"statement, actions"
168,"The CLIP models we used are trained on millions
of image-text pairs collected from the web. Birhane
et al. (2021) shows that such large-scale datasets
often contain problematic and explicit image-text
pairs. As the CLIP model card6suggests, using
CLIP reward for training image captioning models
is intended as a research output, and any deployed
use case of the models is out of scope.
Our captioning models and CLIP models are
trained on English datasets; its use should be lim-
ited to English language use cases. As our pro-
posed method is not limited to English and easily
extended to other languages, future work will ex-
plore the extensions in various languages.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
169,"This tutorial covers work that extensively uses large
(up to hundreds of billions of parameters) language
models, which are associated with substantial finan-
cial and environmental costs (Strubell et al., 2019),
as well as other harms (Bender et al., 2021).
",no ethical concerns,no_ethical_concerns,misuse,2022,statement
170,"We do not foresee any ethical concerns from the
technology presented in this work. We used pub-
licly available datasets designed for summariza-
tion, and do not annotate any data manually. The
datasets used is in English language.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
171,"We do not foresee any ethical concerns from the
technology presented in this work. We used pub-
licly available datasets, and do not annotate any
data manually. The datasets used have reviews in
English language. Human evaluations for summa-
rization were performed on Amazon Mechanical
Turks (AMT) platform. Human judges were com-
pensated at a wage rate of $15 per hour.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
172,"Our approach creates a synthetic dataset using a
public dataset to train a factual consistency check-
ing model. Therefore, in the process of generat-
ing such samples, ethically problematic datasets
can be generated due to the bias of the pre-trained
models, similar to other text generation tasks. For
this reason, once the training process is completed,
we remove the generated sample. And, we will
not release the synthetic dataset itself, and will re-
lease only the trained factual consistency checking
model.
",bias,bias,bias,2022,"statement, actions"
173,"Similar to previous works in adversarial NLP lit-
erature, there are risks that our proposed approach
may be unintentionally utilized by malicious ac-
tors to attack textual ML systems. To mitigate this,
we will not publicly release the full perturbation
dictionary that we have extracted and reported in
the paper. Instead, we will provide access to our
private API on a case-by-case basis with proper
security measures. Moreover, we also suggest and
discuss two potential approaches that can defend
against our proposed attacks (Sec. 6). We believe
that the beneﬁts of our work overweight its poten-
tial risks. All public secondary datasets used in
this paper were either open-sourced or released by
the original authors.
",security,security,security,2022,"statement, concerns, suggestions"
174,"In this work we present a dataset on the transcripts
of a publicly accessible video-streaming platform,i.e., “ Behance ”1. Complying with the discussion
presented by Benton et al. (2017), research with
human subjects information is exempted from the
required full Institutional Review Board (IRB) re-
view if the data is already available from public
sources or if the identity of the subjects cannot
be recovered. However, to protect the identity of
the streamer and any other person whose informa-
tion are shared in the video transcript, we impose
extra processing on the transcribed documents be-
fore presenting them to annotators and publicly
releasing it later. First, in this dataset, we remove
username or any other identity-related information
of the streamers in the transcripts to prevent disclos-
ing their identity. Moreover, the proposed dataset
only provides textual data (i.e., documents), hence
the other content of the videos (e.g., images, au-
dios) are not revealed (to annotators or users) to
protect human identity. Finally, to reduce the risk
of disclosing the information of the people in the
transcripts, in the final version of the dataset, we
exclude the transcripts that explicitly or implicitly
refer to the identify of the target people.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
175,"This paper proposes Differentiable Self-Training
(DRIFT ), a self-training framework for NLP tasks.
We demonstrate that the DRIFT framework can
be used for text classification and named entity
recognition tasks. Moreover, the framework is also
demonstrated to be effective for semi-supervised
classification on graphs. We use publicly available
datasets to conduct all the experiments. And the
proposed method is built using public code bases.
We do not find any ethical concerns.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
176,"Our paper proposed SS-AGA, a novel multilin-
gual knowledge graph completion model for pre-
dicting missing triples in KGs considering theirknowledge transfer. SS-AGA neither introduces
any social/ethical bias to the model nor amplifies
any bias in the data. We the created multilingual
E-commerce product KG dataset by masking all
customers’/sellers’ identity and privacy. We only
collect information related to products without any
personal information leakage. Our model is built
upon public libraries in Pytorch. We do not foresee
any direct social consequences or ethical issues.
",no ethical concerns,no_ethical_concerns,"privacy, bias",2022,"statement, actions"
177,"This paper proposes MoEBERT, which uses a
Mixture-of-Experts structure to increase model ca-
pacity and inference speed. We demonstrate that
MoEBERT can be used for model compression. Ex-
periments are conducted by fine-tuning pre-trained
language models on natural language understand-
ing and question answering tasks. In all the exper-
iments, we use publicly available data and mod-
els, and we build our algorithms using public code
bases. We do not find any ethical concerns.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
178,"This paper uses data obtained from the Twitter De-
veloper API5and freely available social media data
from the Reddit platform using the Pushshift API
(Baumgartner et al., 2020). Moreover, we only pro-
vide the Tweet ID in the annotated datasets along
with a data preparation script in accordance with
the Twitter Terms of Service. We also compensated
the human annotators with a stipend more than the
minimum wage in India.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
179,"Data
A description of the data pre-processing is provided
in §2.1. Datasets that we created will be open-
sourced. In the case of the WSJ dataset, the data is
licensed for use only to members by the Linguistic
Data Consortium. Consequently, we only release
scripts to generate the data we use and not the data
itself. We highlight however that the permuted
document self-supervision task that we train on is
independent of the dataset used and the task can
be reproduced on any other corpus; see also §4.3.
All other datasets we use are licensed freely for
academic use.
Annotation of LM VLMDataset
We conduct a user study to collect pairwise co-
herence judgments on our language model output
dataset. As part of our crowd-sourced user study
on Amazon Mechanical Turk to collect these coher-
ence judgements, we do not collect any personal
information from the participants. Based on the av-
erage time spent to perform the tasks, participants
were paid the equivalent of 16 USD per hour for
their work. The annotation instructions and inter-
face provided to the participants are included in
Appendix A.3.
One potential issue is that the language model
output that we generate from prompts may lead
to malicious text generation by the models. We
flagged the task to warn the workers that there
may be potentially offensive content, and manu-
ally checked the final dataset post curation.
Applicability Across Languages
All our experiments are conducted using data for
the English language. However, as coherence and
discourse relations in text are a universal concept,
and our training data is automatically generated,
we expect the permuted document task to be easily
extensible to other languages.
",offensive content,offensive content,offensiveness,2022,"statement, concerns"
180,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
181,"We created two Telugu NER datasets corre-
sponding to two different domains (Newswire
and Medical), and we open source the two
datasets. The code and datasets can be
downloaded from https://github.com/
mors-ner/anonymous_telner .
We reused publicly available datasets (WikiAnn
and LREC-NER) to compare state-of-the-art meth-
ods.
WikiAnn dataset can be down-
loaded from https://drive.
google.com/drive/folders/
1Q-xdT99SeaCghihGa7nRkcXGwRGUIsKN?
usp=sharing . WikiAnn dataset is licensed
under https://opendatacommons.org/
licenses/by/ . Please read their terms of use13
for more details.
13https://elisa-ie.github.io/wikiann/LREC-NER dataset can be downloaded
from http://ltrc.iiit.ac.in/
ner-ssea-08/index.cgi?topic=5 .
LREC-NER dataset is licensed under a Creative
Commons License. Please read their terms of
use14for more details.
Fair Compensation: We provided the data to an
Elancer IT Solutions Private Limited15company
for NER annotation. In order to perform the annota-
tion process, Elancer IT Solutions Private Limited
chose five native speakers of Telugu with excellent
fluency, the company itself properly remunerates
all the annotators.
Privacy Concerns: We have gone through the
privacy policy of various websites mentioned in
the paper. For example, the website privacy policy
ofwww.greatandhra.com is provided here16.
We do not foresee any harmful uses of using the
data from these websites.
8
",no ethical concerns,no_ethical_concerns,misuse,2022,"statement, actions"
182,"We reused publicly available datasets for this work:
Pereira and Narratives. We did not collect any new
dataset.
Pereira dataset can be downloaded from https:
//osf.io/crwz7/ . Please read their terms of
use2for more details.
Narratives dataset can be dowloaded from
https://datasets.datalad.org/
?dir=/labs/hasson/narratives . Please
read their terms of use3for more details.
We do not foresee any harmful uses of this tech-
nology.
",no ethical concerns,no_ethical_concerns,misuse,2022,statement
183,"The proprietary Wunderlist data was anonymized
and personally identifiable information was
scrubbed. Names were replaced by random names.
In addition, k-anonymization was performed on the
data so that tasks that were created by fewer than
five users or fewer than 100 times in total were
automatically discarded. The result is an aggregate
view of the logs, devoid of any identifiers, private
information or infrequent tasks that can be corre-
lated back to a user. The data cleaning process was
approved by an internal legal review board before
the data was cleared for internal use. None of the
data is exposed in this paper. Example texts pre-
sented in this paper are made up by the authors, and
no text is taken verbatim from the original data.
As LITE is essentially built on pre-trained lan-
guage models, biases existing in the original lan-
guage models can still remain in the final model
(e.g., biased associations between gender and ac-
tions). We did not observe any undesired associ-
ations caused by the models in our experiments,
but it may be required to monitor biases and apply
debiasing techniques before deploying the model
to production systems.
Although LITE is not specifically designed for
English, it will require significant cost to deliver the
outcome to other languages due to the dependence
on English resources (knowledge bases used for
training COMET and English FrameNet).",bias,bias,bias,2022,"statement, actions, concerns"
184,"Psychology has seen a growing scandal around
p-hacking, i.e., the generation of enough experi-
mental variations to produce a significant outcome
in one of them. This risk is low in NLP, as signif-
icance alone is not sufficient for publication: typ-
ically, proof of predictive performance on ideally
several test sets is necessary instead. Reporting
significance in NLP is therefore an additional ro-
bustness measure, indicating the model’s general-
izability. While users might abuse the capabilities
of BooStSa to make significant results more likely,
this would require deliberate tampering (and even
then would not guarantee significance). If used
as intended, however, BooStSa should reduce un-
intended variance via researcher degrees of free-
dom and make results more comparable and repro-
ducible.",abuse,abuse of model,no ethical concerns,2022,"statement, concerns"
185,"To the best of our knowledge, we do not introduce
any ethical concerns in this work. Our work is
based on the existing MRPC and PAWS datasets,
which are sampled from online news articles as
well as Wikipedia. Hence we expect our ﬁndings
to generalize well to other English datasets in the
general domain. Generalization of our work to
domains where usage of language is markedly dif-
ferent (for example, in some forms of technical
writing) is not certain. When our proposed metrics
are used in conjunction with other technology (such
as large generative language models), it does not
affect the existing ethical considerations of using
those technology.
7 Conclusions and Future Work
In our paper, we have proposed two new metrics
to better understand paraphrase pairs: word posi-
tion deviation (WPD) and lexical deviation (LD).
We have applied these metrics to better understand
the MRPC and PAWS datasets, and also to ﬁlter
the output of a paraphrase generation model to ob-
tain speciﬁc forms of paraphrases. However, our
metrics still have some limitations, which can be
address in future work. Although we are able to
measure the extent of structural and lexical alter-
ations, we cannot determine the ﬁne-grained type
of alterations that is being made, for example, a
speciﬁc form of structural alteration or word sub-
stitution. We anticipate that improvements in this
area would be valuable to improve our ability to
effectively characterize various properties of para-
phrases, leading to better data augmentation and
robustness testing approaches that eventually re-
sulting in better performing NLP systems.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions"
186,"The approach proposed in the paper is aimed at
supporting robust and accurate detection of on-
line hate speech. The datasets used in the work
are publicly available and referenced appropriately.
The dataset creators have presented, in detail, the
data collection process and annotation guidelines
in peer-reviewed articles. The offensive terms pre-
sented, as examples, are only intended for better
analysis of the models for research purposes.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
187,"The proposed method has benefits and risks that
should be considered from an ethics perspective.
One principle of ethical AI is transparency , and
we have developed this method with the goal of
improving transparency for system developers, end
users, and other stakeholders to better understand
the inner workings of complex NLP systems. In the
application domain of hate speech detection, we
demonstrated how necessity and sufficiency scores
might be used to diagnose possible classification
biases against identity groups, who are frequently
subjects of online abuse. This can help in address-
ing the known issue of over-sensitivity to identity
terms, ensuring that benign conversations around
issues concerning marginalized groups are not mis-
classified as hate speech.
However, there are also potential risks. We make
use of existing datasets and thus our analysis is lim-
ited by those data: they were collected from public,
online platforms without user’s explicit consent,
and may not accurately represent speakers from all
demographic groups, they are only in English, and
they may be biased towards or against certain top-
ics of conversation. The data and analysis are also
limited to the English language. Training language
models on user data also has privacy implications,
as the language model may then re-generate user
text when deployed.
While transparency and explainability are seen
as desirable properties, they can also expose AI
systems to malicious attacks. In the context of hate
speech, our explainability metrics could potentially
be used to identify and then exploit system vulner-
abilities.
Finally, our approach requires the use of large
language models, which are computationally ex-
pensive to train and can reflect the biases of their
training data. Our method of generating multiple
counterfactual examples per word, rather than sim-
ply removing or masking that word, also increases
the computational resources required.","privacy, misrepresentation, security, computational resources, bias","privacy, misrepresentation, English-centric, security, computational resources, bias","privacy, bias, transparency, security",2022,"statement, concerns"
188,"As discussed throughout the paper, attempting to
model human morality in a machine learning model
has numerous ethical implications; however, that
is not our goal here. Instead, we conduct a black-
box assessment of an existing, publicly-available
model in order to assess whether it has learned any
higher-order ethical principles, and whether they
align with human theories of morality. As such, we
believe there are more limited ethical ramifications
to this work, as outlined below.
We acknowledge that the broad ethical frame-
works studied here were developed in the context
of Western academia, and other ethical systems
and frameworks exist and should also be exam-
ined. Similarly, as the authors, we ourselves are
situated in the North American scholarly context
and acknowledge that despite our goal of neutral
objectivity, our perspectives originate from a place
of privilege and are influenced by our backgrounds
and current environment.
In this work, we deliberately avoid stating that
one moral theory is “better” than another, or that
one pillar within a moral framework is preferable
to another. In essence, we have taken a stance of
moral relativism , which in itself has been criticized
as promoting an “anything goes” attitude where
nothing is inherently wrong (or right). However,
for the purposes of this paper, we believe it was
important to keep a mindset of open enquiry to-
wards the moral principles encoded in Delphi; the
question of which these principles is the “best” or
“most important” is an age-old question and cer-
tainly outside the scope of this paper.
In attempting to map Delphi’s output to annota-
tor characteristics, we have relied on group-level
statistics describing gender, age, education, and
socio-economic status. This demographic informa-
tion has been shown to be correlated with various
moral beliefs; however, individual morality is com-
plex and shaped by personal factors which we do
not consider here.
We have attempted to avoid, as much as possi-
ble, using language that ascribes agency or intent
to the Delphi system. We emphasize here that al-
though we use words like “judgement” to describe
Delphi’s output, we do not suggest that machine
learning models can have agency or accountabil-
ity. For reproducibility, we release both the set of
prompts used in this study, as well as Delphi’s out-
puts (v1.0.4). These can also be used to comparethe outputs of other morality classifiers in future
research.
",bias,bias,no ethical concerns,2022,"statement, concerns, actions, suggestions"
189,"This paper proposes a new dialogue generation
framework that utilizes the simulated dialogue fu-
tures in the inference phase to enhance the gener-
ation of the response. Generative approaches are
widely used in a wide range of dialogue applica-
tions. The proposed method improves the quality
of the generated responses, which could be bene-
ficial to research and real-world applications. The
research will not pose ethical issues. This paper
doesn’t involve any data collection and release thus
there are no privacy issues. All the datasets used
in this paper are publicly available and are widely
adopted by researchers to test the performance of
open-domain response generation models. This
paper conducts human evaluation to evaluate the
quality of the generated responses. Three part-time
research assistants were recruited to do human eval-
uation with clearly demonstrated evaluation rules.
They worked with the pay 100 CNY/hour during
their evaluation.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
190,"This work conducts experiments on the well-known
dialogue datasets, and the dataset pre-processing
does not make use of any external textual resource.
Pre-trained end-to-end dialogue generators using
large-scale text corpus are also employed, which
might be subjected to offensive contexts and demo-
graphic or historical biases buried in the training
data. Although the model releasers have attempted
their efforts to reduce offensiveness contexts and
biases in their training data, the model retains the
potential to generate output that triggers offensive
replies and might express agreement towards of-
fensive or unethical contexts. The reverse situa-
tion also applies, and the model might express dis-
agreement towards ethical contexts. However, due
to the fact that current state-of-the-art end-to-end
pre-trained dialogue generators or pre-trained lan-
guage models are mostly trained on large corpus
or conversations that naturally occur, the above-
mentioned issues are widely known to commonly
exist for these models. Either heuristics or neural-
based methods are suggested to be employed to
post-process the outputs to eliminate any potential
ethical issues presented by the models. Finally, we
declare that any biases or offensive contexts gen-
erated from the model do not reflect the views or
values of the authors.
","bias, offensive content","bias, offensive content",bias,2022,"statement, concerns"
191,"The PERSONA CHAT dataset used in this work is
well-known and widely used. In our view, there is
no known ethical issue with its usage. Large-scale
pre-trained models are also employed, but they are
widely known to be subject to potential problems
such as generating offensiveness context. With its
use, our partner personas generator could generate
unseen personas, which are also subject to potential
offensive generation. An offensiveness check can
be incorporated to alleviate this problem for actual
usage (Baheti et al., 2021).
B Implementation Details
For supervised phase, we set Adam (Kingma and
Ba, 2015) as our optimizer, with hyperparameters
η= 5e−4,β1= 0.9,β2= 0.999,ϵ= 1e−8. The
models are fine-tuned for 2 epochs. For RL phase,
we set Adam as our optimizer, with η= 5e−6,
β1= 0.9,β2= 0.999,ϵ= 1e−8. We update the
model parameters every 20training instances and
validate the model performance every 50updates.
DistilBERT (Sanh et al., 2019) is used to initialize
the model parameters for the critic network. We
set Adam as our optimizer, with hyperparameters
η= 5e−6,β1= 0.9,β2= 0.999,ϵ= 1e−8.
We fine-tune the critic for 1 epoch, and we freeze
it empirically during RL. All the experiments are
conducted based on the TRANSFORMERS library
from H UGGINGFACE (Wolf et al., 2020).
C Analysis on Dialogue Reponse
Generation
We present the progressive change of the testing
perplexity for DRG and PPG on PERSONA CHAT-
ORIin Figure 4.4We observe that they improve
4The result is scaled for the sake of space and clarity.simultaneously, which supports our motivation to
use RL for joint training.
D Human Evaluation Criteria
•(Appropriateness) :""Who is more appropri-
ate given the previous dialogue context?""
•(Informativeness) :""Who is more diverse in-
stead of null answers such as I do not know?""
•(Engagingness) :""Who would you prefer to
talk with for a long conversation?""
•(Human-likeness) :""Which speaker do you
think sounds more like a real person?""
•(Coherence) :""Which persona contains traits
that are more coherent to each other?""
•(Interestingness) :""Which persona is more
interesting and diverse?""
The first four are from the existing work (Li et al.,
2019; Zou et al., 2021) and we propose the last
two for evaluating PPG. We report the first four for
DRG, and we report the last four for PPG.
E Dataset
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
192,"It should be mentioned that all data, including doc-
tors’ proﬁles, patients’ queries, and doctor-patient
dialogues, are collected from the openly accessible
online health forum Chunyu Yisheng whose owners
make such information visible to the public (while
anonymizing patients). Our dataset is collected
by a crawler within the constraints of the forum.
Apart from the personal information de-identiﬁed
by the forum ofﬁcially, to prevent privacy leaks, we
manually reviewed the collected data and deleted
sensitive messages. Additionally, we replaced each
doctor’s name with a unique code randomly gen-
erated to distinguish them while protecting their
privacy. We ensure there is no identiﬁable or offen-
sive information in the released dataset.
The dataset, approach, and model proposed in
this paper are for research purposes only and in-
tended to facilitate studies of using NLP methods
for doctor expertise learning and recommendation
to allow a better user experience on online health fo-
rums. We also anticipate they could advance other
NLP researches like question answering (QA) in
the biomedical domain.",no ethical concerns,no_ethical_concerns,privacy,2022,"statement, actions"
193,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
194,"This work focuses on a limited view of the whole
RE research ﬁeld. Our dataset survey excludes spe-
ciﬁc angles of RE such as temporal RE or bioNLP,
as they are large sub-ﬁelds which warrant a ded-
icated analysis in itself. From a methodological
point of view, in our analysis we did not further
cover weakly-supervised (e.g., distant supervision)
and un-supervised approaches. Finally, given that
our study points out gaps in RE, speciﬁcally cross-
dataset, our experiments are still limited to RC only
and next steps are to extend to the whole pipeline
and to additional datasets and domains.
The data analyzed in this work is based on exist-
ing publicly-available datasets (based on published
research papers).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
195,"Our Mturk data collection recruited annotators
from across the globe without any constraints on
user demographics. The annotators were compen-
sated with above minimum wages and no personal
information was collected from the annotators.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
196,"The datasets used in this paper are open datasets
and do not involve any ethical issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
197,"We note that the gold data used for this project was
collected at a university counseling center at a uni-
versity in the western United States. This induces a
demographic bias in the data. It is highly possible
that this data is neither representative of the various
dialects of the English language spoken around the
globe, nor of mental health concerns in the broader
population. Our models are built using pre-trained
language models, which, by design, are opaque.
Consequently, our results are not interpretable.
The data was anonymized to protect information
disclosures. Text snippets have been paraphrased
by a Psychology graduate to mask stylistic cues.",bias,bias,bias,2022,statement
198,"Several limitations of our study should be taken
into account when considering results in a wider
context. A key issue in building a general self-
disclosure models was the differing labels based on
differing deﬁnitions of self-disclosure across the
data sets considered. (This is a common problem
in computational social science, where constructs
such as ""happy"" or ""liberal"" are often measured
using widely different measures, see Casper et al.
(2018) for more information). It needs to be taken
into account that we assume in our paper that
the different notions of self-disclosure across the
considered data sets approximate the deﬁnition of
self-disclosure validated in psychological literature.
However, the data sets we took into account were
not annotated based on such validated deﬁnitions
but rather had differing labeling instructions,
which might lead to inaccuracies when predicting
’true’ self-disclosure. In future work, data that is
labelled for a validated self-disclosure deﬁnition
should be collected and analyzed. We further only
tested a limited number of multi-task models. In
future work, we’d suggest investigating these in
more detail, which would further contribute to
explaining why our multi-task model outperformed
the single-task model.
In addition, we have not studied how self-
disclosure prediction differs among different cul-
tures, genders and races. Speciﬁcally, it is unclear
how well our recommended general self-disclosure
model applies to speciﬁc subgroups. For exam-
ple, women tend to self-disclose more and express
more emotional content than men (Sheldon, 2013).
Whether this suggests that different models of self-
disclosure would be helpful for men and women is
less clear. Similarly, the amount of self-disclosure
varies widely across settings and cultures. How
this affects models is similarly unclear. These vari-
ations should be studied in a subsequent research
project. Secondly, our training corpora included
mostly native English speakers and hence might
not generalize well to non-native speakers. Fi-
nally, self-disclosure detection could be used for
unethical targeting, e.g. in the context of insurance
companies who want to discriminate on prices for
people who don’t self-disclose much, given that
self-disclosure can inﬂuence relationships and sub-
sequently the mental health of a person. The ap-
plication of our model for such usages is strongly
advised against.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, concerns, suggestions"
199,"We have veriﬁed that all licenses of source datasets
used in this paper allow for their use, modiﬁca-
tion, and redistribution in a research context. The
dataset will be distributed in a manner similar to
SuperGLUE (Wang et al., 2019) i.e. give full credit
assignment to the original data and task creators.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
200,"We use existing public-domain text corpora such
as Wikipedia, ROC Stories, and MS-COCO, and
follow the protocol to use and adapt research data
to generate our weakly-labeled dataset. We will
release the code to generate our dataset. Any bias
observed in NLI systems trained using our methods
can be attributed to the source data and our trans-
formation functions. However, no particular socio-
political bias is emphasized or reduced specifically
by our methods.
",bias,bias,bias,2022,"statement, actions"
201,"Our curated dataset is available at https://
github.com/fpsluozi/tofindwaldo . We will
also follow the same licensing and data sharing
policy as the original Who’s Waldo dataset.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
202,"We use existing public-domain text datasets, such
as SNLI, Winogrande, and ARC, and follow the
protocol to use and adapt research data to compute
instance-level difficulty scores. We will release the
computed difficulty scores, but will not share the
original source data. We recommend readers to
refer to the original source research papers. Any
bias observed in difficulty scores computed using
our methods can be attributed to the source data
and our computation functions. However, no partic-
ular socio-political bias is emphasized or reduced
specifically by our methods.
",bias,bias,bias,2022,statement
203,"We acknowledge the ACL Code of Ethics. In partic-
ular, we only use well-known benchmark datasets
for our evaluation. Both the Wikia and the Reddit
dataset do not include personal data, such as infor-
mation about the authors of the posts (Logeswaran
et al., 2019; Botzer et al., 2021). The list of overlap-
ping entities that we will publish does not contain
any privacy-related or IP-related content either.
5The Bosch Group is carbon neutral. Administration, man-
ufacturing and research activities do no longer leave a carbon
footprint. This also includes GPU clusters on which the exper-
iments have been performed.190
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
204,"This work utilizes public discussions by private
individuals on Twitter. The tweets were collected
with the Twitter streaming API and all information
of the tweeting users, including user names and
ids, was discarded. However, sensitive information
is also found within the analyzed texts, none of
which are made public. The tweets are stored with
restricted access and will be deleted upon research
conclusion. Afterwards only the tweet ids will
be available, which can be hydrated through the
Twitter API only for tweets that still are public.
BComputing Infrastructure & Runtimes
A Nvidia GeForce RTX 2080 Ti graphics card was
used for the training and evaluation of the probing
models. The hyperparameter search with 50 runs
lasted 28 minutes on average and the ﬁnal mod-
els were optimized with an average of 6 minutes
training time.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
205,"While the dialogues in our dataset are grounded
on both structured and unstructured data, they are
limited to tables and text and do not cover other
forms such as knowledge graphs. Additionally, the
conversations are limited to discussions on single
Wikipedia pages. We believe future research can
expand on this for the creation of more open-ended
information-seeking dialogues.
Wikipedia has extensive measures of risks and
employs staff and volunteer editors to make sure
Wikipedia articles meet the requirement and quality
of the Wikimedia Foundation. Our data is based
on Wikipedia pages, and we contain our dialogues
to Wikipedia knowledge. We carefully validate the
dataset collection process, and the quality of our
data is carefully controlled.
The HYBRI DIALOGUE dataset was built from
the OTT-QA dataset, which is under MIT license.
The authors of the OTT-QA dataset paper have
allowed us to utilize the dataset within our use
case.
For the dataset collection task, we required Turk-
ers to have a HIT Approval Rate of greater than
96% and be located in AU, CA, IE, NZ, GB, or
the US. We also required workers to have had 500
HITs approved previously. Workers were shown an
interface containing text input fields and navigationtools. Turkers were also given an instruction page
containing a video demo and a completed example.
The time to complete the task is around 5 minutes,
and Turkers were paid $1.1 per conversation, which
translates to an hourly wage of $13.2 per hour. For
the human evaluation task, Turkers were paid $0.1
per task with an estimated time of fewer than 30
seconds per task. The dataset collection protocol
was approved by the IRB. We follow the user agree-
ment on Mechanical Turk for our dataset creation,
which gives us explicit consent to receive users’
service in the form of data annotation in return for
monetary compensation. Given our settings, the
Turkers understand that their data will be utilized
in machine learning research.
We will be providing open access to our dataset
for use in future research. This includes the sam-
ples of dialogues written by Mechanical Turk work-
ers, the references that each dialogue turn is asso-
ciated with, and the Wikipedia pages in which the
references are located. The dataset will be open-
sourced under the MIT License.
7
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions"
206,"8.1Dataset
We received permission from the original own-
ers of the datasets, NewsSniffer and DiffEngine.
Both sources are shared under strong shar-
ing licenses. NewsSniffer is released under
anAGPL-3.0 License ,26which is a strong
“CopyLeft” license. DiffEngine is released
under an Attribution-NoDerivatives 4.0
International license.27
Our use is within the bounds of intended use
given in writing by the original dataset creators,
andiswithinthescopeoftheirlicensing.
8.2Privacy
Webelievethattherearenoadverseprivacyimpli-
cationsinthisdataset. Thedatasetcomprisesnews
articlesthatwerealreadypublishedinthepublic
domainwiththeexpectationofwidespreaddistri-
bution. Wedidnotengageinanyconcertedeffort
to assess whether information within the dataset
waslibelious,slanderousorotherwiseunprotected
speech. Weinstructedannotatorstobeawarethat
thiswasapossibilityandtoreporttousiftheysaw
anything,butwedidnotreceiveanyreports. We
discussthismorebelow.
8.3
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
207,"Most of our new gold standard data were created
with the help of crowdsourcing. All crowdwork-
ers were compensated following the wage recom-
mended by the crowdsourcing platform Prolific (i.e.
$9.60 per hour). Since we were aware of the offen-
sive nature of the data that the crowdworkers had
to annotate, we inserted a respective warning in the
task advertisement. In order to keep the psycho-
logical strain of the crowdworkers at an acceptable
level, the data to be annotated was split into bins
of 100-200 instances. Furthermore, we allowed
each crowdworker to take part in one single task
only. We also made it very clear in the task descrip-
tion that we follow a linguistic purpose with our
crowdsourcing tasks and the opinion expressed in
the sentences to be annotated in no way reflects the
opinion of (us) researchers designing the tasks.
One of our crowdsourcing tasks included invent-
ing sentences in which a group of people is framed
as a perpetrator (§5.2). Since we did not want
crowdworkers to invent any anti-Semitic, homo-
phobic, Islamophobic or misogynist content, we
introduced the name of a fictitious people which
the crowdworkers were to use in their sentences.
We also made sure that the particular name did not
have any obvious phonetic resemblance to existing
identity groups. Although the resulting sentences
being invented are not directed against any existing
identity groups they may still be considered abu-
sive. However, we think that this is justifiable in
this particular context since we are not aware of any
existing dataset that contains a similar content (i.e.
a focused dataset for learning perpetrator-evoking
verbs) that we could have used for our experiments.
In principle, creating morally disputable content
as part of research is not unusual. Both in plagia-
rism detection (Potthast et al., 2010), deception
detection (Ott et al., 2011) and, quite recently, abu-
sive language detection itself (Vidgen et al., 2021b;
Wiegand et al., 2021a) a procedure similar to ours
was pursued.
One substantial part of the data we are going
to make publicly available as part of this research
will include sentences extracted from Twitter. In
order to protect the privacy rights of the authors
of the tweets and individuals mentioned in them,
we anonymized our data by discarding mentions of
usernames. The public release of a limited number
of tweets as in the range of our dataset is also in
accordance with the regulations of Twitter.A datasheet describing our novel dataset of la-
beled sentences for the task of detecting implic-
itly abusive remarks about identity groups (both
English and German version) following the spec-
ification of Gebru et al. (2018) was added to the
supplementary material.
Our current data focuses on the four identity
groups Jews ,Muslims andgay people andwomen .
This choice was mainly motivated by the fact that
these groups are among the most abused identity
groups on social media. As a consequence, it was
also possible to obtain a reasonable amount of data
(even with our restrictive measures to ensure less bi-
ased datasets). Moreover, these identity groups are
well represented in existing datasets. This allows us
to compare our proposed classifier against baseline
classifiers trained on these existing datasets. We ac-
knowledge that abusive language on the web is also
directed against other identity groups. We leave
their automatic detection to future work. How-
ever, our study suggests that abusive language that
targets these other identity groups will follow the
same language patterns as the instances of abusive
language examined in this paper.

",abusive content,abusive content,manipulation,2022,"statement, concerns, actions"
208,"This research was approved by the Helsinki Ethical
Review Board (IRB) of the Be’er Ya’akov–Ness
Morph. Category Morph. Participants were
guaranteed anonymity. The data was stored on a
secured server, with limited access provided only
to the authors of this paper.
Like with every other machine-learning model,
there is a risk that the training data is unbalanced.
Specifically, we do not intentionally balance the
dataset for ethnicity or political affiliation. More-
over, this work is based on interviews with men
only. Additionally, the language model that we
use, AlephBERT, was trained on large and less
controlled datasets. That may introduce some ad-
ditional aspects of bias. Therefore, our study may
harbor the danger of over-reliance on possibly bi-
ased machine tools.
We do not mean to suggest that an algorithm
can or should be used to diagnose schizophrenia
automatically. This study should not be considered
as a building block for an apparatus that takes au-
tomatic decisions about topics related to mental
health. Our intention is, rather, to use computa-
tional tools to identify and study the importance
of various linguistic characteristics for diagnosing
schizophrenia. Like other machine-learning ap-
plications, explainability is currently a problem-
atic issue (what is it about the usage of nouns that
contributes significantly to the model’s success in
classification?), and undue reliance on machine
classification should be eschewed.","bias, explainability","bias, lack of explainability","bias, transparency",2022,"statement, actions, concerns"
209,"Our work concerns the German Sign Language
(DGS) and is only a part of a bigger research line
that intends to provide communication improve-
ments for the the deaf and hard of hearing com-
munities. In order to respect these communities
and ensure proper representation of their interests,
members of them have been included in our project
as part of the research team, consultants or par-
ticipants in user studies and ELSI workshops (e.g.
Nguyen et al., 2021), as per the recommendations
of the SLLS ethics statement. The isolation of
glosses, known to be inferior to the full linguistic
capacity of the sign language does not intend to
simplify the language but is rather used as a tool
for aiding further research.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
210,"While our work does not introduce a new dataset, it
does depend on a training dataset that was collected
from ﬂuent English-speaking, able-bodied human
subjects. If deployed in a real-world application,
this model would likely perform noticeably worse
for users who speak with non-American accents or
speech impediments. Transcripts for these users
could be disproportionately noisy and the stream-
ing model’s average wait time would likely also be
longer. Care should be taken to assess the sensitiv-
ity and robustness of such a model to non-ﬂuent or
non-American English prior to deployment. This
model should also be used very cautiously in situ-
ations where mistakenly eliding ﬂuent portions of
speech from the captions or transcript could incur
dire consequences, such as in an emergency call
center.
8
","sensitivity, robustness",English-centric,no ethical concerns,2022,"statement, suggestions"
211,"The main objective of this work was to build a
quality large-scale corpus that could be used to
learn automatic summarization neural models for
Catalan and Spanish. To achieve this objective, we
selected a set of Spanish news sites, including from
Spanish mass media to regional newspapers, and
we collected as many data as possible from them.
To increase the quality of the corpus, we filtered
the article-summary pairs following basic statistics
of the text. However, we did not apply any kind
of content filtering. Therefore, our filtering could
include biased content such as political tendency,
geographic imbalance or gender biases (Stanczak
and Augenstein, 2021). A future direction towards
improving the dataset quality would be to alleviate
that biases, for example, by means of deduplicat-
ing content, augmenting artificially the samples to
balance gender (Sun et al., 2019), politics, and ge-
ographic aspects, or either manually selecting an
unbiased subset of the dataset.
The articles collected in the dataset are un-
der Creative Common or private licenses. Nowa-
days, we are working on obtaining authoriza-
tion for the distribution of all sources. Those
newspaper sources under Creative Common li-
cense or the private ones with authorization
are freely provided. DACSA can be re-
quested at https://xarrador.dsic.upv.
es/resources/dacsa .",bias,bias,bias,2022,"statement, concerns, suggestions"
212,"We believe our work makes a positive impact by
focusing heavily on the need for privacy consid-
erations when exploring low-resource settings for
semantic parsing. However, as our methods rely
heavily on large pretrained language models such
as GPT3, we may inherit similar biases which such
models are known for (Brown et al., 2020).
",bias,bias,bias,2022,"statement, concerns"
213,"Our work heavily relies on OpenAI’s GPT-3 and
Codex models, which are large language models
trained on big datasets. Such language models may
reﬂect biases present in their training data ( Brown
et al. ,2020 ;Bender et al. ,2021 ). However, our
use of constrained decoding largely mitigates the
risks from such bias as we only allow the model to
generate outputs allowed by a small grammar. Fur-
thermore, the outputs are interpreted by machines
rather than directly shown to humans. The potential
for harm may increase when the grammars used in
constrained decoding allow for a wider variety of
outputs (such as including unconstrained free-text
ﬁelds), and if semantic parsing is used for particu-
larly sensitive domains.
",bias,bias,"bias, misuse",2022,"concerns, statement, actions"
214,"The data used in this paper was collected in ac-
cordance with applicable policies, terms of use,
privacy notices, and customer privacy settings that
disclose to customers how their data may be used.
The annotators of the data were compensated for
their work consistent with applicable laws and reg-
ulations.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
215,"The goal of this paper is to enable a wider audience
of readers to understand and engage with scien-
tiﬁc writing. A risk, though, is that such attempts
might instead widen the gap to accessing scientiﬁc
information. The texts in the datasets we train our
models on are in General or Academic American
12https://github.com/talaugust/
definition-complexity
English. Many people, especially those who have
been historically underrepresented in STEM disci-
plines and medicine, may not be comfortable with
this dialect of English. This risks further alienating
the readers we hope to serve. This is a common
issue in NLP systems (Sap et al., 2019), since the
majority of datasets are in General American En-
glish. An important and exciting direction in NLP
is making models more ﬂexible to dialects and low-
resource languages (e.g., the ACL 2022 special
theme being “Language Diversity”).
While our results suggest that the lighter control
of reranking generations leads to less hallucinated
information, strong supervision of deﬁnition fac-
tuality is important for any future deployment of
such a system. While hallucinated information
can be damaging in any generation context, in-
correct scientiﬁc deﬁnitions could mislead read-
ers and potentially contribute to broader scientiﬁc
misinformation. Furthermore, a bad actor could
use these models to generate ﬂuent but incorrect
deﬁnitions at scale, potentially contributing to mis-
information campaigns with a veneer of scientiﬁc
language (Britt et al., 2019). We trained our models
on data we believe is trustworthy (e.g., questions
and answers from NIH websites); and we release
our training data and models to allow for further
work on encouraging factuality in these model gen-
erations.","English-centric, hallucinations, misinformation","English-centric, hallucination, misinformation",manipulation,2022,"concerns, suggestions, actions"
216,"This work seeks to develop a structure-aware equiv-
ariance learning framework for table-to-text genera-
tion. Since the proposed method focuses on improv-
ing prior generation systems by better utilization
of structural information, it does not introduce bias
towards specific content. The distinction between
beneficial use and harmful use depends mainly on
the data. Proper use of the technology requires that
input corpora are legally and ethically obtained.
We conduct experiments on two open benchmark
in the way they intended to. Although we create
a harder version of ToTTo dev set, the table trans-
formation operations we use are content-invariant,
whereas the ground-truth generation remains the
same as it is in the original dataset, ensuring no
further social bias is introduced.
",no ethical concerns,no_ethical_concerns,"bias, misuse",2022,"statement, actions"
217,"The New York Times daily crossword puzzles are
a copyright of the New York Times. We have ob-
tained preliminary approval from the New York
Times to release this data under a non-commercial
and research use license, and are in the process of
ﬁnalizing the exact licensing terms and distribution
channels with the NYT legal department.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
218,"In consideration of ethical concerns, we provide
the following detailed description:
1.All of the collected documents and event
descriptors come from publicly available
sources. The legal advisor of our institute
and/or the original dataset constructor con-
firms that the sources of our data are freely
accessible online without copyright constraint
to academic use.
2.ECO Bank contains 5000 annotated docu-
ments with 988 event descriptors. After
double-checking, we guarantee that ECO
Bank doesn’t contain samples that may cause
ethic issues. The dataset does not involve any
personal sensitive information. All references
in the annotated data are double-checked for
plausibility and grammaticality by different
human annotators. All documents and event
descriptors are also manually checked to en-
sure they are informative and logically coher-
ent. We manually check the content of each
piece of data in ECO Bank to ensure that it
does not contain any hate speech or attacks on
vulnerable people.
3.We hired 5 annotators who have bachelor de-
grees. Before formal annotation, annotators
were asked to annotate 20 samples randomly
extracted from the dataset, and based on aver-
age annotation time we set a fair salary (i.e.,
35 dollars per hour) for them. During their
training annotation process, they were paid as
well.
",no ethical concerns,no_ethical_concerns,security,2022,"statement, actions"
219,"Ethics institutional review board (IRB) approval
was obtained from the corresponding ethics board
of the University of Warwick prior to engaging in
this research study. Our work involves ethical con-
siderations around the analysis of user generated
content shared on a peer support network (Talk-
Life). A license was obtained to work with the user
data from TalkLife and a project proposal was sub-
mitted to them in order to embark on the project.
The current paper focuses on the identification of
moments of change (MoC) on the basis of con-
tent shared by individuals. These changes involve
recognising sudden shifts in mood (switches or es-
calations). Annotators were given contracts and
paid fairly in line with University payscales. They
were alerted about potentially encountering dis-
turbing content and were advised to take breaks.
The annotations are used to train and evaluate nat-
ural language processing models for recognising
moments of change as described in our detailed
guidelines. Working with datasets such as TalkLife
and data on online platforms where individuals
disclose personal information involves ethical con-
siderations (Mao et al., 2011; Keküllüo ˘glu et al.,
2020). Such considerations include careful analysis
and data sharing policies to protect sensitive per-
sonal information. The data has been de-identified
both at the time of sharing by TalkLife but also by
the research team to make sure that no user handles
and names are visible. Any examples used in the
paper are either paraphrased or artificial. Poten-
tial risks from the application of our work in being
able to identify moments of change in individuals’
timelines are akin to those in earlier work on per-
sonal event identification from social media and
the detection of suicidal ideation. Potential mitiga-
tion strategies include restricting access to the code
base and annotation labels used for evaluation.",privacy,privacy,privacy,2022,"statement, concerns, actions"
220,"We collect our data from public datasets that per-
mit academic use. The open-source tools we useare freely accessible online without copyright con-
ﬂicts.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
221,"This work deals with user-generated text in the
medical domain. However, our work and models
should not be used as text understanding tools for
real-life medical systems without human supervi-
sion and verification. Our system is not error-free,
and using it could lead to a misunderstanding of
the true intentions of people seeking medical care.
","errors, misunderstandings",no_ethical_concerns,no ethical concerns,2022,"statement, suggestions, concerns"
222,"Masking Sensitive Information
Due to the nature of anonymous networks such as
Tor, raw data posted on the Dark Web may contain
private or illegal information. Such information
includes (but is not limited to) Bitcoin addresses,
credit card information, and social security num-
bers. Since CoDA was compiled by randomly se-
lecting web documents from the Dark Web, the
dataset may contain such information. Therefore, it
is important that a public Dark Web dataset such as
CoDA addresses ethical issues regarding sensitive
information.
To prevent the use of CoDA for malicious pur-
poses such as the extraction of sensitive informa-
tion, we identify types of potentially sensitive data
(such as email, IP, URL, crypto addresses, and
social security numbers) which are subsequently
masked (refer to Section 3.6 for the detailed de-
scription on mask identiﬁers used). These identi-
ﬁers are matched using regular expressions and
each page has been manually double-checked on
whether such content has been properly masked
by the authors. During this time, the authors did
not ﬁnd sensitive content outside of the masked
information.
As mentioned in Section 4, some data analysis
methods are conducted with unmasked versions of
the Dark Web to prevent bias. However, we only
use the unmasked version of CoDA for a fair anal-
ysis between the Dark and the Surface Web data,
and do not utilize or disclose information found in
the unmasked data in any way.
Handling Illegal Content
A signiﬁcant portion of the Dark Web deals with
explicit, pornographic content (violence, child
pornography, torture, etc.). The act of accessing
or viewing such media is illegal by law in many
parts of the world. To prevent the access of such
media, we collect crawled Dark Web pages in the
form of HTML and parse HTML tags to retrieve
only the text data. In addition, URL addresses and
onion addresses that may link to such illegal media
are also masked as previously mentioned (note thatall URL addresses and onion addresses have been
masked, regardless of their content). Consequently,
the authors and the annotators do not have access
to media that are illegal by law.
It is worth noting that CoDA still contains texts
of various activities that occur in the Dark Web,
some of which are illegal in nature (drug trade,
counterfeit products, etc.). However, the inclusion
of text in the dataset that describes potentially ille-
gal activities is not of ethical concern. Therefore,
we do not censor text data that correspond to illegal
activities.
Ethics on Annotation
Dark Web content often contains sensitive and il-
licit activities. Dealing with such content during the
annotation process may be unsettling for some peo-
ple, so we chose annotators who are experienced
with the Dark Web and has given consent to being
exposed to such content. The annotators recruited
for classifying CoDA were specialists who work
at a cyber threat data analytics & intelligence com-
pany specializing in Dark Web data. To ensure that
the annotation process is fair, each of the ten anno-
tators handled the same number of pages and were
given equal compensations.
Preventative Measures to Discourage
Non-Academic Use of CoDA
The content of CoDA includes text descriptions
of various Dark Web activities. A potential harm
of releasing this dataset is bringing increased at-
tention to these activities. We strongly believe
that our research should be used for scientiﬁc pur-
poses only and discourage non-academic use of
CoDA. We take a preventative approach by only
permitting access to CoDA to researchers with re-
search purposes that abide by the ACL Code of
Ethics. The terms of use agreement can be found
athttps://s2w.inc/resources/coda .
","bias, misuse, consent","privacy, misues","privacy, bias, misuse",2022,"statement, concerns, suggestions"
223,"We hereby acknowledge that all of the co-authors
of this work are aware of the provided ACM Code
of Ethics and honor the code of conduct. This
work is mainly about the pretraining and multi-
task learning of LMs for structural prediction. The
followings give the aspects of both our ethical con-
siderations and our potential impacts to the com-
munity. This work uses LMs, for which the risks
and potential harms are discussed in (Brown et al.,
2020). There are potential undesirable biases that
existed in task-agnostic data (e.g., from Wikipedia)
and multi-task downstream datasets (mostly cre-
ated from news articles). We do not anticipate the
production of harmful outputs, especially towards
vulnerable populations, after using our model or
training NLP models on our datasets.
8 Environmental Considerations
We adopt the pretrained LMs from the (Du et al.,
2021), whose energy cost and carbon footprint dur-
ing pretraining were 80.6 MWh and 4.6 tCO2e,
respectively. Additionally, the structure pretraining
takes less than 5% gradient-steps of the number of
pretraining steps of LMs, and thus the estimated
auxiliary cost for energy is comparatively smaller.
In addition, training and tuning pretrained LMs
on a wide range of tasks and datasets consume a
plenitude of energy and increase emissions of car-
bon dioxide. To alleviate the problem, in this work
we make efforts to study the multi-task training,
which only involves training on a combination of
all datasets at once. Our results (e.g., Figure 6)
show that, despite the gap between multi-task and
multi-task finetune on smaller models, the perfor-
mance gap becomes minor when the model size
scales up to 10 billion parameters. This indicates
","bias, misuse, environmental_impact","bias, environmental impact","bias, misuse, environmental_impact",2022,"ststement, concerns, actions"
224,"For years the press has been arguing the use of AI
and its pros and cons. One advance could be used
in various ways and thus lead to different outcomes.
To take a cultural look at how this work and other
works in similar tracks will take effect, we would
like first to take a brief on how might our work
be used in both good and bad ways, then move
on to applying our advance and ethical reasons for
developing our toolkit, along with privacy issues.
For our demo, the outcome can shift in between
justice and harmful outcomes. EL could be viewed
as having an expert to extract key concepts from
a given text, which means that it could be used in
education to help the students find a related term
in their reading before they fully understand the
field. This could also be used in specialized do-
mains such as biological and pharmaceutical for
fast retrieval of useful concepts (Marrone, 2020).
However, this could also be used in harmful ways.
The chance of EL being used for detecting particu-
lar views in social media might be further applied
to ban a specific group from expressing opinions,
harming freedom of speech and equality. But if we
look at it from a different perspective, if such use
could be controlled by the users of the social media,
potentially people who have difficulties can filter
out the harmful languages to them (Thuraisingham,
2020) and find what they wanted faster.
To ensure our work could be used in the right
way, we extracted our domain from XLore, where
it’s only a general KB without the worry of having
harmful potential entities. We also separated the
features. This raises the challenge to train the rank-
ing model to favor a specific semantic pattern and
thus makes it harder to be used against free speech.
Our work purely ranks the similarity in context
rather than learning the complete set of all entities,
this could prevent the linking result from being
biased to only popular entities (Sciavolino et al.,
2021), yet we still worry that specific context might
lead to the linking of only popular entities. As a
result, we strongly call for more work conducted
to study the context and candidate similarity in re-
trievals instead of joining the popularity into final
performance for better equality. We noticed that in
recent years the protection for minor languages has
finally drawn more attention (Zhang et al., 2021b),
and one of the reasons for conducting our demon-
stration is the attempt in calling both English and
Chinese, the two most popular languages, speakers
to better consider the difference in languages and
robustness while developing methods not only for
the sake of equality between languages but also for
better protection of the minor languages because
not all languages are like English.
On the other hand, privacy has raised a signifi-
cant portion of attention (Tucker, 2019). It is essen-
tial to discuss how our tool might relate to privacy.
Our demo is based on XLore, which means it only
uses data publicly available on the internet, but the
risk of privacy leakage still remains while being
applied to the downstream tasks. During usage, a
level of caution to prevent privacy issues should
still be kept for the sake of respect. In advance, we
suggest usages of our method to be checked before
actual deployment in a downstream system.
One overall solution to resolve the release of
methods in data science is to discuss and consider
the caution of ethics and respect during education.
Some might argue the key is to take extra care dur-
ing the development of such tools but to notice
critical factors in a system that might lead to harm-
ful usage requires strong integrity and respect to
others. It is only with high ethical standards one
could better consider the design and take better con-
sideration of one’s system during the design and
before releasing.223","bias, misuse",,"bias, misuse",2022,"statement, concerns, suggestions"
225,"Ethical Dialogue Systems
We acknowledge the potential risks inherent in the
deployment of goal-oriented dialogue systems, and
especially note that care must be taken to ensure
persuasive dialogue systems are designed for bene-
ficial use as discussed by Wang et al. (2019). Con-
cretely, when applying our framework, care must
be taken to ensure that the goal of the system (de-
fined by the primary success attribute of the accept-
ability score) should be generally accepted as ben-
eficial. For example, our basis for dialogue accept-
ability in this work is with respect to raising money
for children’s charity. In general, the achievement
of the system’s goal should not intentionally lead
the user or any other party to harm. Additionally,
the definition of acceptability, through its primary
or any other correlated attributes, should not al-
low for discriminative responses, purposefully ma-
licious discourse, or other violations of accepted
ethical standards. For example, we include senti-
ment as secondary attributes in the acceptability
score, which, when applied via dialogue rollouts,
encourages the system to be courteous, polite, and
respectful. It is possible with minimal effort to in-
clude further secondary attributes that identify bias,
hate speech, and other indicators to help the system
remain safe to use.
Annotator Compensation
All manual annotators were recruited on a volun-
tary basis in an educational setting and did not
receive or expect monetary compensation. Specifi-
cally, two graduate students and one postdoc in our
lab served as our annotators.
environmental_impact
All training and inference in this work was done
with two NVIDIA Quadro RTX 8000 GPUs. The
most compute-intensive portion of the work was
the additional domain adaptation pre-training for
RoBERTa-large-adapted (see Section 3.2), which
took approximately two weeks. After that the multi-
seed self-play evaluations took approximately four
days, and all other operations (e.g., training and
evaluating PF models, fine-tuning DialoGPT) took
24 hours or less.
","bias, misuse, environmental_impact","harm, misuse","bias, misuse, environmental_impact",2022,
226,"As we propose a new multi-lingual dialogue sum-
marization dataset and conduct experiments based
on large pre-trained language models, we make
several clarifications to address potential concerns:
•Dataset: Since our MSAMSum is derived
from the SAMSum (Gliwa et al., 2019), which
is a well-constructed and human-labelled
dataset. Therefore, our dataset inherits the
contents of SAMSum and does not contain
toxic information.
•Model: The experiments described in this
paper are based on the mBART-50-large (Tang
et al., 2020) and make use of V100 GPUs.
Despite we run dozens of experiments, our
results could help reduce parameter searches
for future works. We also consider to alleviate
such resource-hungry challenge by exploring
light-weight distilled models.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
227,"We acknowledge that there is some risk, like any
other technology that makes inferences at the in-
dividual level, that the technology presented here
will be used for harm. However, due to the public
nature of the content and the fact that the potential
harm already exists using basic keyword search,
we believe that the marginal risk added by our clas-
sifier is minimal.
",misuse,misuse,misuse,2022,statement
228,"The fact that even the largest LMs appear to fol-
low yet do not actually follow users’ instructions
has important implications, especially considering
the increasing commercial use of LMs. While tra-
ditional ﬁne-tuned models also pose challenges
in interpretability, with prompt-based models, an
illusion of instruction following can be more per-
nicious than having no instructions at all. The in-
tuitive interface that prompts provide might make
them more accessible to lay users, and can mis-
lead users to think that their instructions arebeing
understood and followed. Our results suggest that
cautions are needed even more than they were with
traditional ﬁne-tuned models.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement 
229,"8.1 B IGNEWS Collection
All news articles were collected in a manner con-
sistent with the terms of use of the original sources
as well as the intellectual property and the pri-
vacy rights of the original authors of the texts, i.e.,
source owners. During data collection, the authors
honored privacy rights of content creators, thus did
not collect any sensitive information that can re-
veal their identities. All participants involved in the
process have completed human subjects research
training at their affiliated institutions. We also con-
sulted Section 1078of the U.S. Copyright Act and
ensured that our collection action fell under the fair
use category.
8.2 Dataset Usage
All of the newly collected datasets in this work will
be made available upon request. Pretraining corpus
details are included in Section 3. The other seven
datasets used for downstream evaluation are ob-
tained in the following ways. CongS ,HP,BASIL ,
V AST andSEval are acquired by direct download.
CongS is released under the ODC-BY 1.0 license
(free to share, create, and adapt). HPandSEval are
developed in shared tasks by the NLP community,
which allow the use of copyrighted material with-
out permission from the copyright holder for re-
search purposes (Escartín et al., 2017). For V AST ,
the author explicitly states “We make our dataset
and models available for use”. BASIL is devel-
oped by the last author and her collaborators. For
YTandTW, we consult with the corresponding
authors and obtain the datasets by agreeing that we
will not further distribute them. Dataset details are
listed in Section 5.1 and Appendix D.
8.3 Benefit and Potential Misuse
Intended use. The models developed in this work
can assist the general public to measure and under-
stand ideological language used in diverse genres
of texts. For example, POLITICS can help the
general public know where their representatives
stand on key issues. Our experiments in Section 5
demonstrate how POLITICS would be deployed
in real life when handling applications in both ide-
ology prediction and stance detection. We deem
that our extensive experiments have covered the
major usage of POLITICS.
Failure mode is defined as situations where POL-
ITICS fails to correctly predict the ideology of an
individual or a given text. In such cases, POLI-
TICS might deliver misinformation or cause misunderstanding towards a political figure or a pol-
icy. For vulnerable populations (e.g., people who
maybe not be able to make the right judgements),
the harm could be tremendously magnified when
they fail to interpret the model outputs or blindly
trust machine responses. Ideally, the interpretation
of our model’s predictions should be carried out
within the broader context of the source text.
Misuse potential. Users may mistakenly take the
machine prediction as a golden rule or a fact. We
would recommend any politics-related machine
learning models, including ours, put up an “use
with caution” message to encourage users to check
more sources or consult political science experts
to reduce the risk of being misled by single source.
Moreover, POLITICS might also be misused to
label people with a specific political leaning that
they do not want to be associated with. We suggest
that when in use the tools should be accompanied
with descriptions about their limitations and imper-
fect performance, as well as allow users to opt out
from being the subjects of measurement.
Potential limitation. Although multiple genres are
considered, the genre coverage is not exhaustive,
and does not include other trending media or con-
tent of different modalities for expressing opinions,
such as TV transcripts, images, and videos. Thus,
the predictive performance of POLITICS may still
be under investigated. Further, in downstream eval-
uation, POLITICS is only trained and tested in
the same domain, so its cross-genre ability needs
further evaluation.
Bias Mitigation. During data preprocessing, we
create BIGNEWSBLN to ensure that all ideologies
have almost equal presence to minimize potential
bias. POLITICS is not designed to encode bias.
In Figure 6, the discrepancy in perplexities among
different ideologies is more related to the greater co-
herence among Republicans than Democrats, rather
than POLITICS encoding biased knowledge.
In conclusion, there is no greater than minimal
risk/harm introduced by either BIGNEWSBLN or
POLITICS . However, to discourage the misuse,
we will always warn users that model predictions
are for informational purpose only and users should
always resort to the broader context to reduce the
risk of absorbing biased information.1363","misinformation, misuse, manipulation","misuse, bias","misinformation, misuse, manipulation",2022,"statement, concerns, actions"
230,"The research has been designated by IRB at
NewYork-Presbyterian Hospital as Not Human
Subject Research. The Protocol Number is 20-
10022833.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
231,"The use of very large language models runs the risk
of exposing users to offensive or sensitive language
that might be contained in training data. Poten-
tial harms include, but are not limited to, offensive
references to classes of people and beliefs, encour-
agement of violence outside the game, and socially
inappropriate sexual references. Any implemen-
tation outside a sandboxed research environment
will need to build guardrails appropriate to the au-
dience and game environment, and especially to
provide protections for minors. In addition, im-
plementations must be able to handle adversarial
probes designed to elicit offensive language.
A further concern is that this technology may
make it easier for users to manipulate NPCs to per-
form in socially inappropriate ways or to construct
socially inappropriate objects. Longer-term, the
ability to enable users themselves to generate code
that can affect game state may pose security threats.
","offensive language, sensitive language, misuse","offensive language, misuse, manipulation",misuse,2022,"statement, concerns"
232,"In this work, we introduce a lightweight framework
for self-training of language models with only a
few labeled examples. We expect that progress and
findings presented in this paper could further bene-
fit NLP applications with limited labeled data. In
the real-world setting, it is usually not only expen-
sive to obtain large-scale labeled data for each task
but also brings privacy and compliance concerns
when large-scale data labeling is needed. The pri-
vacy concerns could be further exacerbated when
dealing with sensitive user data for peronslization
tasks. Our framework which only needs few-shot
labeled data could help in this to obtain state-of-the-
art-performance while alleviating privacy concerns.
The proposed framework is tested across different
tasks and could be used for applications in vari-
ous areas including finance, legal, healthcare, retail
and other domains where adoption of deep neural
network may have been hindered due to lack of
large-scale manual annotations on sensitive user
data.
While our framework advance the progress of
NLP, it also suffers from associated societal im-
plications of automation ranging from job losses
for workers who provide annotations as a service
as well as for other industries relying on human
labor. Moreover, it may bring additional concerns
when NLP models are used by malicious agents for
propagating bias, misinformation and indulging in
other nefarious activities. However, many of these
concerns can also be alleviated with our framework
to develop better detection models and mitigation
strategies with only a few representative examples
of such intents.
The proposed method is somewhat compute-
intensive as it involves large-scale language model.
This might impose negative impact on carbon foot-
print from training the described models. In order
to reduce the storage and training cost, the pro-
posed design tunes only a small number of adapter
parameters with few-shot labels while keeping the
large encoder frozen.
","unemployment, bias, misinformation","privacy, unemployment, misuse","unemployment, bias, misinformation",2022,"statement, concerns"
233,"With CONT AINER , we have achieved state-of-the-
art Few-Shot NER performance leveraging Gaus-
sian Embedding based contrastive learning. How-ever, the overall performance is still quite low com-
pared to supervised NER that takes advantage of
the full training dataset. Consequently, it is still not
ready for deployment in high-stake domains (e.g.
Medical Domain, I2B2 dataset), leaving a lot of
room for improvement in future research.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
234,"All the datasets that we use are publicly available.
We report only aggregated results in the paper. For
context mining, we have used data either available
in the public domain or available through official
APIs of the corresponding social media. Neither
have we, nor do we intend to share any personally
identifiable information with this paper. We also
make our codebase publicly available here - https:
//github.com/parag1604/CRUSH .
Our model CRUSH helps advance the state-
of-the-art in hate speech detection by incorporat-
ing continual pre-training, capturing user writing
biases and leveraging both user & post context
(which is publicly available as well). This in turn
should be able limit the amount of hateful threads
in social networks/media by better detection of
hate speech, thus promoting a more friendly and
welcoming environment for people from all race,
religion, ethnicity, gender etc.
Unfortunately, these models are not completely
free from all potential negative impact. One such
example being that our models have hate knowl-
edge infused within them during the CP phase
(refer Section 3.1) of our training pipeline. As
shown in Table 3, these language models could
be used potentially used for generating hateful
words/sentences given an initial prompt.
However, the hateful words generated by the
HateLM can be also identified by the platform if
the platform uses a hate speech detection algorithm
like ours or uses a set of hate lexicons to directly
filter out such keywords generated (Gitari et al.,
2015). Moreover, our final model - CRUSH - is
not exactly suitable for toxic language generation
as it is further pre-trained on the user style dis-
crimination task and fine-tuned on the hate speech
classification task making its classification capabil-
ities (potential use by the social media platforms to
detect hate speech) stronger than the hate speech
generation capabilities (the potential of the model
being abused with malicious intent). So, the plat-
forms using our better-informed model will easily
detect any hate speech generated by the model.
Moreover, large-scale technologies for bet-
ter generation of hate speech using GPT-2 and
other generative models (Wullach et al., 2021;
Hartvigsen et al., 2022) (with the intention to train
hate speech classifiers better) already exist and as a
model trained only for classification, our model is
likely to be far weaker than these models for gen-eration of harmful content. While these language
models can be used both to benefit or disrupt our
lifestyle just like any other technology (Douglas,
2014), we urge the researchers to exercise ultimate
caution while using them, including ours. For the
same reason, we do not make the trained model
parameters publicly available (except for the code
to promote reproducibility).
Also, one of the factors increasing bias in the
classification model is the data it is trained upon.
Hence, any potential bias in the datasets that are
manually annotated by human beings (who are not
free from bias) can result the model being biased
for/against some specific target groups.
To incorporate better inductive bias into the
model, we have trained the model to initially map
text sequences posted by the same user to vectors
that are close in the embedding space. This helps
the model to identify the dialects of the various
users and help the model perform better. How-
ever, this inductive bias may cluster the linguistic
characteristics of some groups which in turn might
potentially increase the chances of that particular
language style being classified as hate speech if
the annotated data contains only hateful instances
from that particular dialect. This may make the
biased if those particular dialects are used
predominantly by some protected category (Blacks,
Jews, Women, Mexican etc.).
The most simple and effective solution
here would be to annotate data for each di-
alect/vernacular group, potentially stratifying the
dataset into clusters using the pre-trained language
model. If there are balanced examples from each of
these dialect/vernacular or at least each protected
category (Blacks, Jews, Women, Mexican etc.) for
both the hate and non-hate categories, such biases
can be well mitigated.
Further, our model like any other hate speech
model is not suitable for in-the-wild deployment
without explicit human scrutiny. Language use is
often very specific to each platform. So, the dis-
tribution of words and data-points may not match
the training data distribution of the model. Thus,
it is extremely important to first test the model
on the platform, check for potential biases against
the protected categories and individual linguistic
groups/dialects and deploy it as a preliminary filter
assisting the human experts detecting hate speech.","bias, misuse, offensiveness","hateful language, bias, misuse","bias, misuse, offensiveness",2022,"statement, concerns, suggestions"
235,"This work proposes knowledgeable prompt tuning
which uses external knowledge bases to construct
the verbalizer. Users should be aware of the poten-
tial error in the external KBs, or even the injection
of malicious words.
","erros, malicious content",malicious language,no ethical concerns,2022,"statement, concerns"
236,"We discuss three ethical considerations relevant to
our work. First, the MFTC is composed of mono-lingual tweets about US-centric topics. Whether or
not our conclusions hold for results across different
languages and cultures is yet to be evaluated. This
limitation may cause the perpetuation of Western
biases and values (Mehrabi et al., 2021). How-
ever, we believe that our experimental setup offers
a systematic approach to studying such cultural
influences when pertinent data is available.
Second, the MFTC has low annotator agreement
(Hoover et al., 2020, Table 6), potentially caused
by the subjectivity and complexity of annotating
values. Selecting the majority label as golden label
may perpetuate the ‘tyranny’ of the majority, which
is especially dangerous when dealing with values.
We expose the impact of the annotator agreement in
Section 4.6 and identify an avenue for addressing
it as a future direction in Section 6.
Finally, the importance of understanding moral
values has been recognized by computer scientists
(Russell et al., 2015) and designers (Friedman et al.,
2008). However, we recognize that value classifi-
cation can be misused, especially, when sensitive
attributes such as gender and race are attached to
the data. For instance, authorities could use it to
automatically identify and suppress liberal minori-
ties in non-liberal countries. Additional research
is necessary for addressing such problems, e.g., by
devising techniques that mitigate bias and unfair-
ness by design (Kleinberg et al., 2018; Dinan et al.,
2020; Vargas and Cotterell, 2020).
","bias, misuse","English-centric, bias, misuse","bias, misuse",2022,"statement, concerns, actions"
237,"With our controlling methods, it is not one hundred
percent guaranteed that the generations will have
the desired attributes, but the probability for the
generations to exhibit the desired attributes will
increase. When applied to detoxiﬁcation, although
the probability of toxicity degeneration will de-
crease, the controlled language model may still
produce unsafe text. We would like to clarify that
the offensive language generated by the language
model controlled with our methods does not repre-
sent any opinion of the authors.
Besides, our proposed methods control the high-
level attributes of the generation, such as toxicity,
topic, or sentiment, but there is no guarantee of
factual accuracy for the generation, which is a well-
known problem in NLG models. Our controlling
methods may not be used for factual accuracy con-
trolling. While reducing hallucination is not the
focus of this work, knowledge-grounded generation
techniques can be used to alleviate this problem.
","accuracy, halluconations","offensive language, hallucinations",no ethical concerns,2022,"statement, concerns, actions"
238,"Dataset and Model Release The Copy Right
Act, 20008of Bangladesh allows reproduction and
public release of copy-right materials for non-
commercial research purposes. As a transformative
research work, we will release BanglaBERT un-
der a non-commercial license. Furthermore, we
will release only the pretraining data for which we
know the distribution will not cause any copyright
infringement. The downstream task datasets can
all be made publicly available under a similar non-
commercial license.
Quality Control in Human Translation Trans-
lations were done by expert translators who provide
translation services for renowned Bangla newspa-
pers. Each translated sentence was further assessed
for quality by another expert. If found to be of low
quality, it was again translated by the original trans-
lator. The sample was then discarded altogether if
found to be of low quality again. Fewer than 100
samples were discarded in this process. Translators
were paid as per standard rates in local currencies.
Text Content We tried to minimize offensive
texts in the pretraining data by explicitly crawl-
ing the sites where such contents would be nom-
inal. However, we cannot guarantee that there is
absolutely no objectionable content present and
therefore recommend using the model carefully,
especially for text generation purposes.
8http://bdlaws.minlaw.gov.bd/
act-details-846.html
",offensive content,offensive content,no ethical concerns,2022,"statement, concerns"
239,"This section sums up the procedure we adopted to
ensure the ethical compliance of our experiment.
7.1 Preliminary review
Before starting the experiment, procedure and ma-
terials were carefully reviewed by the University of
Aberdeen Ethics Board. Our experiment proposal
was accepted without major revisions.
7.2 Recruitment
Participants were recruited through physical inter-
action on campus (by flyer distribution), depart-
ment mailing list or social media public posts. No
recruitment qualification was specified, beside the
lack of health conditions that are known to affect
individuals diet. This includes pregnancy, suffering
from eating disorders or psychological treatments.
This was done since our system has been developed
to work in ""standard"" situations, while the afore-
mentioned cases would have pose high risks for
participants. Participants were showed a consent
form containing all the information regarding the
experiment procedure. All participants had to con-
firm their acceptance of these conditions (through
check-boxes and signature) in order to proceed with
the experiment. Participants were given an email
contact in case of problems during the experiment.
7.3 Pay and workload
Each participants received £20 (or 20 Cfor partici-
pants outside of UK) at the end of the experiment,as a token of gratitude for their contribution. Ac-
cess to the token of gratitude was bound to the
compliance of the following condition:
1.To complete the experiment (that is, using
MFP for two weeks; giving the final feedback)
2.To provide, to the best of their capabilities, the
most complete and accurate food diaries they
could.
Requirement 1) also included PANAS forms for
those participants who agreed to do so. For 2) par-
ticipants were supervised and given support about
meal logging and eventual missing entries. Par-
ticipants were also informed of the possibility of
abandoning the experiment (up to the point of data
analysis), which would result in exclusion from
receiving the token of gratitude.
7.4 Data protection and storage
A MFP account for each participant was generated
through temporary email that was in no way linked
to their identity. Following the experiment conclu-
sion, all accounts have been blocked. Data have
been safely stored and anonymised.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
240,"Event extraction is a standard task in NLP. We do
not see any signiﬁcant ethical concerns. Our work
is easy to adapt to new event types by offering some
examples and pre-deﬁned templates. Therefore, the
expected usages of our work is to identify interest-
ing event records from user textual input such as a
piece of sentence or document.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
241,"We collect our data from public datasets that permit
academic use. The open-source tools we use for
training and evaluation are freely accessible online
without copyright conflicts.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
242,"We collect our data from public datasets that permit
academic use and buy the license for the datasets
that are not free. The open-source tools we use for
training and evaluation are freely accessible online
without copyright conflicts.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
243,"We collected all the data with authorization from
the organization that owned the data and signed the
agreement. We release the benchmark followingthe CC BY-NC 4.0 license. All collected datasets
are anonymized and reviewed by the IRB commit-
tee of each data provider to preserve privacy. Since
we collect data following real-world distribution,
there may exist popularity bias that cannot be ig-
nored.
",bias,bias,bias,2022,"statement, concerns"
244,"Secure access to the shared task dataset was pro-
vided with IRB approval under the University of
Maryland, College Park protocol 1642625 and ap-
proval by the Biomedical and Scientific Research
Ethics Committee (BSREC) at the University of
Warwick (ethical application reference BSREC
40/19-20).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
245,"Automated scientiﬁc fact checking has great po-
tential value to the scientiﬁc community, as well
as for addressing phenomenon such as the prop-
agation of scientiﬁc misinformation. Our aim in
releasing models for scientiﬁc claim generation
is to improve the generalizability of science fact
checking systems in domains with less training re-
sources. When training our fact checking models
with generated or synthetic data, there are questions
regarding the veracity of the generated data and
whether a model trained on inferred labels could
produce trustworthy judgments. We hope that by
introducing this task and models, we will enable
the community to study such questions, while con-
tributing to data curation in a domain in which such
curation would normally require signiﬁcant manual
efforts and cost.
","unreliable_generations, veracity",trustworthiness,unreliable_generations,2022,"statement, concerns"
246,"Dual use DALC v2.0 and all the accompanying
models are exposed to risks of dual use from malev-
olent agents. However, by making publicly avail-
able the resource and documenting the process be-
hind its creation and the training of the models
(including their limitations and errors), we may
mitigate such risks.
Misrepresentation As the error analysis has
shown (§ 5), even the best system is far from be-
ing perfect, with a relatively high number of False
Positive for the OFF subclass. We thus recommend
caution before deploying such a model without any
additional human supervision.
Privacy Collection of data from Twitter’s users
has been conducted in compliance with Twitter’s
Terms of Service. Given the large amount of users
that may be involved, we could not collect informed
consent from each of them. To comply with this
limitations, we have made publicly available only
the tweet IDs. This will protect the users’ rights
to delete their messages or accounts. However, re-
leasing only IDs exposes DALC to fluctuations in
terms of potentially available messages, thus mak-
ing replicability of experiments and comparison
with future work impossible. To obviate to this
limitation, we make available another version of
the corpus, Full Text. This version of the corpus
allows users to access to the full text message of
all 11,292 tweets. The Full Text dataset is released
with a dedicated licence. In this case, we make
available only the text, removing any information
related to the time periods or seed users. We have
also anonymised all users’ mentions and external
URLs. The licence explicitly prevents users to ac-
tively search for the text of the messages in any
form. We deem these sufficient steps to protect
users’ privacy and rights to do research using inter-
net material.","dual_use, misrepresentation",misrepresentation,"misrepresentation, misuse",2022,"statement, concerns, actions"
247,"CHAPTER BREAK is constructed from two sources:
public domain books published prior to 1919 (from
the held-out set of PG-19) and works of fanfiction
extracted from an online dump of stories posted
on Archive of Our Own (AO3). We refer readers
to Rae et al. (2020) for more details about PG-19.
For AO3, we apply multiple filters to obtain long
fanfiction stories rated as suitable for “General Au-
diences”. We refer readers to Appendix A for more
preprocessing details. More generally, this work fo-
cuses on long-range language models, which could
potentially be misused to generate offensive out-
put. However, the main purpose of this paper is to
present a dataset which provides a better evaluation
of the discourse-level capabilities of such models.",misuse,misuse,misuse,2022,"statement, concerns"
248,"We acknowledge that the group of authors from
whom we selected primary sources lacks diversity
because we selected from among digitized, pub-
lic domain sources in the Western literary canon,
which is heavily biased towards white, male writers.
We made this choice because there are relatively
few primary sources in the public domain that are
written by minority authors and also have substan-
tial amounts of literary analysis written about them.
We hope that our data collection approach will be
followed by those with access to copyrighted texts
in an effort to collect a more diverse dataset. The
experiments involving humans were reviewed by
the UMass Amherst IRB with a status of Exempt.
",bias,bias,bias,2022,"statement, concerns"
249,"We present a new dataset that is derived from the
S2ORC dataset (Lo et al., 2020), which is released
under CC BY-NC 2.0 license. The Huggingface
models (Wolf et al., 2020) we develop upon are
released under Apache License 2.0.
Our annotators were compensated for their work
at a rate of double the minimum wage in our local
area",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
250,"DEGREE fine-tunes the pre-trained generative lan-
guage model (Lewis et al., 2020). Therefore, the
generated output is potentially affected by the cor-
pus for pre-training. Although with a low possi-
bility, it is possible for our model to accidentally
generate some malicious, counterfactual, and bi-
ased sentences, which may cause ethics concerns.
We suggest carefully examining those potential is-
sues before deploying the model in any real-world
applications.
",no ethical concerns,malicious content,no ethical concerns,2022,"statement, concerns"
251,"Our proposed models are based on the multilin-
gual pre-trained language model that is trained on
a large text corpus. It is known that the pre-trained
language model could capture the bias reﬂecting
the training data. Therefore, our models can poten-
tially generate offensive or biased content learned
by the pre-trained language model. We suggest
carefully examining the potential bias before de-
ploying our model in any real-world applications.
however, since the original performance for cross-lingual trans-
fer is not high enough, the beneﬁts of correcting tokens are
more signiﬁcant than this drawback.
",bias,bias,bias,2022,"statement, concerns, suggestions"
252,"To the best of our knowledge no code of ethics
was violated throughout the experiments done inthis paper. We reported all hyper-parameters and
other technical details necessary to reproduce our
results, and release the code and dataset we col-
lected. We evaluated our model on two different
datasets and tasks (source and article fake news
classification), but it is possible that results may dif-
fer on other datasets. However, we feel our method-
ology is solid and applies to any social media fake
news setting. For space constraint we moved some
of the technical details and discussion to the Ap-
pendix section. The results we reported supports
our claims in this paper and we believe it is re-
producible. Any qualitative result we report is an
outcome from a machine learning model that does
not represent the authors’ personal views. For any
results that we discuss on the data we use, we will
not include account information and all results will
be anonymous.
In our dataset release for (Baly et al., 2020b), we
include sources, users, and articles. Sources are
public information provided in (Baly et al., 2020b),
and we map each to an ID. We scraped up to 300 ar-
ticles for each source (as many as we could), which
we map to an ID. We also scraped users that inter-
act with articles, which we also release. Each user
is given by their Twitter ID (which may be invalid
or not provided if the user deleted their profile),
and their graph ID. The Twitter ID of the Tweet
the user propagates about the article is also given.
We also scraped users that follow sources, and this
information is released by providing the user ID’s
that interact with each source ID’s. Finally, we
provide the representations for each user, article,
and source we used as our initial embedding in the
graph. Our data is meant for academic research pur-
poses and should not be used outside of academic
research contexts. All our data is in English.
Our framework in general does not create direct
societal consequence and is intended to be used
to defend against fake news. While our model
could be used to build better fake news spreaders,
our approach of identifying information communi-
ties through inference operators, could also prevent
against that.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
253,"The ICD coding task is crucial for making clini-
cal, operational and financial decision in health-
care. Traditionally, medical coders review clinical
documents and manually assign the appropriate
ICD codes by following specific coding guidelines.
Models such as our ICDBigBird could help to re-
duce time and cost in data extraction and reporting
significantly.
However, we need to be aware of the risks of
over-relying on any automatic encoding model. No
matter how efficient an automatic encoding model
is, it is still possible to misclassify patients’ condi-
tion with erroneous ICD codes which may affect
their treatment. Thus we believe that any automatic
encoding model should only be used to assist, not
replace the judgement of trained clinical profes-
sionals.
",no ethical concerns,misdiagnosis,no ethical concerns,2022,"statement, concerns"
254,"Dataset Biases QMSum and AQuaMuSe contain
meeting transcripts and documents in English and
thus mainly represent the culture of the English-
speaking populace. Political or gender biases may
also exist in the dataset, and models trained on
these datasets may propagate these biases Addi-
tionally, the pretrained BART model carries biases
from the data it was pretrained on. We did not
stress test these models for biases and request that
the users be aware of these potential issues in ap-
plying the models presented.
Crowdsourcing Protocols Workers were com-
pensated $1 per example, calibrated to equal a
$12/hour payrate. We use the following qualifi-
cations to recruit MTurk workers with good track
records: HIT approval rate greater than or equal
to 97%, number of HITs approved greater than or
equal to 10000, and located in one of the follow-
ing English native-speaking countries: Australia,
Canada, New Zealand, United Kingdom, United
States.
Misuse Potential and Failure Mode When prop-
erly used, the summarization models described in
this paper can be time-saving. However, the cur-
rent model outputs may be factually inconsistent
with the input documents, and in such a case could
contribute to misinformation on the internet. This
issue is present among all current abstractive sum-
marization models and is an area of active research.
Environmental Cost The experiments described
in the paper primarily make use of A100 GPUs. We
typically used a single GPU per experiment, and the
experiments may take up to a day when repeating
across random seeds. The largest backbone model
used, BART-Large , has 400 million parameters.
While our work required extensive experiments,future work and applications can draw upon our
insights and need not repeat these comparisons.
8
","bias, misinformation, misuse","English-centric, bias, misuse","bias, misinformation, misuse",2022,"statement, concerns"
255,"We built the dataset by collecting texts from Twitter
and annotating them by crowdsourcing. For crowd-
sourcing, we employed 3,847 workers. It took ap-
proximately five minutes for a task of annotating
10 utterances. Every worker was paid 4 JPY per
10 utterances, and in total, the built dataset costs
195,700 JPY . Since the dataset was collected from
Twitter, it may include contents that are harmful
for some of the dataset or its application users. For
building the dataset through the Twitter API and
crowdsourcing, we did not include any sensitive
information that allows personal identification.
The dataset or models trained on it enable down-
stream applications to infer the emotions of their
users, resulting in facilitating communication be-
tween the users and the applications. In terms of
dialogue systems, this ability is considered valu-
able for both task-oriented and non-task-oriented
dialogue systems. For example, it assists the user
in decision-making and solves the user’s worry and
trouble. In contrast to such benefit, it is difficult
for the model to infer the emotion accurately, with
the relatively small dataset. Therefore, prediction
errors by the model, especially for sensitive utter-
ances or negative emotions, may bring harmful
experiences on the users.
",misuse,harmful content,misuse,2022,
256,"Our interview and survey studies—which are in-
tended to uncover community assumptions, con-
straints, and practices—necessarily surface those
of some but not all practitioners. Although we
aimed to recruit as widely as possible, our partici-
pants were recruited via snowball sampling, seeded
with researchers and industry groups that were well-
known to us, as well as on social media (and there-
fore biased by our followers). Additionally, our in-
terviews and surveys were only in English. Those
who participated largely focus on research, work
on English language NLG systems, and live and
work in the Global North. While this may reﬂect
the NLG research community and who has been
able to participate in it, our ﬁndings may not re-
ﬂect the assumptions or practices of practitioners
working in other settings.
The results of our studies are also rather descrip-
tive, and (though we think they are comprehensive)
they cannot provide an exhaustive set of assump-
tions, practices, or F&I concerns, and should be
interpreted accordingly. While our goal is to en-
courage further reﬂections about practices, the fac-
tors that shape them, and possible implications, we
may risk instead inadvertently discouraging certain
evaluation practices.
",bias,English-centric,bias,2022,"statement, concerns"
257,"Although our method requires ﬁne-tuning of the
pre-trained multilingual language model, the com-
putational cost of our experiments is not high. We
utilize two pieces of NVIDIA V100 and it takes
around 1 hour for the ﬁne-tuning process. This
is partly due to the relatively small QA training
dataset used for ﬁne-tuning. It is possible that if our
method is applied to either a much larger training
dataset for ﬁne-tuning or a much larger pre-trained
language model, the computational cost and power
consumption will go up. To reduce such costs, one
way is to ﬁne-tune only part of the pre-trained lan-
guage model. Another way is to apply the recently
proposed Adapter method (Houlsby et al., 2019) toﬁne-tune the language model.
Our method relies on machine translation sys-
tems. It has been found in a previous study that in-
dustrial MT systems as well as SOTA academic MT
systems may suffer from gender bias (Stanovsky
et al., 2019), and it would not be surprising if other
types of societal biases and stereotypes are also
found in machine translated texts. If our method
uses translationese containing societal biases, our
learned original-to-translationese projection func-
tion will likely also contain such biases, which may
affect the fairness of the ﬁnal trained system. How-
ever, this is not due to our method but rather the
translated text we use. Nevertheless, this is some-
thing we need to keep in mind if our method is
adopted for real applications.
","bias, fairness, computational_cost",bias,bias,2022,"statement, concerns"
258,"Our dataset is composed solely of English text.
This means that our analysis applies uniquely to
the English language, and results may differ in
other languages. Moreover, for the purpose of this
study, we curated a dataset of 170,135tweets. We
emphasize that in order to protect the anonymity
of users, we remove all author IDs from the data,
and replace all usernames with the general token
“user.” In the Urban Dictionary dataset we received
from Wilson et al. (2020), we similarly remove the
author IDs and only consider the entry text.
",english-centric,English-centric,no ethical concerns,2022,statement
259,"This work advocates for improved inclusion of
DHH individuals. A risk of the study is that results
may not generalize across conversational corpora.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
260,"The main goal of DISCO is to generate
functionality-aware code embeddings, producing
similar representations for code clones and differ-
entiating security bugs from the benign programs.
Our data is collected from either the open-source
projects, respecting corresponding licences’ restric-
tions, or publicly available benchmarks. Mean-
while, throughout the paper we make sure to sum-
marize the paper’s main claims. We also discussed
DISCO ’s limitation and potential future work for
clone detection in Section 5.3. We report our
model configurations and experiment details in Ap-
pendix A.4.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
261,"The data provided here contains extreme speech
that can be shocking and harmful. We present this
dataset as a way to peel back the veil of extreme
speech against the selected under-represented com-
munities around the world. We want to motivate
the analysis of this overlooked area as a whole and
the investigation of the various levels of extreme
speech (derogatory, exclusionary and dangerous)
as found in online social media. This data is not in-
tended and should not be used for pretraining mod-
els applied to real-world tasks, since a model pre-
trained on this data could potentially exhibit and
propagate the extreme speech found in the passages
we collected.
Further, while we endeavored to include as many
communities around the world as possible, the data
we collected and the list of communities we in-
cluded are of course non-exhaustive. For each
country, we had a close circle of annotators, there-
fore it is possible other marginalized groups in
these countries were not covered (although we
made efforts to keep this to a plausible minimum).
7.2
",misuse,misuse,misuse,2022,statement
262,"In our work we deal with hate speech, which could
potentially cause harm (directly or indirectly) to
vulnerable social groups. We do not support the
views expressed in these hateful posts, we merely
venture to analyze and provide solutions to miti-
gate this online phenomenon.
Further, we could only examine a specific prob-
lem (neutral vs. offensive vs. hateful language) in
specific languages. This is a non-exhaustive list
and there is a lot we did not cover. Care should be
taken to use these methods only in the examined
languages since generalization may not be feasible
(in fact, we show there are issues with our method
in Arabic).",misuse,misuse,misuse,2022,statement
263,"In our work we are dealing with data that can cat-
alyze harm against marginalized groups. We do
not advocate for the propagation or adoption of this
hateful rhetoric. With our work we wish to moti-
vate further analysis and documentation of sensitive
data that is to be used for the training of models
(for example, using templates from Mitchell et al.
(2019); Bender and Friedman (2018)).
Further, while classifying protected attributes
such as race or gender is important in analyzing
and identifying bias, care should be taken for the
race and gender classifiers to not be misused or
abused, in order to protect the identity of users,
especially those from marginalized demographics
who are more vulnerable to hateful attacks and fur-
ther marginalization. In our work we only predict
these protected attributes for investigative purposes
and do not motivate the direct application of such
classifiers. Further, in our work we are using di-
alect (AAE) associated with African Americans as
a proxy to race due to a lack of available annotated
data. It should be noted that not all African Ameri-
cans make use of AAE and not all AAE users are
African Americans.
Finally, in our work we only focused on English
and a specific set of attributes. Namely, we consid-
ered race (African American) and gender. This is a
non-exhaustive list of biases and more work needs
to be done for greater coverage of languages and
attributes.","bias, misuse, security",bias,"bias, misuse, security",2022,statement
264,"The dataset presented in this paper aims to make
progress in the evaluation of multilingual gender
bias in MLMs, however we argue that it should
not be used to train such models. As the presented
dataset is intended as a test set, training on it would
defeat its purpose as a test of gender bias in MLMs.
The presented dataset is based on the CPS dataset,
an English crowdsourced dataset aimed at evaluat-
ing social biases in the United States (Nangia et al.,
2020). For the purpose of this study we made the as-
sumption that the biases in the CPS dataset relating
to gender can be extended to the other languages
studied and are relevant in cultures where the lan-
guages are spoken, however we caution against the
blind implementation of such systems without an
understanding of the target culture.
This work also focused exclusively on binary
gender. The non-trivial nature of representing
non-binary people in languages with strong gen-
der agreement rules, such as German, substantially
complicates the process of creating natural sen-
tences that could be used for evaluation. For this
reason, and because it is an important area with
its own challenges, we leave the representation of
non-binary people in multilingual settings to future
work, where it can be studied with care as a topic
in its own right.
Additionally, we caution against concluding that
models are completely bias free when they gener-
ate scores that theoretically unbiased models are
expected to generate. It may be that these models
still encode biases that cannot be captured using
the proposed measure or dataset, which may later
manifest once a model is implemented.",bias,"bias, binary gender",bias,2022,"statement, concerns"
265,"In this section, we discuss the main ethical con-
siderations of MSCTD: (1) Intellectual property
protection. The English utterance and image of
MSCTD is from OpenViDial dataset (Meng et al.,
2021). For our translation and sentiments, its per-
missions are granted to copy, distribute and modify
the contents under the terms of the Creative Com-
mons AttributionShareAlike 3.0 Unported License
and Creative Commons CC0 License, respectively.
(2) Privacy. The data source are publicly avail-
able movies. Its collection and Chinese/German
annotation procedure is designed for chat transla-
tion purpose, and does not involve privacy issues.
(3) Compensation. During the sentiment annota-
tion, Chinese and German translation, the salary
for annotating each utterance is determined by the
average time of annotation and local labor compen-
sation standard. (4) Data characteristics. We refer
readers to the content and Meng et al. (2021) for
more detailed characteristics. (5) Potential prob-
lems. While principled measures are taken to en-
sure the quality of the dataset, there might still be
potential problems with the dataset quality, which
may lead to incorrect translations in applications.
However, moderate noise is common in large-scale
modern translators, even for human translated sen-
tences, which should not cause serious issues.
",intellectual property,intellectual property,intellectual property,2022,"statement, concerns"
266,"Automatic legal judgment prediction is a sensi-
tive research area. The proposed EPM model is
a preliminary work and is not ready to be produc-
tized. The goal of designing the EPM model is
to surpass the performances of existing SOAT ap-
proaches which are not ready to be productized
either.
Legal cases contain personal privacy informa-
tion. Therefore, we use a public dataset that has
been anonymized, i.e., CAIL, which is collected
from the official website for disclosing legal case
information. The private information of the cases
has been anonymized when the cases are published
on the official website.
",no ethical concerns,,no ethical concerns,2022,
267,"Like any conversation or generation model, we note
that the quality of the models’ responses depends
on the quality of its training data. Our Base-LM
model was trained on The Pile dataset (Gao et al.,
2020) and Pushshift Reddit dataset (Baumgartner
et al., 2020). Since the contents in these datasets
were collected online, they may include underlying
biases or potentially offensive words. These biases
and toxicities can be projected into our models.
Therefore, we highly recommend that additional
steps are taken to filter out profanity and inappro-
priate responses when the model is deployed to the
real world.
Furthermore, while we intend our method to be
used to mimic fictional characters from movies,
shows and stories to build more engaging conversa-
tion models, we also recognize that it is possible to
use our method to mimic real-life individuals based
on their utterances. Some potential risks include
impersonating individuals, which can be harmful
to the targeted individuals, and mimicking figures
to generate content that can be harmful to groups of
individuals. We hope that our method is deployed
in a safe manner to avoid such malicious usage.
","bias, toxicity, impersonation, harmfulness",no_ethical_concerns,"bias, toxicity, misuse, impersonation, harmfulness",2022,statement
268,"Our dataset consists of raw text data from two pub-
licly available corpora: Prachatai-67k and Wong-
nai review. These corpora use public copyright
licenses (LGPL and Creative Commons) that en-
able free distribution. The data has a minimal risk
for privacy violation since all the data were pub-
lished in a public space, such as a news site and
a restaurant review site. All the news articles and
restaurant reviews are meant to be shared publicly,
not privately. Hence, the dataset does not contain
any confidential information. Our preprocessing
step, which includes cleaning data and tokeniza-
tion, does not alter the original contents of the texts.
On average, the annotators were compensated at
least twice the local minimum wage. The annota-
tors were paid by the number of entity-mentions
annotated and the number of documents that they
have read. We distributed the same amount of doc-
uments for each annotator for fair consideration.
This dataset addresses the data scarcity issue for
Thai, which can be considered as a lower-resource
language. However, this dataset only includes the
central Thai dialect, which most Thai understand.
It is also the dialect for official usage and is often
used as a written language by Thai internet users. It
reduces the language technology disparity gap be-
tween Thai and high-resource languages. In addi-
tion, it can facilitate researchers and the NLP com-
munity to investigate the N-NER task in a multi-
lingual setting. We will open-source the dataset
and distribute it publicly under the CC by SA 3.0
license. We will also publish the source code and
the models’ weights from our experiments to as-
sist the NLP community in N-NER research and
reduce unnecessary energy usage from training the
models.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
269,"In this paper, we take the position of an attacker,
and propose to conduct a backdoor attack and ad-
versarial attack against PFTs. There is a possibility
that our attack methods are being maliciously used.
However, research on attacks against PFTs is still
necessary and very important for two reasons: (1)
we can gain insights from the experimental results,
that can help us defend against the proposed attacks,
and design better prompt-based models; (2) we re-
veal the universal vulnerability of the prompt-based
learning paradigm, so that practitioners understand
the potential risk when deploying these models.
",misuse,misuse,misuse,2022,statement
270,"Our work explores how to stimulate large PLMs
with few samples. We conduct experiments under
the few-shot setting, where requires less training
time and fewer resources than normal full-data set-
ting. Also, we open up our codes and hyperpa-
rameters to facilitate future reproduction without
repeated energy cost.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
271,"This paper does not contain direct social influences.
However, we believe the model analysis and in-
terpretation techniques discussed in this paper are
critical for deploying deep learning based mod-
els to real-world applications. Following previous
work in this direction such as Jacovi and Goldberg
(2020), we advocate to carefully consider the ex-
planations obtained from interpretation methods
as they may not always reflect the true reasoning
process behind model predictions.
Besides the three notions of faithfulness dis-
cussed in this paper, there are other important as-
pects for measuring interpretations that could be
applied to evaluate interpretations. Also, We are
not claiming that the proposed paradigm are per-
fect as faithfulness measurements. For example,
we recognize that it requires further and detailed
analysis on either the model itself or the interpre-
tation methods lead to a low performance on the
stability metric, although we do try to make sure
models behaviors do not change substantially be-
tween an input pair.
Moreover, experiments in this paper are all based
on mainstream English corpora. Although our tech-
niques are not language specific, there could be
different conclusions given the varying properties
of languages. For example, the discussion for de-
pendency parsing could be easily affected by the
language one considers.",no ethical concerns,English-centric,no ethical concerns,2022,"statement, concerns"
272,"We construct an evaluation set for evaluating con-
trolled text generation. The data samples in this set
are all from the existing works with open-source
codes, model checkpoints, and generated results.
We directly use the generated results if the authors
have released them. Otherwise, we adopt the samesetting as the original papers to make these models
generate texts. We do not apply extra selection
strategies to the generated results.
We resort to Amazon Mechanical Turk (AMT)
for the annotation of this evaluation set. We do not
invade the privacy or collect personal information
of annotators. We pay each annotator $0.06 for
each survey including four generated texts and one
negative sample. The payment is determined based
on the length of data samples. We additionally ask
annotators to check whether there is a potential
ethical problem in the data, and remove these prob-
lematic data in the evaluation set. After annotation
on AMT, we manually review all the annotated
samples from an ethical perspective. However, we
admit that there may still exist unpredictable bias
in this evaluation set.
",bias,bias,"privacy, bias",2022,"statement, concerns"
273,"Since we propose a new multilingual text gener-
ation benchmark MTG, we solve some possible
ethic considerations in this section.
English dataset We choose ROCStories,
SQUAD 1.0, ByteCup and CNN/DailyMail as the
English datasets for story, question, title generation
and text summarization tasks. All of them are
available for research use under their licenses.
They can be downloaded free from their websites12.
We ensure that these datasets are only used for
academic research and the dataset construction
process complies with the intellectual property and
privacy rights of the original authors. Also, our
proposed benchmark suite MTG should only be
used for academic research purposes.
Annotation process As described in Sec. 3.3,
we hire some full-time and part-time language ex-
perts to do the annotation. Full-time experts are
paid $40per day and part-time annotators are paid
$0.2per example13. Their salary is higher than
the local average hourly minimum wage. All an-
notators are aware of any risk of harm associated
with their participation. The annotation process is
in compliance with the intellectual property and
privacy rights of the recruited annotators. The an-
notation protocol is proved by the legal department
inside the company.
Risk Concern In this paper we propose a new
ensemble metric measuring to what degree is the
generated text close to human-level. The further
pursue for more human-like multilingual genera-
tion will possibly raise safety concerns.
12ROCStories requires for some necessary contact informa-
tion
13Full-time employees work at most 8hours per day, and the
local minimum hourly wage is $3.7. The part-time annotators
can produce at least 20examples per hour.
",no_ethical_concerns,no_ethical_concerns,"misuse, intellectual property",2022,statement
274,"This paper proposes a new kind of analogical
benchmark with explanations to rationalize models’
predictions. The dataset is collected from Civil Ser-
vice Exams of China, which is publicly available
and has been used in other public datasets before,
such as LogiQA (Liu et al., 2020a). The annotated
explanations for each problem in our dataset are
crowd-sourced by working with ByteDance. The
construction team remains anonymous to the au-
thors, and the annotation quality is guaranteed by
the double-check strategy as mentioned in §4.2.
We ensure that all annotators’ privacy rights are
respected in the annotation process. All annotators
have been paid above local minimum wage and
consented to use the datasets for research purposes
covered in our paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
275,"Below we discuss the ethics considerations in our
work.
The limitation to our work is two-fold. Firstly,
our Chinese entailment graphs focus on the task
of predicate entailment detection, and does not at-
tempt to independently solve the more general prob-
lem of reasoning and inference: this more general
task would also involve other resources including
argument hypernymy detection, quantifier identifi-
cation and co-reference resolution. These are out
of the scope of this work. Secondly, while we have
shown the effect of cross-lingual complementar-
ity, adding in more languages to the ensemble is
not directly straight-forward: this would require
linguistic expertise and NLP infrastructure in the
respective languages; including more languages,
and eventually including arbitrary languages, is
one of the directions for our future work.
The risk of our work mostly stems from our use
of large-scale news corpora: if the media cover-
age itself is biased toward certain aspects of the
world or certain groups of people, then these bi-
ases would be inherited by our entailment graphs.
Our response to this is to include as many diverse
news sources as possible to reduce such biases to
the minimum: our source corpus for building Chi-
nese entailment graphs includes 133 different news
sources from a variety of countries and regions.
For the computational cost of building Chinese
entailment graphs, the algorithm for open relation
extraction takes roughly 140 CPU hours to process
the entirety of Webhose corpus; the entity typing
model takes roughly 180 GPU hours on NVidia
1080Ti GPUs to do inference on the entirety of
Webhose corpus; the local learning process takes
less than one hour, and, the global learning process,
our major computational bottleneck, takes roughly
800 CPU hours to finish.
The major datasets of use, namely, Webhose
corpus, CLUE dataset and the CFET dataset, are
open corpora with no specified licenses, thus our
academic use is allowed; no license was specified
for the Levy-Holt dataset as well; our own CFIGER
dataset as well as the constructed entailment graphs
can be distributed under the MIT license.",bias,bias,bias,2022,"statement, concerns, actions"
276,"Considering the nature of NLP-based QA systems,
our method keeps the risk to output false (e.g. incor-
rect answers to factoid questions) or biased (e.g. im-
precise count of answer numbers) answers, which
might cause issues in trustworthy or practical uses.
However, we’d like to clarify that this work is in-
tended for discovering more accurate and efficient
systems on KBQA regardless of the exact content
in a KB, the answers to specific questions given by
our method does not reflect the authors’ point of
view.
3See the guideline in supplementary materials.1798
","incorrect answers, bias","bias, inaccuracy","bias, manipulation",2022,"statement, concerns"
277,"We collected data for TARA by downloading
raw data from the ofﬁcial NYT API at https:
//developer.nytimes.com . According to
the Terms of Use at https://developer.
nytimes.com/terms and NYTimes.com
Terms of Service located at https://help.
nytimes.com/hc/en-us/articles/
115014893428-Terms-of-service , NYT
granted us a license to access the NYT APIs and
scrape their data. We ensure that our dataset has
been collected in a manner which is consistent
with the terms of use of NYTimes.
We only release our dataset TARA for academic
purpose. In order to retrieve the same raw data
we scraped from the NYT API, multiple requests
for months between January 1, 2010 and May 31,
2020 need to be made following the instructions
athttps://developer.nytimes.com/
docs/archive-product/1/overview .
As introduced in Section 3.2, we annotated the
data using crowd-workers through Amazon Me-
chanical Turk. They are voluntary participants who
were aware of any risks of harm associated with
their participation. We require the workers to be
located in either Australia, Canada, Great Britain
or the United States such that they are English
speakers. We also require the workers to have HIT
Approval Rate (%) for all Requesters’ HITs greater
than or equal to 98%. All crowd-workers were
compensated by a fair wage determined by estimat-
ing the average completing time of each annotation
task. Each worker earn $2.4 per 10 queries and
each query should take less than a minute to anno-
tate. Example screenshots of the NYT data and our
annotation interface can be found in Appendix A.
",no_ethical_concerns,no_ethical_concerns,misuse,2022,statement
278,"Our experiments are all conducted on openly avail-
able and widely used datasets. We do not augment
any information to those data in this research, hence
this research is not expected to introduce any addi-
tional biased information to existing information
in those data. However, the model may potentially
capture biases reflective of the pre-trained language
models and datasets we use for our experiments,
in such biases have pre-existed in these pre-trained
models or datasets. This is a common problem for
models trained on large-scale data, and therefore
we suggest conducting a thorough bias analysis
before deploying our model in any real-world ap-
plications.2654
",bias,bias,bias,2022,"statement, concerns"
279,"Question answering systems are useful tools in
complement to human experts, but the “word-of-
machine effect” (Longoni and Cian, 2020) demon-
strates the effects of a potentially dangerous over-
trust in the results of such systems. While the meth-
ods proposed in this paper would allow more thor-
ough usage of existing resources, they also bestow
confidence and capabilities to models which may
not have much domain expertise. T+DAPT models
aim to mimic extensively domain-trained models,
which are themselves approximations of real ex-
perts or source documents. Use of domain adapta-
tion methods for low-data settings could propagate
misinformation from a lack of source data. For ex-
ample, while making an information-retrieval sys-
tem for biomedical and COVID information could
become quicker and less resource-intensive using
our approach, people should not rely on such a sys-
tem for medical advice without extensive counsel
from a qualified medical professional.","overtrust, misinformation",misuse,"overtrust, misinformation",2022,"statement, concerns"
280,"Collection of GOVREPORT -QSandWIKIBIO-
SUM.We comply with the terms of use and copy-
right policies of all data sources during the col-
lection of GOVREPORT -QS andWIKIBIOSUM.
Personal and other sensitive information is not col-
lected to ensure the privacy of content creators. Be-
fore annotating GOVREPORT -QS, we obtain con-
sents from the annotators and inform them of their
rights to temporarily suspend or quit the annotation
process. During annotation, annotators are fairly
compensated ( ≈$15per hour).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
281,"Potential Risks Our entity linking system has
several potential malicious use cases (e.g., disinfor-
mation, generating fake news, surveillance). For
example, Fung et al. (2021) introduced a novel ap-
proach for fake news generation. The technique
works by ﬁrst taking a genuine news article, extract-
ing a multimedia knowledge graph, and replacing
or inserting salient nodes or edges in the graph. To
build such a multimedia knowledge graph, the au-
thors do use an EL system. Another example is that
our EL system may be used as part of a malicious
surveillance system (e.g., automatically tracking
the locations of celebrities based on social media
posts and online news).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
282,"As contrastive learning often involves data augmen-
tation and manipulation, our ethical consideration
mainly focuses on properly dealing with bias in the
dataset. As bias and fairness created by contrastive
learning algorithms are still under-explored, we
will also discuss such relevant topics in the section
on future directions.
","bias, fairness",bias,"bias, manipulation",2022,statement
283,"The 4,000 Chinese cases that this research is based
upon are drawn from the China Judgements On-
line ( https://wenshu.court.gov.cn/ ), which
is available for everyone to search cases’ judg-
ments once logged in. We notice that previous re-
searches ( Tsarapatsanis and Aletras ,2021 ;Leins
et al. ,2020 ) are worried about the ethical con-
cerns raised in terms of applying the NLP tech-
nique into the legal domain. Some researchers be-
lieve that processed datasets cause people to harm
the relevant parties as such datasets enable a more
straightforward retrieval process than searching
through the original datasets published by govern-
ment agencies. We would like to point out that the
judgments published on China Judgment Online
already redacted most of the important personal
information. Nevertheless, to mitigate against any
remaining concerns, in our dataset, we only pub-
lish the case reference number and URL instead of
the full text which is available. Besides, we only
allow the index dataset to be used in research, not
for any other purpose. To download or any other
way to use judgments in China Judgment Online,
please follow the website’s terms and conditions.",no_ethical_concerns,misuse,misuse,2022,statement
284,"Our work did not involve any new data or annota-
tion collection, and as such did not require crowd-
sourced or in-house workers, or introduces any
new models and related risks. Instead, we examine
existing resources and common data balancing ap-
proaches. In Section 4.4 we speciﬁcally discuss the
relation between these practices and implications
on social bias in models.
",no_ethical_concerns,no_ethical_concerns,bias,2022,statement
285,"PolyAI Limited is ISO27k-certified and fully
GDPR-compliant.
18For example, boundaries for intents like greet ,why and
change are clear, while others such as make ornot_working
are more prone to ambiguity and different interpretation.
Before data collection: all the data has been
collected by workers of PolyAI Limited and all the
annotators are also employees of PolyAI Limited.
During data collection: we did not include any
personal information (e.g. personal names or ad-
dresses) and all the examples that included any
had been fully anonymised or removed from the
dataset. All the names in the dataset are created by
randomly concatenating names and surnames from
the list of the top 10K names from the US registry.
Upon collection, the dataset has undergone an addi-
tional check by the internal Ethics committee of the
company. NLU++ is licensed under CC-BY-4.0.",no_ethical_concerns,no_ethical_concerns,privacy,2022,statement
286,"PolyAI is ISO27k-certiﬁed and fully GDPR-
compliant. Before data collection, we informed the
crowd-sourced human workers that their voluntary
participation will allow us to collect, store, publish,
and use their fully-anonymous data for research
purposes. During data collection, we did not ask
workers for their own personal information (e.g.
name, postcode); instead, we provided ﬁctional (but
realistic looking) proﬁles for them to impersonate.
We instructed workers on how to hide their caller id,
we did not store any inbound phone numbers, and
we use fully anonymised identiﬁers in our dataset.
Finally, we offered a fair compensation (around
the average hourly wage in the UK, pro-rata) to all
workers from all locales.
",no_ethical_concerns,no_ethical_concerns,privacy,2022,statement
287,"MixQG is subject to biases found in the training
data of both the underlying text-to-text models and
all QA datasets that we have used for pre-training.
We do not collect a new dataset for question gener-
ation and instead reuse data from previously pub-
lished works. As such, we rely on the published
works to follow the responsible data collection prac-
tices. The model is currently English language only
which limits its practical applications in the real
world. We hope to make MixQG multilingual as
more diverse QA datasets become available in the
future. We validate the proposed model by conduct-
ing a human evaluation. We recruited 10 teachers
for a study that lasted a maximum of two hours and
gifted each participant a $50 gift card.
",bias,"bias, English-centric",bias,2022,"statement, concerns"
288,"We collected the data for our training set and test
sets from open online sources, while making sure
their terms allow research application and privacy
is not impugned. NAKDIMON ’s architecture does
not encourage memorization of training data and
the system is not trained for generating text.
We consider a main use case for our system to be
assisting Hebrew learners in reading. We therefore
expect NAKDIMON to facilitate life in Israel for im-
migrants still struggling with Hebrew, among other
underprivileged groups. Automatic dotting can
increase inclusion in Hebrew-prominent societies
for literacy-challenged individuals, and derivative
improvements in text-to-speech applications can
assist those with impaired vision. Lastly, dotting
can help researchers with limited understanding of
Hebrew access resources in the language.
Hebrew is a gendered language. Orthographi-
cally, in many cases the lack of dots masks gen-
der ambiguity, allowing both masculine and femi-
nine readings for a given word (e.g.שׁ/ שׁ
you.fem sent’ / ‘you.masc sent’). While well-
performing automatic dotting can help alleviate
these ambiguities and reduce the amount of poten-
tially prejudiced readings, we recognize the large
body of work on gender bias in NLP (Blodgett
et al., 2020), including in Hebrew NLP (Moryossef
et al., 2019), and the findings that an imbalanced
training set may result in an even more skewed dis-
tribution of gender bias in applications (Zhao et al.,
2017). We believe our unlexicalized approach is
more robust to such bias compared with other sys-
tems, and have already started quantifying and ad-
dressing these issues as we find them in ongoing
work. In the meantime, we offer this paragraph as
a disclaimer.",bias,bias,bias,2022,"statement, concerns"
289,"Abuse of Anonymization Mechanisms Text
Anonymization is an important ﬁeld of research for
the protection of privacy of individuals as well as
for enabling freedom of speech. Still, anonymiza-
tion mechanisms may be exploited for negative
causes. Speciﬁcally, guaranteed anonymity on
the internet might lead individuals to spread hate
speech. Furthermore, mechanisms as ours can be
used to anonymously generate and spread fake re-
views or fake news. Important areas of research
ﬁghting these problems include hate speech and
toxicity detection (Djuric et al., 2015; MacAvaney
et al., 2019) as well as fake review detection
(Mukherjee et al., 2013; Barbado et al., 2019) and
fake news detection (Shu et al., 2017; Ruchansky
et al., 2017).
Bias in Large Language Models Large lan-
guage models such as GPT-2, which our proposed
approach is based on, often inherit biases towards
various demographics from the large amount of
data they are trained on (Sheng et al., 2019; Abid
et al., 2021). These biases can cause unforeseen
effects when generating language output and could
potentially alter statements of authors whose texts
are being anonymized. An increasing amount of
work is aiming to understand and tackle such biases
in language models (Vig et al., 2020; Liang et al.,
2021).
Evaluation Fairness In this paper, we evaluate
our approach experimentally and compare its per-
formance to mechanisms proposed in previous re-
search works. Since no code was publicly released
for the approaches we are comparing ours to, we
implemented the mechanisms ourselves. While we
replicated the original systems as close as possi-
ble to the description in the papers using all the
information available, we cannot guarantee that
they are exactly the same as not all the information
about preprocessing and implementation details is
publicly available.",bias,bias,bias,2022,"statement, actions, concerns"
290,"Our research considers several measures to miti-
gate concerns of bias in machine learning: (i) we
implement standard social science practices to se-
lect corpora and training data (Barberá et al., 2021);
(ii) for the pre-training stage, we gather a corpus
with unprecedented global coverage to reduce re-
gional biases; (iii) we move beyond the biases in-
troduced from dictionary-based methods by using
machine learning, as suggested by Wilkerson and
Casas (2017); (iv) finally, we use multiple coders
for the training data. However, copyright issues
prevent us from sharing the raw data and hinder
FAIR data principles (Wilkinson et al., 2016).
The broader goal of producing accurate and valid
conflict data is to prevent or mitigate harm. These
types of data provide a more objective means to
understand and study conflict and armed violence.
Our effort is an attempt to produce higher-quality
data resources to serve this purpose.
",no ethical concerns,bias,"bias, misuse",2022,"statement, actions, concerns"
291,"The nature of text generation leads to multiple eth-
ical considerations when applied to applications.
The main failure mode is that the model can learn
to mimic target properties in the training data that
are not desirable.
Faithfulness and Factuality Since models cre-
ate new text, there is the danger that they may nei-
ther be faithful to the source material nor factual.
This can be exacerbated when the data itself has
highly abstractive targets, which require the model
to generate words not seen in the source material
during training. This often leads the model to gen-
erate content inconsistent with the source mate-
rial (Maynez et al., 2020b; Kryscinski et al., 2020;
Gabriel et al., 2021).
Trustworthy Data If the data itself is not trust-
worthy (comes from suspect or malicious sources),
the model itself will naturally become untrustwor-
thy as it will ultimately learn the language and
topics of the training data. For instance, if the train-
ing data is about Obama birther conspiracies, and
the model is asked to generate information about
the early life of Obama, there is a risk that such
false claims will be predicted by the model.
Bias in Data Similarly, biases in the data around
gender, race, etc., risk being propagated in the
model predictions, which is common for most
NLP tasks. This is especially true when the models
are trained from non-contemporary data that do not
represent current norms and practices (Blodgett
et al., 2020).
The above considerations are non-malicious,
in that the model is merely learning to behave as its
underlying source material. If users of such models
are not aware of these issues and do not account
for them, e.g., with better data selection, evalu-
ation, etc., then the generated text can be damaging.
Generation models can also be misused in
malicious ways. These include generating fake
news, spam, and other text meant to mislead large
parts of the general population.
","faithfulness, trustworthiness","faithfulness, trustworthiness","bias, misuse, manipulation",2022,"statement, concerns, actions"
292,"In this paper, we presented a method to learn cross-
lingual and environment-agnostic representations
for Vision-and-Language Navigation. Vision-and-
Language Navigation task can be used in many real-
world applications, for example, a home service
robot can bring things to the owner based on natu-
ral language instructions, making people’s life eas-
ier. Our learned representations enable the agent to
understand multi-lingual instructions and improve
agents’ generalizability to unseen environments.
However, currently we learn our cross-lingual rep-
resentation from three languages (i.e., English,
Hindi, and Telugu) only due to dataset availabil-
ity, which might limit its generalization to other
languages. Besides, similar to other instruction-
following agent, our agent might fail to reach the
target given some instructions, which requires fur-
ther human assistance.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
293,"Potential benefits Our conclusions show bias
changes as a function of whether the form in which
input is presented is different from that of training.
Our results hint at how zero-shot generalization
may provide some hopeful representation toward
minimizing harm and bias in these large-scale lan-
guage models. Further, our BBNLI dataset is de-
signed to integrate detailed stereotypes and more
complex logical statements that will be crucial to
the accelerating advancements in natural language
inference problem and measuring biases in multi-
task systems, more broadly.
Anticipated risks While this study is intended to
shed a more nuanced and context-sensitive light to-
ward various social biases in T0 as measured using
two benchmarks, a potential risk lies in the models,
tasks, prompt templates, domains and subtopics we
were not able to exhaustively include. In BBNLI ,although we did our best to approach the top stereo-
types and biases that appear in real-life, we were
not able to include every ethnicity, gender, and re-
ligious point of view. Given these limitations, the
risk of using our benchmark could be that the model
will show biases in social-cultural categories we did
not account for. Additionally, with the added com-
plexity of skip-logic embedded within the premise
and hypotheses, there may be some outputs that
produce unexpected, unrelated biases that were not
explicitly determined.
Moreover, the stereotypical hypotheses we de-
vised are harmful social biases that have real-life
consequences to certain groups of people. Fur-
ther, out intent is to address these highly problem-
atic statements as clearly as possible to understand
biases. However, when these hypotheses are
taken out of context and interpreted at face-value,
they can cause serious damage to what a model
might output or create misunderstanding of our
study’s purpose.
Lastly, we acknowledge that as human re-
searchers ourselves, we are prone to exuding biases
that we have accumulated from our personal envi-
ronments. As such, this work should be expanded
upon by future works and more importantly, our
bias dataset can be strengthened through increased
collaborative efforts with scholars from the social
sciences and humanities.
",bias,bias,"bias, misuse",2022,"statement, concerns"
294,"FLOTA shortens the average length of sequences
processed by PLMs, thus reducing their energy re-
quirements, a desirable property given their other-
wise detrimental environmental footprint (Schwartz
et al., 2019; Strubell et al., 2019).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
295,"As part of our model, we use contextualized word
embeddings to model the polarized framing of con-
cepts. However, contextualized word embeddings
are known to be biased (Basta et al., 2019; Zhao
et al., 2019; Bender et al., 2021), which bears the
risk of impacting our results. We see this as an
important research question for future work.
The user base of Reddit has been shown to be
disproportionately young and male compared to
the general population of the US (Shatz, 2017). We
acknowledge that this limits the generalizability of
our results, and we try to be particularly careful
when drawing conclusions in the paper.
",bias,"bias,  generalisability",bias,2022,statement
296,"While more accurate transcription of printed and
handwritten documents in low-resource settings
can expand research access for language and his-
tory scholars, it could also potentially facilitate
government surveillance of marginalized commu-
nities. Separately, bad actors could more easily
scan and digitize document images containing sen-
sitive information and use them for nefarious pur-
poses.

",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
297,"Our experiments were all run for the English lan-
guage, and even though we expect the study design
to be adaptable to other languages, we have not
verified this assumption experimentally and limit
our claims to the English language. Expanding
the claims to other languages would require trained
question generation models in the studied language.
The teacher annotators that participated in our
study were compensated at a rate above minimum
wage, and we have insured that no personally iden-
tifiable information is available in the annotations
we’ve released.
",no ethical concerns,English-centric,no ethical concerns,2022,statement
298,"Like any system that makes predictions, those made
byGeMM are not necessarily accurate and may be
used by malicious actors to generate fake infor-
mation to mislead their audience. Additionally,
GeMM is an extension of RoBERTa and therefore
inherits the biases learned during the training of
RoBERTa. Our work focuses exclusively on En-
glish and Arabic numerals. As noted by Thawani
et al. (2021a), the units in WiCo are heavily biased
towards European and American units as they are
over-represented in English Wikipedia.
Figure 8: Left: Instructions for labeling task. Right : we show the interface used by the labelers29",bias,English-centric,bias,2022,statement
299,"Innovations in technology often face the ethical
dilemma of dual use: the same advance may of-
fer potential beneﬁts and harms. For the IE tech-
nologies introduced in this tutorial, the distinction
between beneﬁcial use and harmful use depends
mainly on the data. Proper use of the technology
requires that input text corpora, as well as other
modalities of inputs, are legally and ethically ob-
tained. Regulation and standards provide a legal
framework for ensuring that such data is properly
used and that any individual whose data is used has
the right to request its removal. In the absence of
such regulation, society relies on those who apply
technology to ensure that data is used in an ethical
way. Besides, training and assessment data may be
biased in ways that limit system accuracy on less
well represented populations and in new domains,
for example causing disparity of performance for
different sub-populations based on ethnic, racial,
gender, and other attributes. Furthermore, trained
systems degrade when used on new data that is
distant from their training data. Thus questions
concerning generalizability and fairness need to be
carefully considered when applying the IE tech-
nologies to speciﬁc datasets.
A general approach to ensure proper, rather
than malicious, application of dual-use technol-
ogy should: incorporate ethics considerations as
the ﬁrst-order principles in every step of the system
design, maintain a high degree of transparency and
interpretability of data, algorithms, models, and
functionality throughout the system, make software
available as open source for public veriﬁcation and
auditing, and explore countermeasures to protect
vulnerable groups.
","bias, fairness","bias, transparency","bias, misuse, transparency",2022,statement
300,"F.1
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,-
301,"As mentioned earlier, SETs have multiple functions
such as (1) faculty members examining their teach-
ing performance in a diagnostic way, (2) allow-
ing institution leaders to review and describe the
quality of course offerings, (3) part of an instruc-
tor’s larger portfolio to demonstrate their teaching
history during high-stakes settings, and (4) sum-
maries being released to students to guide them
with course selections. Given this wide range of
uses for the SET summaries, our work’s purpose
is to take initial steps towards developing thought-
ful, accurate, and well-designed representations
that can be provided to draw accurate inferences
about teaching quality, course design, and student
learning. However, it is also critical to examine all
aspects of SETs through an ethical lens. Errors in
NLP-based analysis could lead to misinterpretation
and inaccurate judgments in high-stakes settings.
Therefore in the following, we discuss how each
module of our SETSum website affects the inter-
pretation of SET results.
For quantitative items, we provide visualizations
of two statistics that are directly computed from
SET data. Therefore, no errors or biases should be
introduced compared to the standard PDF report.
In fact, some instructors who participated in our
evaluations say that the response rate feature for
each individual question helps them understand the
results with less bias.
For open-ended items, to obtain their sentiment
distributions, we develop sentiment analysis mod-
els to obtain sentence-level sentiments. Though
we obtain good sentiment prediction performance
(Table 1), errors are inevitable. We use these fea-
tures to demonstrate the relative number of positive
to negative comments (ratio). In general, unless
very few students evaluate a course (low response
rate for comments), the system can still convey
the information fairly well. Another important fea-
ture that we develop as part of this system is to
group comment sentences by aspects. Although we
achieve similar aspect prediction F1 scores consis-
tent with past research, we find that the results are
not precise enough yet for widespread use. Our hu-
man evaluators notice that some sentences from the
open-ended comments are inaccurately clustered.
Therefore, in future iterations of this system, we be-
lieve it is very important to develop a more accurate
aspect extraction model. The final important fea-
ture is the unsupervised extraction summarizationmodule. We choose an extraction method because
it does not suffer from faithfulness (not staying
true to the source) issues as abstractive methods
(Cao et al., 2018). Meanwhile, our algorithm ex-
tracts summaries with more balanced sentiments
(Table 2). Nonetheless, we hope to find a summa-
rization approach that aligns more closely with the
sentiments underlying the students’ comments.
Though instructors express positive attitudes to-
wards our system and 4 instructors think it help
them understand SETs with less bias, we believe
that additional thorough evaluations need to be con-
ducted in the future. Outside of SETs, our work
recognizes the different ways, reporters, and meth-
ods that could be used to assess teaching effec-
tiveness, including but not limited to peer reports,
analysis of classroom sound, student learning, and
an instructor’s own teaching portfolio.
Finally, at this time our system is designed to
be used and reviewed by instructors or other re-
viewers, and it does not directly make any broad
judgments or decisions (e.g., whether the instructor
is qualified for promotion). The primary end-users
of the system should be instructors who wish to
analyze their SET findings more thoroughly and
acquire the main takeaways more efficiently. Other
reviewers and administrators can use the system
to view the SET findings in a broader scope, such
as reading the report summary per department or
per division. Overall, the goal of SETS UMis to
help instructors and other reviewers to understand
more of students’ needs and make improvements
to future course design.
",bias,"bias, misuse",bias,2022,"statement, concerns, suggestions"
302,"The paper focuses on improving the knowledge
selection component for dialog systems.
Intended use. The intended use of this grounded
dialog system is to perform chit-chat with the user
on topics such as books and movies. We also hope
that our released system can help research in knowl-
edge selection.
Bias. Our model is developed with the use
of large pretrained language models such as
RoBERTa (Liu et al., 2019a) and GPT2 (Radford
et al., 2019), both of which are trained on large
scale web data that is known to contain biased or
discriminatory content. The datasets that we train
on also include subjective knowledge (comments
on movies) that may express the bias of the writers.
Misuse potential. Although our system is
knowledge-grounded, the output from our system
should not be treated as factual knowledge. It
should also not be considered as advice for any
critical decision-making.","bias, nonfactual outputs",bias,"bias, misuse",2022,"statement, concerns, suggestions"
303,"Both the authors of our source texts and the authors
of our questions are based primarily in the US, and
represent a relatively privileged, educated popula-
tion. A system that performs well on our dataset is,
thus, only demonstrating its effectiveness on main-
stream US English, and should not be presumed to
be effective on text in other languages or language
varieties.

",no_ethical_concerns,English-centric,no ethical concerns,2022,statement
304,"Disproportionate assistance. One of the find-
ings of our work was that the collaboration model
discussed is more effective at assisting users who
are already skilled at writing tasks. We noted in
the paper that an important direction of future work
is to develop systems that cater to the novice user
group as well. An ethical consideration is that if
such a system in its current state were deployed,
it could lead to an increase in the disparity in per-
formance between the two user groups. We be-
lieve that recording this observation is important as
human-centered machine learning systems become
more prevalent.
Appropriate remuneration for crowd workers.
To complete the HIT on AMT, workers need to in-
teract with the model a minimum of 2 times before
submitting the caption—it is explicitly mentioned
that they are free to reject the suggestions and ac-
cepting/rejecting suggestions has no bearing on the
payment. From a small internal pilot (also con-
firmed with Mechanical Turk experiments) we esti-
mate an average completion time to be 10 minutes
with an additional 2 minutes to read the instructions,
so the payment is set to $3 for the HIT (prorated to
an hourly wage of $15). The estimated completion
time for third-party evaluation was 1 minute so the
payment was set to $0.25 per annotation (prorated
to an hourly wage of $15).",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions"
305,"The main target of this paper is to propose a new
extension of the NLI task that focuses on predict-
ing the whole distribution instead of one single
label. Our new formalization can potentially make
the related application of NLI more reliable in the
practice, as the models trained on our proposed task
will focus more on minority opinions which may be
ignored in the traditional formalization. Nonethe-
less, we are strongly against the use of current NLI
models in any critical applications (e.g. admission,
jury, etc.). While NLP models can use to help hu-
man judgment (and the results should be veriﬁed
by a human), their robustness and fairness are still
an unsolved issue, and cannot replace the work of
human experts.","robustness, fairness","robustness, fairness",bias,2022,"statement, concerns"
306,"Datasets. The DUC 2006 and 2007 datasets were
obtained according to the DUC website ( duc.
nist.gov ) requirements. It was not possible for
others to reconstruct the document sets and refer-
ence summaries of the dataset from the crowdsourc-
ing tasks.
The datasets are composed of new articles
mainly from the late 1990s from large news out-
lets, compiled by NIST. All data exposed by our
systems are directly extracted from those articles.
For extraction, we do not intentionally add in any
rules for ignoring or boosting certain information
due to an opinion.
Crowdsourcing. Due to the need for English
speaking workers, a location filter was set on
the Amazon Mechanical Turk ( https://www.
mturk.com ) tasks for the US, UK and Australia.
All tasks paid according to a $10 per hour wage,
according to the estimated required time of each
task. The payment was either paid per assignment,
or as a combination with a bonus.
Compute resources. Our MSumm and
MSugg models required between 2 and 20
hours of training (usually around 4 hours),
depending on the configuration. We trained on
one NVIDIA GeForce GTX 1080 Ti GPU with
11GB memory. The pretrained base model was
trained once and reused in all subsequent training.
Outputting at inference time is computationally
cheap: MSumm runs upto about 1 second, but
mostly in a few hundred milliseconds, and
MSugg runs upto about 7 seconds, but mostly in
under 4 seconds. Training with a batch size of 8
used about 3GB GPU memory for MSumm , and
about 9GB memory for MSugg (since there are
many more input units per document set, i.e., all
noun phrases versus sentences).",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
307,"Computation. We ran on 3 GPUs for 20 min-
utes to ﬁnteune each of the salience model and the
fusion model.
The summarization model runs 10 minutes on 4
GPUs to generate a summary. Most of the time is
spent on the clustering step, in which we calculate
the SuperPAL similarity score between all salient
proposition pairs.
Dataset. The DUC 2003 and 2004 and TAC
2008-2011 datasets were acquired according to the
required NIST guidelines ( duc.nist.gov ).
Crowdsourcing. All human annotations and
evaluations conducted with crowdsourcing were
compensated as a 12$ per hour wage. We esti-
mated the task payment by completing sample as-
signments and obtaining the average assignment
time.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
308,"The model proposed in this paper is intended to
study how syntax emerge from unsupervised learn-
ing objectives. It can also help understand lan-
guages with limited annotations. However, as we
showed in this paper, the syntax predicted by cur-
rent models can contain errors and be inﬂuenced bythe choice of datasets, so the model’s output should
be used with caution and examined by experts. Our
model has been tested on 10 diverse languages. Our
ﬁndings on these languages should generalize to
languages with similar linguistic properties, but we
suggest careful empirical studies before applying
our approach to languages distant from those we
study in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
309,"Our work aims at reducing the risk of generating
hallucinations, and even possibly misinformation,
for abstractive summarization models so that such
models can be used safely for real-world applica-
tions. While we demonstrate that we can alleviate
this problem, we stress that there is still a long
way to go for improving factuality. Thus, we stress
that such models should be used with caution for
real-world applications.",misinformation,no_ethical_concerns,misinformation,2022,"statement, suggestions"
310,"From an ethics standpoint, we provide a brief
overview and show samples from the datasets that
our models are trained on throughout the paper and
also in the Appendix. Explanation graph genera-
tion improves the interpretability of neural com-
monsense reasoning systems and could prove to
be effective in understanding and debugging such
models. Hence we do not foresee any major risks
or negative societal impact of our work. However,
like any other ML model, the graphs generated
by our models may not always be completely ac-
curate and hence should be used with caution for
real-world applications.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions"
311,"The usage of pretrained generative models in any
downstream application requires careful consider-
ation of the real-world impact of generated text.
In our approach, we provide concrete inputs for
grounding the generated text to specific entities and
relations which encode real-world facts, thereby re-
ducing the possibility of propagating unintended
stereotypical and social biases embedded within the
pretrained models. However, since these entities
and relations are derived from existing knowledge
bases like ConceptNet (Speer et al., 2017), there
is potential for transfer of bias present in these re-
sources to the generated texts. Additionally, the
graph-to-text generative models in GRADApose
the same risk as other data-to-text generative mod-
els (Ribeiro et al., 2020; Hoyle et al., 2020; Mager
et al., 2020) i.e. the models can be made to gen-
erate incorrect facts by providing incorrect data as
input. Therefore, we recommend restricting the use
of G RADA to low-risk, unbiased graphs inputs.
",bias,bias,bias,2022,statement
312,"The goal of this work is to advance state-of-the-
art research in the ﬁeld of misinformation detec-
tion by analyzing multiple documents on the same
topic. We build new benchmark datasets using a
fake news generator, and propose a detector that
achieves high performance in such scenarios. We
have released the constructed datasets and detector
codes in this submission as a useful reference for
future research. We hope that our work will encour-
age more efforts in this direction and beneﬁt the
community.
However, as with any work that utilizes text gen-
eration, our work involves the risk of being applied
to produce false information to mislead or manip-
ulate readers. Therefore, we promise not to share
codes or checkpoints of our generator to avoid po-
tential negative consequences. To improve repro-
ducibility, we describe the general idea and a few
crucial details of the fake news generator.
","misinformation, manipulation",misuse,"misinformation, manipulation",2022,statement
313,"We proposed a novel method to calibrate class-
imbalanced token classifiers, and demonstrated the
method for NER models. This calibration method
is a step toward responsible use of AI by offer-
ing a measure of reliability, but also has risks that
should be considered from an ethical point of view.
Calibrated scores are a measure of transparency,
and users can interpret a well-calibrated model bet-
ter. However, all transparency methods expose AI
systems to malicious attacks by providing more
information about the internal workings of the sys-
tem. This risk should be taken into account in
sensitive tasks, e.g. when an NER model is used to
extract personally identifiable information for pri-
vacy reasons. Also, users should be warned that a
low calibration error does not guarantee robustness
in out-of-domain settings. Therefore, in the case of
safety-critical tasks such as medical applications of
NER, a low calibration error should be interpreted
with caution.
Further, low calibration errors should not be used
to justify inherently unethical tasks or those out of
the scope of the capabilities of NLP technologies.
Every task should be evaluated in terms of feasi-
bility and ethical use regardless of reliability and
transparency of trained models. It is also impor-
tant to keep in mind that a well-calibrated model
can become miscalibrated as the data changes, and
continuous calibration is needed to deal with the
ever-changing nature of language.
",malicious attacks,"vulnerability, robustness, miscalibration","transparency, security",2022,"statement, concerns, suggestions"
314,"7.1 Data Collection
We ensured that our dataset was obtained following
Twitter’s terms and conditions.
The full text corpus will not be released due to
Twitter’s Privacy Policy. Only the IDs of the tweets
and their labels are be available on the following
repository6.
7.2 Benefits and
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
315,"As noted in Antoniak and Mimno (2021), collect-
ing gendered names from population-derived data
has the limitation of centering the majority popu-
lation, in this case US-born, white children. More-
over, while filtering for names not recognized by
the GloVe or BERT vocabularies ensures our study
only includes names that have pre-trained represen-
tations, this filtering might perpetuate biases in our
tests since it disproportionately affected non-white
names and female names.
Researchers have called on the NLP community
to move beyond the gender binary (Larson, 2017;
Prabhakaran et al., 2019). While our study included
gender-neutral names and pronouns, we acknowl-
edge that this set is drastically smaller than that
of gendered names. We leave a deep study into
the impact of gender-neutral names or pronouns as
future work.
Using names as a proxy for gender is fraught
with potential limitations and biases, particularly
when an individual’s gender identity does not
match the gender historically associated with their
name. However, NLP systems might make gen-
dered associations based upon a speaker’s name
even when the speaker’s gender is not explicitly
mentioned. As discussed in footnote 1, we ac-
knowledge these issues and strive to parallel the
circumstances in which these systems may be de-
ployed in the real world.
",bias,bias,bias,2022,"statement, concerns"
316,"This work extends the bias measuring methodol-
ogy of Kurita et al. (2019) to multimodal language
models. Our case study shows that these language
models are influenced by gender information from
both language and visual contexts - often ignoring
visual evidence in favor of stereotypes.
Gender is not binary, but this work performs
bias analysis for the terms “male” and “female”
– which are traditionally proxies for cis-male and
cis-female. In particular, when images are used
of male and female presenting individuals we use
images that self-identify as male and female. We
avoid guessing at gender presentation and note
that the biases studied here in this unrealistically
simplistic treatment of gender pose even more se-
rious concerns for gender non-conforming, non-
binary, and trans-sexual individuals. A critical
next step is designing more inclusive probes, and
training (multi-modal) language models on more
inclusive data. We welcome criticism and guid-
ance on how to expand this research. Our im-
age based data suffers from a second, similar,
limitation on the dimension of race. All indi-
viduals self-identified as “white” or “black”, but
a larger scale inclusive data-collection should be
performed across cultural boundaries and skin-
tones with the self-identification and ifappropri-
ate prompts can be constructed for LLMs.",bias,"binary gender, exclusivity, binary race",bias,2022,"statement, concerns"
317,"Working with endangered/Indigenous languages
and language data, there is always substantial
risk of unwittingly perpetuation of colonial
harms (Bird, 2020). This is obviously an extremely
complex issue, but according to Bird (2020)
and other working in the space of NLP for
endangered/Indigenous languages, perhaps the
most critical aspect in working with Indigenous
language data is that researchers actively develop
meaningful relationships with members of these
respective language communities.
In our case, our work is lead by an instructor of
Mapuzugun and member of the Chilean Mapuche
community, who knows first-hand the oppression
the Mapuche people have sufferred and the harms
they have undergone by being forced to operate in
Spanish. This work is also partially funded by a
program dedicated to addressing the long-standing
colonial harms in Chile, by specifically helping
Indigenous students through their studies.
We do not anticipate any serious harms by the
development of our system, and we believe that
the positive reception by the Mapuche volunteers
who participated in our case study will be mirrored
by its reception by the wider Mapuche community.
It is also important, though, to acknowledge its
limitations and make it clear that our tool is meant
to be a companion tool for learning and can by no
means substitute instructors of the language.
No indigenous language data were collected or
are released through this project. We re-used exist-
ing, publicly available tools and corpora. The tool
is provided for free: it is currently behind a simple
username and password setting to ensure that its
traffic is not overwhelming, so that the tool remains
available to the Mapuzugun instructors and learners
that need it the most (and who already have access
to it).
",no_ethical_concerns,colonial harms,misuse,2022,"statement, suggestions"
318,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
319,"Our work aims to expedite bug resolution by mo-
bilizing developers and guiding them in absorbing
content in long discussions that is relevant towards
implementing the solution. Through this, we hope
to reduce the life span of software bugs and vulnera-
bilities that can significantly disrupt everyday oper-
ations. Our system is designed to assist developers
and should not be considered as a replacement for
the critical reasoning that is needed during bug res-
olution. Over-relying on this system to always alert
developers when a solution has been recommended
could have the opposite effect of causing delays in
bug resolution for cases that the system is unable
to handle. Additionally, if developers choose to
rely solely on the system’s generated description
and ignore the discussion context, the solutions
they implement could potentially be incomplete or
incorrect, if the system’s output misses important
details. Instead, developers should use the gener-
ated output to guide their focus and understanding
as they read through the discussion.
To build our system, we used data from GitHub,
in accordance with its acceptable use policy, and
no additional permission was required. Namely,
the policy states: “Researchers may use public,
non-personal information from the Service for re-
search purposes, only if any publications resulting
from that research are open access.”7We use only
publicly available data and use it only for research
purposes. Additionally, the data we used to train
and evaluate models (and publicly release) does
not contain personal information (e.g., usernames
of users who authored utterances and linked men-
tions). We require that any future work using our
dataset must abide by GitHub’s official policy as
well. For evaluation, we conducted human evalu-
ation, for which participants willfully volunteered
to be part of the study. They were not compensated
for their participation.",no_ethical_concerns,misuse,privacy,2022,"statement, concerns"
320,"Our dataset has been collected in a manner that
is consistent with the licenses provided from the
sources (i.e., GitHub repositories).
The evaluation methodologies described in our
study is expected to assist researchers in evaluat-
ing and reporting ML models for code summariza-
tion, and assist software developers (i.e., users of
those models) in understanding the reported met-
rics and choosing the correct model that fits their
use case. Our work can be directly deployed in
code summarization research, and can potentially
generalize to other software engineering areas us-
ing ML models (§6.1). We expect our work to help
researchers build ML models for code summariza-
tion (and other SE areas) that are more applicable
to their intended use cases.
We do not claim that the methodologies and use
cases described in our study are the most realistic
ones, nor do we try to provide an exhaustive list of
them. In particular, the continuous-mode use case
(§3.3) is inspired by our own observations during
using and developing ML models for code summa-
rization. We try our best to design this use case
to reflect the most common and realistic scenarios,
but other use cases may be more valid in certain
scenarios (§6.2).
We conducted experiments involving computa-
tion time/power, but we have carefully chosen the
number of times to repeat the experiments to both
ensure reproducibility of our research and avoid
consuming excessive energy. We provided details
of our computing platform and running time in
Appendix D.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
321,"Our work did not involve any new data collection
or annotation and did not require in-house workers
or introduce any new models and related risks. In-
stead, we examine how character-level noise can
help the transfer between closely related languages,
especially in low-resource zero-shot settings.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
322,"The OntoNotes 5.0 dataset (Weischedel et al.,
2013), licensed through LDC, annotates various
genres of texts in three languages (English, Chi-
nese, and Arabic) with structural information and
shallow semantics. OntoNotes 5.0 inevitably con-
tains personal information and offensive content.
However, we only run experiments on the dataset
and do not disseminate it or our trained models,
which are only available upon request. We also
make sure the examples shown in our paper are
anonymized. The pre-trained language model GPT-
2 can also encode certain social biases (Liang et al.,
2021). Our research in probing could help us un-
derstand and mitigate these biases.
","privacy, bias, offensiveness","privacy, offensive content","privacy, bias, offensiveness",2022,statement
323,"In any safety-critical context like aeronautics, there
is an inherent risk associated with the use of auto-
matic methods supporting human operators. This
is why our proposed techniques are limited to a
responsible use on ground, at least until the un-
derlying models can be certified for in-flight use
thanks to rigorous methods from the Trusted Artifi-
cial Intelligence research field. In any case, such
systems are only meant to support human analysis
and decision making by decreasing workload, not
to replace them.
The NOTAMs collected worldwide and used in
this study are public data (accessible via numerous
official platforms online). The datasets used for
named entity recognition and translation are pro-
prietary and built internally by expert annotators as
part of their work (with the permission to be used
in our work). The ICAO dataset used for criticality
prediction is public and enables research use.
The Huggingface Transformers framework sup-
porting model training in our study is open sourced
under the permissive Apache 2.0 license. Every
model training mentioned in this paper (RoBERTa
and DeBERTa v2 ones) took less than 12 GPUhours for pretraining and for each of the three down-
stream tasks. The hyperparameters used by these
models in our experiments are the default ones
from the Transformers library (faithful to the origi-
nal papers), except when explicitly mentioned (e.g.
varying the hidden size).
",manipulation,safety,manipulation,2022,"statement, concerns"
324,"The Georgian annotations were done by a lin-
guist with appropriate compensation after educat-
ing them about the research purpose and the annota-
tion process. We claim ownership of the Georgian
dataset for open distribution as the text is taken
from the Modern Georgian and Political texts sub-
corpora of the Georgian National Corpus, which is
an open-sourced work that allows for modiﬁcations,
derived work and redistribution. The quality of the
annotations was examined manually and empiri-
cally. The source code and the data will be released
open-source. Finally, the limitations of the work
lay within the reported performance. There should
be no potential risks given these stated limitations.
",no_ethical_concerns,no_ethical_concerns,intellectual property,2022,statement
325,"Beyond the concrete changes suggested during the
discussion, it is important to consider the broader
ethical implications of task-oriented dialogue sys-
tems in healthcare settings. Although the goal of
such systems may not be to replace human health-
care providers, it is likely that deployed systems
would support clinicians, defraying workload for
overburdened individuals. In doing so, these sys-
tems may have signiﬁcant impact on healthcare
decision-making. Machines are imperfect, and thus
a possible harm is that these systems may misin-
terpret user input or make incorrect predictions—
a mistake that in high-stakes healthcare settings
could prove detrimental or even dangerous. Re-
searchers and developers should be cognizant of
possible harms stemming from the use and misuse
of task-oriented dialogue systems for healthcare
settings, and should implement both automated
(e.g., strict thresholds for diagnostic suggestions)
and human (e.g., training to ensure staff awareness
of potential system fallibilities) safeguards.
Moreover, a potential beneﬁt of these systems
is their potential to meaningfully and beneﬁcially
extend healthcare access to underserved popula-
tions. As such, it is important to ensure that auto-
mated systems do not fall prey to the same biasesoften observed among human healthcare providers
(FitzGerald and Hurst, 2017). Systems trained to
perform healthcare tasks using datasets that are not
representative of the target population may exhibit
poorer performance with users who already experi-
ence marginalization or are otherwise vulnerable,
impeding or even reversing beneﬁts. We call upon
researchers to examine, debias, and curate their
training data such that task-oriented dialogue sys-
tems for healthcare applications elevate, rather than
diminish, outcomes for the historically underserved
users which they are best poised to beneﬁt.","bias, misuse","harm, bias","bias, misuse",2022,"statement, concerns, suggestions"
326,"In this section, we discuss several known limita-
tions and ethical considerations of our work. We
do not recommend any kind of text generation tech-
nology to be deployed on Wikipedia given this is
an active area of research.
10.1 Dependency on Evidence from the Web
reflects Bias on the Internet
Biographies, whether written as books or available
online, reflect societal bias. While many Wikipedia
editors rely on web-based references to create their
articles, and we follow the same strategy in this
work, relying on the web is flawed. The prominent
reason is that the internet is full of bias in it of it-
self. For example, Donna Strickland, who received
a Nobel Prize, did not have a Wikipedia article10
10https://wikimediafoundation.org/news/
2018/10/04/donna-strickland-wikipedia/#:
~:text=Donna%20Strickland%20is%20an%
as there was not sufficient content about her on the
web as a basis for her article. Thus, it is important
to recognize that the availability of references is
problematic, affecting the downstream ability to
write accurate, comprehensive biographies. Fur-
ther, information on the web can be contradictory,
information can be affected by the passage of time,
and not information on the web is necessarily fac-
tually correct. Our proposed modeling mechanism
does not have a way to explicitly recognize or cor-
rect for these challenges, which also plagues text
generation generally.
10.2 Focus on English Limits Inclusivity from
Other Languages
Our work focuses on text generation in English
only, which limits inclusivity purely on the basis of
language. This is challenging as the content of the
internet and Wikipedia itself is different in various
languages. For example, articles about people from
Germany may be more likely to be located on the
German version of Wikipedia. Another factor is
that the content of the references may be written
in another language, and then used by a bilingual
individual to write an article in English about that
subject. This is often the case for many biograph-
ical subjects who may be more well known in a
non-English speaking area.
10.3 Evaluation focuses on Women Only, Not
Other Groups
There are a very large number of marginalized
groups in the world and numerous important in-
tersectional aspects to consider. When discussing
identity, a wide variety of factors and personal
views influence individuals when thinking about
how they describe themselves. Our evaluation
dataset focuses on women alone, which leaves out
many groups, including non-binary people. Further,
Wikipedia may not reflect the up-to-date informa-
tion — names and gender are both mutable, for
example — and Wikipedia articles do not ask each
subject to self-report their gender. Finally, we note
that by grouping people into hard categories, there
can potentially be harm — such as limiting people
from opportunities because of their gender or race.
However, we strongly believe that it is important
to recognize bias in its various forms as it exists,
particularly in popular, default online sources of
information such as Wikipedia.
20optical,of%20a%20Sloan%20Research%
20Fellowship.10.4 Bias in Style, Word Choice, and Tone
In this work, we focus on bias manifesting as un-
equal prevalence and length of biographical content
on Wikipedia, focusing specifically on different
intersectional groups of women. However, bias
manifests in a number of other ways. Studies have
indicated that the words used in biographies about
women compared to biographies about men (Di-
nan et al., 2019) also differs, and is reflective of
gendered terminology. For example, many articles
about women are actually written with a lot of infor-
mation about men, such as their husband’s careers,
and articles about actresses describe more often
their physical appearance. This is also a manifes-
tation of bias, and we do not present any focused
modeling techniques to address this type of bias
explicitly.
10.5 Biographies as Records
In the modern internet, a large number of events
are recorded for the public record. These include
events that people may personally prefer to forget,
often termed right to be forgotten11. Automati-
cally generating biographies about individuals may
collate such information in an easily accessible
public place, which can conflict with this personal
right. This has a complex but important interac-
tion with marginalized groups. For example, many
celebrities who are women, transgender, or a part
of another marginalized group are far more likely
to have news articles written about intimate per-
sonal details such as plastic surgeries. Thus, it is
important to consider the interaction of biograph-
ical data with individual privacy. This is a larger
challenge of biographical information generally.","bias, English-centric, exclusivity","bias, English-centric, exclusivity","bias, misuse",2022,"statement, concerns, suggestions"
327,"Research on active learning improves the labeling
of data, by efﬁciently supporting the learning al-
gorithm with targeted information, so that overall
less data has to be labeled. This could contribute
to creating machine learning models, which would
otherwise be infeasible, either due to limited bud-
get, or time. Active learning can be used for good
or bad, and our contributions would—in both cases–
show how to make this process more efﬁcient.
Moreover, we use pre-trained models, which can
contain one or more types of bias. Bias, however,
affects all approaches based on ﬁne-tuning pre-
trained language models, but therefore this has to
be kept in mind and mitigated all the more.
",bias,bias,bias,2022,"statement, actions, concerns"
328,"We have performed our research so far only for the
English language. Though we believe that similar
results can be obtained for non-English languages
we have yet to demonstrate this. We only use query
and product interaction data for our work. Anyidentifiable user information is completely stripped
before we can access the data. As our method is
ultimately used to retrieve a set of products in an
e-commerce store, incorrect predictions will not
cause harm to the user besides an unsatisfactory
experience.
",English-centric,English-centric,misuse,2022,statement
329,"There is still a mismatch in the adoption of the
methods we release and our understanding of
them (Bianchi and Hovy, 2021). We are releasing
a resource for multi-lingual emotion detection, but
any list of language resources runs the risk of be-
ing (mis)interpreted as exhaustive, with languages
included being regarded as more important than
those that are not. We would like to emphatically
state that this is not the case here: we tried to in-
clude as many languages as possible to allow for
a wide comparison and provide a basis for further
research. Any omission should not be read as a
value judgment.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
330,"In this paper, we propose term attention entropy
as a proxy for unintended bias in attention-based
architectures. Our approach allows us to extract,
for a given classifier and data set, a list of terms that
induce most of the bias in the model. While this
list is intuitive and easy to obtain, we would like to
point out some ethical dual-use considerations.
The process of collecting the list is a data-driven
approach, i.e., it is strongly dependent on the task,
collected corpus, term frequencies, and the chosen
model. Therefore, the list might lack specific terms
or include terms that do not strictly perpetrate harm,
but are prevalent in the sample. Because of these
twin issues, the resulting lists should notbe read as
complete or absolute. We discourage users from de-
veloping new models based solely on the extracted
terms. We want, instead, the terms to stand as a
starting point for debugging and searching for po-
tential bias issues in the task at hand, be it in data
collection or model development.
Further, while the probability is low, we can
not exclude the possibility that future users run
EAR on other tasks and data sets to derive private
information or profile vulnerable groups.
","bias, privacy","bias, privacy","bias, misuse",2022,"statement, concerns"
331,"We understand that providing social bias tests as a
quantifiable indicator for bias carries a significant
risk. A low score on a social bias test might be used
to assert that a model is fully devoid of bias. As
Nangia et al. (2020), we strongly advise against this.
Tests can be an indication of issues. Conversely,the
absence of a high score does not necessarily entail
the absence of bias. Neither do replace a thorough
investigation of the data.
",bias,bias,bias,2022,"statement, suggestions, concerns"
332,"In this paper, we isolate the harmful sentence com-
pletions generated by LLMs from templates having
as subjects LGBTQIA+ identity terms. The harm-
ful sentences should not be used to train a language
or classification model.
We use a finite list of identity terms represen-
tative of the LGBTQIA+ community. While this
list may be useful to understand the studied phe-
nomenon, we do not claim this list is exhaustive aslanguage changes and novel terms are constantly
added to our vocabulary.
",harmful_content,incompleteness ,misuse,2022,statement
333,"This paper assess the extent to which existing tool-
ing can help us understand unsafe phenomena ex-
hibited by E2E conversational models when de-
ployed with humans. As part of this study, we
release SAFETY KITas a “first aid kit” for quickly
assessing safety concerns. As noted, the tooling
provided in SAFETY KIThas several limitations
which restrict its utility, and it is thus recommended
for use only as a preliminary step towards consid-
ering the ethical and social consequences related to
the relative safety of an end-to-end conversational
AI model. We describe several limitations as well
as additional ethical considerations here.
Language Firstly, the unit and integration tests
are limited to English-language data that has largely
been collected using crowdworkers located in the
United States. As the very notion of offensiveness
is highly dependent on social context (Hovy and
Yang, 2021), this will be insufficient for measur-
ing the appropriateness of a model’s responses in
other dialects, cultures, and languages (Schmidt
and Wiegand, 2017). Approaches, like the HON-
EST score (Nozza et al., 2021) can help begin to
address this issue on a language basis. However,
even for English speakers in the United States, the
tools posed in this work may have limited utility:
see discussion in the next paragraph.
Bias and accuracy of automatic tooling For the
unit tests, we rely on automatic tooling to provide
a picture of the behavior of a conversational agent.
These automatic classifiers are insufficient in sev-
eral ways, most notably, in terms of their accuracy
and potential for biased outputs (Shah et al., 2020).
Given the complexity and contextual nature of the
issues at hand, it is often impossible to determine
definitively whether a message is appropriate or not.For offensive language detection, inter-annotator
agreement (IAA) on human labeling tasks is typ-
ically low (Fortuna, 2017; Wulczyn et al., 2017).
In order to resolve this disagreement, aggregate or
majority “ground truth” labels are assigned, which
run the danger of erasing minority perspectives
(Blodgett, 2021; Basile et al., 2021; Basile, 2021).
And even for examples with high agreement, it
is likely that these existing classifiers may make
mistakes or do not adequately assess the appro-
priateness of a response – see the error analyses
of the results in §3.1.1 and §3.1.2. For example,
these tools may have difficulty with complex sen-
tence construction, such as sentences with multiple
negation, or with pieces of text that contain subtle
cultural references, etc.
In particular, these tools may have limited util-
ity for underrepresented and marginalized groups.
Various social factors affect how people produce
language, and given that crowdworker demograph-
ics differ substantially from the general population
of the United States (Ross et al., 2009), we would
likely expect that these technologies work less well
on some varieties of English. Indeed, recent work
has shown that popular toxicity detection and miti-
gation methods themselves – including ones used
in this work – are biased (Röttger et al., 2021).
For example, Sap et al. (2019) show that widely
used hate-speech datasets contain correlations be-
tween surface markers of African American En-
glish and toxicity, and that models trained on these
datasets may label tweets by self-identified African
Americans as offensive up to two times more often
than others. Zhou et al. (2021) show that exist-
ing methods for mitigating this bias are largely
ineffective. Xu et al. (2021a) show that popular
methods for mitigating toxic generation in LLMs
decreases the utility of these models on marginal-
ized groups, potentially resulting in harms such
as forcing marginalized users to code-switch. No-
tably, the list of words and phrases used to detect
which responses contain unsafe language (§3.1.1)
contains words like twink ; filtering out or mark-
ing these words as “unsafe” may have the effect of
limiting discourse in spaces for LGBTQ+ people
(Bender et al., 2021).10It is important that future
contributions to SAFETY KITbe inclusive of under-
represented communities, and as such, more work
is needed to be done to understand the impact of
existing safety tooling on those communities.
10Observation made by William Agnew.
Lastly, most of these tools are static (or are
trained on static data) and as such do not account
for value-change, such as when a word takes on
a new cultural meaning or sentiment, like “coron-
avirus.”","bias, English-centric","bias, English-centric","bias, misuse",2022,"statement, concerns"
334,"This paper fits in to the body of responsible and
fair AI research by offering best practices towards
ensuring the annotation ecosystem for dataset cre-
ation is responsive to the relevant groups in the
Arabic-speaking world, and by extension local com-
munities. The general goals of this work are to limit
representational and potential downstream alloca-
tive harms (Barocas et al., 2019). Moreover, the
work strives to advise on building NLP systems that
allow annotators to be a voice for their local com-
munities, where they are appreciated for their skill
and ability to provide deep problem understanding.
Expert Inclusion: Regional/country experts with
Arabic variety proficiency are included in all stages
of the guideline formulation, research design and
implementation, and producing the comparative
annotation results in Section 3. Their expertise is
critical to the entire research process.15
Audience: This work is targeted for the NLP com-
munity. The guidelines are formulated with an
understanding that practitioners may not have the
resources to implement them all to the fullest ex-
tent. They are north stars to aim for, however if
language/content understanding and reviewer sup-
port are not achievable with the resources at-hand,
the practitioner can reduce the dialect/variety cov-
erage of the annotation accordingly. We advocate
for a nuanced approach in dataset creation over
comprehensive coverage.
Scope: This work is limited to Arabic varieties,
with the hope that researchers can gain insight into
handling other polyglossic, multidialectal global
languages as well as a sense for the complexities
of dataset creation for NLP. There are certainly
nuances to Arabic annotation that are not covered
in this work that affect annotation such as code
switching and orthographic considerations.
Furthermore, we focus on the broad Arabic di-
alects when discussing representation in the guide-
lines (§4). We recognize that there are other, deeply
important areas of diversity and representation in-
cluding gender, political stance, religion, etc., how-
ever we considered these outside the scope of this
particular paper. What we describe here are rec-
ommended minimal representation requirements
considering language at as high of a level as pos-
15Due to safety concerns, we have not included these ex-
perts in authorship, and cannot include them all by name
in acknowledgments. Instead, we have acknowledged them
personally and professionally where possible.sible. Of course other groupings have particular
manners of language use that could be required for
fully accurate annotation.
Broader Impacts: NLP systems are embedded in
our multifaceted, ever-changing societies, and it is
therefore necessary to consider the model’s poten-
tial or realized impacts, as well as the productive
and adversarial manners in which the world can
feedback to the model (Sambasivan et al., 2020;
Hagerty and Rubinov, 2019). The primary scope
of this paper is data, however in what follows we
discuss elements of model support that can provide
constructive feedback to the system.
First, this paper calls for soliciting stakeholder
input, particularly through working with annota-
tors who are of the community the model aims to
capture. But further consultation with groups such
as regional user-advocacy groups can be important
to garner a higher-level view and broader prob-
lem understanding to prevent potential issues and
low performance for underserved groups (Martin Jr
et al., 2020; Caliskan et al., 2017; Bruckman, 2020;
Ovadya and Whittlestone, 2019; Abid et al., 2021).
Other practices to aid the practitioner in envi-
sioning the possible impacts of their work include:
staged system roll-out or prototyping to get ahead
of any unforeseen issues before full launch, and
performing an impact investigation. Impact investi-
gations are worthwhile, though they are neither sim-
ple nor straightforward and there are no clear norms
(Prunkl et al., 2021; Partnership on AI, 2021).
The nature of statistical prediction means careful
error handling is of the highest importance, as these
systems will never be mistake-free, and in fact NLP
systems can have surprising or unanticipated errors.
In creating NLP systems, practitioners can ask what
can be done to minimize potential negative impacts
of errors (Hellman, 2019). At the massive scales at
which AI can operate, even a small error rate could
affect many people (Sullivan, 2016).
And, to garner constructive feedback, mean-
ingful transparency measures are important (Di-
akopoulos, 2016; Mitchell et al., 2019), as are
mechanisms for external feedback for model im-
provement in order to allow the model to be respon-
sive to external events.
These practices are generally important, but
especially-so for the Arab world, as much of
MENA is in-conflict, afflicted by ongoing ten-
sions and political violence (Amnesty International,
2020; Johnsen, 2021) that can be amplified by tech-
nology (Shea and al-Hassani, 2021) or harnessed
by violent and/or authoritarian state actors (Human
Rights Watch, 2017).","misuse, transparency","misuse, limited scope","misuse, transparency",2022,"statement, concerns, suggestions"
335,"We do not foresee any immediate ethical concerns
for our method as we use several constraints (less
divergence from the extracted knowledge, consis-
tency with the dialog context) that allow the gen-
eration to be restricted to the context. In general,
we expect our dialog system to be more engaging
and accessible to the user. Since we use PTLMs
as knowledge source, we inherit the general risk of
generating biased or toxic language, which should
be carefully ﬁltered. In our work, we perform ex-
plicit ﬁltering steps to make sure that the knowl-
edge is appropriate . Furthermore, our selection
step promotes more factually correct knowledge to
be selected. However, the generations may incor-
porate biases that are already present in the dialog
datasets due to crowd-sourced data collection. Fi-
nally, our generations are limited only to the En-
glish language. Hence we suggest that a system like
ours should likely not be used as a ‘black box,’ but
would best be used in a setting where its outputs can
be ‘audited’. Carbon footprint: Our system uses
post-hoc knowledge injection which refrains from
retraining newer dialog models to accommodate
dynamically evolving external knowledge. This
promotes green NLP applications (Schwartz et al.,
2020; Strubell et al., 2019) reducing carbon foot-
prints that stem from training (or even ﬁnetuning)
large language models.","bias, environmental_impact","bias, environmental impact","bias, environmental_impact",2022,"statement, concerns"
336,"The proposed approach takes steps towards a novel
paradigm that might partially mitigate the need for
energy-intensive GPU training – potentially leading
to positive environmental_impact down the line.
The approach may also have positive impacts on
accessibility as strong computational resources are
not required when setting up a new controlled text
generation system. We do however acknowledge
that strong controlled generation methods that rely
on discriminators have the potential to regurgitate
sensitive training data and produce harmful outputs
and toxic language (Xu et al.; Gehman et al., 2020;
Wallace et al., 2020). However, if used properly
and for good, we anticipate a positive impact on
debiasing and safe generation.
","sensitive_data, toxic_language","harm, toxic language","bias, misuse, environmental_impact",2022,"statement, concerns"
337,"We proposed a more efﬁcient way of building TOD
systems by leveraging demonstrations in place of
descriptions, leading to increased accuracy with
minimal/no data preparation overhead. We con-
duct our experiments on publicly-available TOD
datasets in English, covering domains which are
popular for building conversational agents. We
hope our work leads to building more accurate
TOD systems with similar or less overhead and
encourages further research in the area.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
338,"This research used a considerable amount of com-
putational resources and this is our main ethics
concern for conducting this work. We did try to
keep the number and the size of models we experi-
mented with limited, to reduce the carbon footprint
of the experiments. We hope the results we show
in this paper are worth the computational resources
used.
In this study, we looked at coarse-grained
groups deﬁned by the text content mentioning re-
ligion/race/gender, which may obfuscate the be-
havior of the models with respect to ﬁner-grained
groups, such as females and males. Similarly, we
did not consider intersectionality.
Bias mitigation can lead to undesirable outcomes.
For example, one aspect we did not look into is
what happens with other groups when the miti-
gation is applied only for one of the groups. In
addition, we focused only on group fairness and
do not provide any insights into individual fairness.
We also recognize that abstract metrics have limita-
tions and the societal impacts resulting from bias
mitigation are not well understood (Olteanu et al.,
2017). These issues are universal to bias mitigation
techniques and not particular to our use case.
Last, but not least, the datasets we used are En-
glish only. We acknowledge the importance of per-
forming similar studies on multi-lingual datasets.
","carbon_footprint, bias","environmental impact, English-only","bias, environmental_impact",2022,"statement, concerns"
339,"The spread of clickbait on social media by news
publishers to promote click-through to their web-
sites has been empirically found to decrease their
perceived credibility in readers (Molyneux and
Coddington, 2020). There is, of course, nothing
wrong with monitoring and optimizing the effec-
tiveness of marketing a newly published news arti-
cle, especially in cases where the editors make an
honest effort to reach and inform their target audi-
ence. But the clickbait in our corpus mostly spreads
trivial facts that could have been easily ﬁtted into
the length limits of a social media post, which is
why we consider these posts to fall short of the
journalistic ideal. However, it is as of yet unclear,
in terms of journalism ethics, whether clickbait is
an acceptable means to an end for publishers (i.e.,
whether it is “necessary in driving audiences to
the journalism they need by giving them the jour-
nalism they seem to want.”), or whether it serves
to “crowding out «real» journalism by reducing
quality in favor of the need for a click-through at
whatever cost” (Harte, 2021).
Facebook intervened twice with algorithmic ﬁl-
ters to reduce the amount of clickbait that people
are exposed to in their timelines—even though this
probably also lowered Facebook’s user engagement
metrics. Our technology demonstrates another,
complementary way of relatively simply circum-
venting the purported exploitation of the curiosity
gap by giving the audience a choice on whether
or not they wish their cognitive “loopholes” to be
exploited. If a sufﬁciently large portion of peo-
ple decide to adopt spoiling tools, that would send
a clear message to publishers and social media
platforms alike. Spoiling clickbait, as opposed to
removing it, however, still gives publishers the ben-
eﬁt of the doubt, since, as the publishers claim,
there are people who enjoy these kinds of trivia.
",no_ethical_concerns,no_ethical_concerns,misuse,2022,statement
340,"Identifying values in argumentative texts could
be used in various applications like argument
faceted search, value-based argument generation,
and value-based personality proﬁling. In all these
applications, an analysis of values has the oppor-
tunity to broaden the discussion (e.g., by present-
ing a diverse set of arguments covering a wide
spectrum of personal values in search or inviting
people with underrepresented value-systems to dis-
cussions). At the same time, a value-based analysis
could risk to exclude people or arguments based on
their values. However, in other cases, for example
hate speech, such an exclusion might be desirable.
While we tried to include texts from different
cultures in our dataset, it is important to note that
these samples are not representative of their re-
spective culture, but intended as a benchmark for
measuring classiﬁcation robustness across sources.
A more signiﬁcant community effort is needed to
collect more solid datasets from a wider variety of
sources. To facilitate the inclusivity of different
cultures, we adopted a personal value taxonomy
that has been developed targeting universalism and
tested across cultures. However, in our study, the
annotations have all been carried out by annotators
from a western background. Even though the value
taxonomy strives for universalism, a potential risk
is that an annotator from a speciﬁc culture might
fail to correctly interpret the implied values in a
text written by people from a different culture.
Finally, as mentioned in Section 4, we did not
gather any personal information in our annotation
studies, and we ensured that all our annotators get
paid more than the minimum wage in the U.S.",no_ethical_concerns,no_ethical_concerns,privacy,2022,"statement, concerns"
341,"Enhancing the domain converge of pre-traind lan-
guage models (PLMs) with external knowledge
is increasingly important, since the PLMs cannot
observe all the data during training and cannot
memorize all the necessary knowledge for solv-
ing down-stream tasks. Our KALA contributes to
this problem by augmenting domain knowledge
graphs for PLMs. However, we have to still con-
sider the accurateness of knowledge, i.e., the fact in
the knowledge graph may not be correct, which af-
fects the model to generate incorrect answers. Also,
the model’s prediction performance is still far from
optimal. Thus, we should be aware of model’s
failure from errors in knowledge and prediction, es-
pecially on high-risk domains (e.g., biomedicine).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
342,"Retrieving the most relevant documents from the
user’s query is increasingly important in a real-
world setting, as it is widely used from web search,
to question answering, to dialogue generation sys-
tems. Notably, our work contributes to the accurate
retrieval of documents with the proposed data aug-
mentation strategies, thus improving the document
retrieval performances on real-world applications.
However, we have to still consider the failure of
retrieval systems on low-resource but high-risk do-
mains (e.g., biomedicine), where the labeled data
for training retrieval models is limited yet one fail-
ure can yield a huge negative impact. While we
strongly believe that our data augmentation strate-
gies – interpolation and perturbation of document
representations – are also helpful to improve the
retrieval performances on such low-resource do-
mains, the model’s prediction performance is still
far from perfect, and more efforts should be made
to develop a reliable system.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
343,"As we propose a novel conversation summarization
dataset creation pipeline and modeling components,
this section is divided into the following two parts.
8.1 New Dataset
Intellectual Properties and Privacy Rights We
make use of publicly-available StackExchange data
for all our annotations. We manually reviewed our
dataset output for quality and potential problems.
Compensation for Annotators Compensation
was determined by standard in-house rates, amount-
ing to about $6 per data point collected.
8.2 NLP Application
Bias Biases may exist in the datasets, such as
political bias and gender bias in Yahoo! Answers.
Thus, models trained on these datasets may propa-
gate these biases.
Misuse Potential and Failure Mode When
used as intended, applying the summarization mod-
els described in this paper can save people much
time. However, the current models are still prone
to producing hallucinated summaries, and in such a
case, they may contribute to misinformation on the
internet. We move the needle in faithful summa-
rization in this paper, but further research is needed
to ensure the faithfulness of abstractive summaries
to address this issue, as this issue is present among
all current abstractive summarization models.
Environmental Cost The experiments described
in the paper make use of V100 GPUs. We used
up to 8 GPUs per experiment. The experimentsmay take several hours. Several dozen experiments
were run due to parameter search, and future work
should experiment with distilled models for more
light-weight training. We note that while our work
required extensive experiments to draw sound con-
clusions, future work will be able to draw on these
insights and need not run as many large-scale com-
parisons. Models in production may be trained
once for use using the most promising settings.
","bias, environmental impact","bias, environmental impact","bias, misuse",2022,statement
344,"Risk: In this work, we have not controlled the
entities in utterance templates during generation.
This presents a risk of private information
propagating into the synthetic data. We note that
the entities themselves are not introduced during
generation, but carried over from real data. As
mentioned in Section 8, entity control methods
such as considered in the present work with GIT
can prevent such identiﬁable information from
being exposed to model training.
Data Protection: There are multiple guardrails to
safeguard customer data in our organization. In ad-
dition, we remove all metadata and personal iden-
tiﬁable information (PII) from utterances before
using them for NU model building and synthetic
data generation with GIT in this work.
",privacy,privacy,no ethical concerns,2022,statement
345,"This dataset should be ONLY used for NLP re-
search purpose. Questions are artificial and do not
contain any personal information. Answers are
NOT provided by legal professionals and should
NOT be used for any legal purposes.
8
",no_ethical_concerns,no_ethical_concerns,privacy,2022,
346,"We declare that all authors of this paper acknowl-
edge the ACM Code of Ethics and honor the code
of conduct. This work essentially considers black-
box attacks on the private persona attributes and
proposes effective learning strategies to prevent
chatbots from overlearning private personas.
Dataset . During our dataset collection, all the
conversations and personas are collected from pub-
licly available datasets including PersonaChat and
DNLI. All the speakers are anonymized and no
identifiable personal information is included.
Model . For training our LM-based chatbots, we
follow standard methods. We are well aware of the
bias issue inside current language models. In the
future, if there are other fairer language models, we
will extend our defenses on them.

",bias,bias,"privacy, bias, security",2022,"statement, concerns"
347,"We contend that while automatic rumour detection
systems can beneﬁt combating the spread of mis-
information, there are potential risks to them. If
these systems are deployed on social media to mon-
itor user posts without human oversight, there are
implications when these systems misclassify (par-
ticularly in the false positive cases) and users are
wrongfully accused for posting misinformation. As
such, we recommend these tools to be used as an
aid, e.g. by ﬁltering the enormous volume of data
and help human analysts to narrow down and detect
harmful stories on social media.
",wrong_accusation,misuse,wrong_accusation,2022,"statement, concerns"
348,"We acknowledge that our proposed model may be
susceptible to learning harmful biases present in
the dataset. In and of itself this has the poten-
tial to harm minorities, marginalised communities
and project stigmas present in society. Further, we
recognise that our efforts to improve coherence,
cohesion and control might be misused to author
offensive or ﬁctitious content. Therefore, we advo-
cate for morally correct and responsible practices
in the case of real-world application.
","bias, misuse, offensive content","bias, offensive content","bias, misuse",2022,"statement, concerns"
349,"Although Tweets are publicly available, given the
sensitivity of the task, we took the following extra
measures, in light of what has been previously pub-
lished by (Benton et al., 2017) and (Šuster et al.,
2017).
•We obtained access to the D2S dataset after
signing a data use agreement (DUA), and we
followed all the agreements and instructions
stated in the DUA. The dataset is stored on
a secure server and not published with other
researchers but those mentioned in the DUA
and got approval.
•We did not obtain institutional review board
(IRB) approval, since the dataset falls under
exempt determination and not IRB approval,
as stated in the code of federal regulations
CFR 46.101(b)(4)4published by the United
States Department of Health and Human Ser-
vices (HHS).",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
350,"During the dataset collection and annotation pro-
cess, we have considered several ethical issues. To
respect the intellectual property of dataset sources,
we only used the publicly available charts that com-
ply with their terms and conditions. According to
Statista publication rights,2users are given open
access to the publicly available charts for academic
purposes. According to the terms and conditions
for Pew,3users are allowed to download and pub-
lish the content as long as they are attributed to the
Center or are not attributed to a different party. Ac-
cording to OECD4terms and conditions, users can
crawl and use the data in their own work for any
purpose unless where restrictions apply. According
to OWID5terms and conditions, all their data are
open access and users can download or utilize the
data in their own work.
In order to fairly compensate the Mechanical
Turk annotators, we considered the minimum wage
in the United States at the time ($7.25 USD per
hour). The estimated time taken for each task is
3-5 minutes. Hence, these annotators received $0.6
USD for each task. Additionally, to protect the
privacy of these annotators, all of their annotations
were anonymized.
To ensure the reproducibility of our experimental
results, our hyperparameters settings are provided
in Appendix A.5.
Our models can be abused to mislead the public
about the charts content and implications. While
our models provide state-of-the-art results on most
of the existing datasets, we can not guarantee that
their output will be correct all the time.","misuse, inaccuracies",misuse,intellectual property,2022,"statement, concerns"
351,"We work within the scope of acceptable privacy
practices suggested by Chancellor et al. (2019)
and considerations presented by Fiesler and Pro-
feres (2018) to avoid coercion and intrusive treat-
ment. The primary source of the dataset used in
this study is Reddit. Although Reddit is intended
for anonymous posting, we take further precau-
tions by performing automatic de-identiﬁcation of
the dataset using named entity recognition (Zirikly
et al., 2019). All examples used in this paper are
further been anonymized, obfuscated, and para-
phrased for user privacy (Benton et al., 2017) and
to prevent misuse as per the moderate disguise
scheme suggested by Bruckman (2002). Taking
inspiration from Benton et al. (2017), we also
keep the annotation of user data separate from raw
user data on protected servers linked only through
anonymous IDs. Our work focuses on building
an assistive tool for screening suicidal users and
providing judgments purely based on observational
capacity. We acknowledge that it is almost impos-
sible to prevent abuse of released technology even
when developed with good intentions (Hovy and
Spruit, 2016). Hence, we ensure that this analysis
is shared only selectively to avoid misuse such as
Samaritan’s Radar (Hsin et al., 2016).
We further acknowledge that the studied data
may be susceptible to demographic, expert annota-
tor, and medium-speciﬁc biases (Hovy and Spruit,
2016). While the essence of our work is to aid in the
early detection of at-risk users and early interven-
tion, any interventions must be well-thought, fail-
ing which may lead to counter-helpful outcomes,
such as users moving to fringe platforms, making
it harder to provide assistance (Kumar et al., 2015).
Care should be taken to not to create stigma, and
interventions must hence be carefully planned by
consulting relevant stakeholders, such as clinicians,
designers, and researchers (Chancellor et al., 2016),
to maintain social media as a safe space for indi-
viduals looking to express themselves (Chancellor
et al., 2019). It is also essential that clinicians and
human moderators are not overburdened (Chancel-
lor et al., 2019). For instance, “Alarm fatigue” is
when alarms are so excessive, many of which are
false positives, that healthcare providers become
desensitized from alarms (Drew et al., 2014).
We also agree that suicidality is subjective (Keilp
et al., 2012), wherein the interpretation may vary
across individuals on social media (Puschman,
2017). We do not make any diagnostic claims,
rather help prioritize the users that should be evalu-
ated by the medical professionals ﬁrst, as part of a
distributed human-in-the-loop framework (de An-
drade et al., 2018).","privacy, misuse","privacy, misuse","privacy, bias, misuse",2022,"statement, concerns, actions"
352,"During the dataset collection and annotation pro-
cess, we had many ethical issues to take into con-
sideration. To respect the intellectual property of
the chart publishers, we only used publicly avail-
able charts from resources that provide publication
rights of downloaded content for academic pur-
poses. According to the terms of use and publi-
cation rights for Statista,4users are granted pub-
lication rights only to free studies of Statista, so
we only used the free publicly available webpages.
According to the terms and conditions for Pew,5
users are allowed to use the content as long as they
are attributed to the Center or are not attributed to
a different party.
To fairly compensate the Mechanical Turk an-
notators, we compensated the annotators based on
the minimum wage in the United States at the time
(7.25 US$ per hour) and the estimated time taken
for each task (1 minute). Hence, these annotators
received 0.10 - 0.15 US$ for each chart, depend-
ing on the number of candidate paragraphs asso-
ciated with it. Additionally, to protect the privacy
of these annotators, all of their annotations were
anonymized.
To ensure the reproducibility of our experimental
results, we have provided the hyperparameter set-
tings and estimated training time in Appendix A.3.
We foresee one possible misuse of our models
that is to spread misinformation. Currently, our
model outputs tend to appear fluent but contain
some hallucinations and factual errors, as detailed
in §5.3. Hence, if such model outputs are pub-
lished without being corrected, it may mislead and
misinform the general public.
",misinformation,misinformation,"misuse, misinformation",2022,"statement, concerns"
353,"Data in MULTI HIERTT is collected from the
FinQA dataset (Chen et al., 2021) and FinTabNet
dataset (Zheng et al., 2021). FinQA is publicly
available under the MIT license6. FinTabNet is pub-
licly available under the license CDLA-Permissive-
1.07. Both licenses permits us to compose, modify,
publish, and distribute additional annotations upon
the original dataset.
For the internal annotation of MULTI HIERTT ,
each expert is paid $20 per hour. For the external
annotation, we hire 23 graduate students majoring
in ﬁnance or similar disciplines. We regard creat-
ing one question-reasoning pair, or validating one
document’s annotation as a unit task. And we pay
around $1.1 for each unit task. Averagely, an anno-
tator can ﬁnish 7 unit tasks per hour after training
6https://opensource.org/licenses/MIT
7https://cdla.dev/permissive-1-0/and practicing. And the hourly rates are in the
range of $6 and $9 based on the different work-
ing speed (above the local average wage of similar
jobs). In total, the approximate working hours to
annotate MULTI HIERTT dataset is 1500 hours. The
whole annotation work lasts about 70 days.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
354,"For the human evaluation in this paper, we recruit
several annotators on Amazon Mechanical Turk
from English-speaking countries. We pay the an-
notators USD$0.15 for each annotation task. Each
task can be finished on average in 1 minute, which
amounts to $9.0 per hour that is above the US fed-
eral minimum wage ($7.25). To ensure the quality
of the human evaluation results, we perform quality
control in a few ways. First, the annotators will be
shown our scoring standards (Appendix H) before
their tasks, and are asked to follow them. If the task
is not done properly, either as determined by expert
judgements (we recruit 3 native English speakers
to validate the results of the Turkers’ annotations)
or there are obvious patterns such as constantly giv-
ing the same score for all tasks, we remove their
annotations. We also compute agreement score to
check for the consistency among the annotators.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
355,"This paper introduces a dataset and system for up-
dating an existing piece of text to incorporate in-
formation from external evidence. Depending on
the veracity of the external evidence, systems for
solving this task could potentially be abused by bad
actors to spread misinformation.
",misinformation,misuse,misinformation,2022,"statement, concerns"
356,"The research on toxicity raises some ethical issues.
In terms of our work, the parallel corpus we cre-
ated can indeed be used in the reverse direction,
i.e. to “toxify” sentences. However, although we
did not thoroughly evaluate the quality of such
toxiﬁcation, our intuition is that it would not be
high enough to make the corrupted sentences look
natural. The reason is that the toxic part of our cor-
pus consists of real toxic sentences fetched on the
Internet, whereas their non-toxic counterparts are
“translations” performed by crowd workers. We
suggest that they obey the common regularities ob-
served for translationese (texts manually translated
from their original language into a different one):
they differ from regular texts in terms of vocabu-
lary (Koppel and Ordan, 2011) and syntax (Lem-
bersky et al., 2011). The manually detoxiﬁed texts
are different from the original non-toxic texts writ-
ten by Internet users from scratch. While they are
still recognized by human assessors as plausible
sentences, we suggest that a sequence-to-sequence
model trained to get translationese as input would
not be as successful in transforming real texts (as it
was shown for machine translation models (Freitag
et al., 2019)).
Thus, although our corpus can be used in the
reverse direction, it is not symmetric, which makes
it less efﬁcient as training datasets for “toxiﬁers”.
However, we should emphasize that these state-
ments are our hypotheses and should be further
investigated. Finally, we argue that the risk of
using our corpus for toxiﬁcation is perhaps not
game-changing, as simpler approaches based on
patterns (e.g. including a set of predeﬁned obscene
fragments into neutral texts) can serve the same
purpose relatively well.
",no ethical concerns,misuse (toxification),no ethical concerns,2022,"statement, concerns"
357,"Data Privacy In line with prior behavioral stud-
ies, our work illustrates that sociolinguistic cues
are essential for predicting code-switching points.
To deploy our speaker-informed model, we must
protect the identity and privacy of users through
techniques such as federated machine learning: de-
ploying local models to end-users without sending
any user information back to the cloud (Kone ˇcný
et al., 2016). Local models and data should be
encrypted to prevent breaches and tampering with
algorithms, as well as possible reconstruction of
training data (Hitaj et al., 2017; Carlini et al., 2019;
Zhang et al., 2020), minimizing the risk of leaking
speaker information. Additionally, deployed sys-
tems should only collect and access information if
the user agrees to it. All conversational participants
voluntarily shared the data we use.
Moreover, this research is important to conduct
because there is evidence that human users react
positively to appropriately adaptive technologies
(Branigan et al., 2010). Specifically, initial experi-
ments indicate that users rate dialogue systems that
incorporate code-switching higher than ones that
do not (or that do it less naturally) (Ahn et al., 2020;
Bawa et al., 2020). A classifier, such as the one we
explore in this work, can be very useful for devel-
oping a naturalistic dialogue system that is useful
and enjoyable to use by people of diverse linguis-
tic backgrounds. Our work focuses on English-
Spanish code-switching which is widespread and
accepted, but different regions and cultures have
varying opinions of code-switching. It is important
to understand these before building an application
for a new language pair (Do ˘gruöz et al., 2021).","privacy, security","privacy, security","privacy, security",2022,"statement, suggestions, concerns"
358,"This work will not pose ethical problems, the data
resources we use are all from published works and
do not involve privacy issues related to data collec-
tion. The data is collected from BBC and contains
thousands of diverse speakers, allowing the speech
recognition models to generalize to all speakers. In
terms of computational experiments, we used pub-
licly available pre-trained models, which makes the
training more environmentally friendly and lowers
the computational requirements to reproduce our
work.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
359,"Since our approach is unsupervised and based on
the coreference signal produced by Stanford’s de-
terministic coreference system (Lee et al., 2011,
2013), it is prone to echoing biases present in the
linguistic rules embodied by Stanford’s resolver.
Moreover, as most coreference resolvers, the ap-
proach we presented is not designed for a partic-
ular use case, but it is rather expected to be em-
ployed within more complex NLP systems. Spe-
ciﬁc domains in which these systems are applied
(e.g., biomedical data, legal documents) might re-
veal potential fairness shortcomings in the underly-
ing Stanford’s sieve-based system. Depending on
the setting of application (e.g., voice assistants or
search engines), these possible defects could pro-
duce undesirable outcomes. For instance, wrongly
classifying two people as the same person is pos-
sible to affect information extraction results (e.g.,
search engines). Further studies on alternative do-
mains are needed to assess these aspects.
Contextual word embedding models such as
ELMo (Peters et al., 2018), BERT (Devlin et al.,
2019), and SpanBERT (Joshi et al., 2020) are pre-
trained with self-supervised procedures on large
portions of unlabeled text. These models are op-
timized to capture statistical dependencies and
might retain and amplify prejudices and stereotypes
present in the training data (Kurita et al., 2019).
Since the method we propose relies on such pre-
trained models, it inevitably inherits possible biases
that might affect its fairness.
","bias, fairness",bias,bias,2022,"statement, concerns"
360,"The annotations in this paper were compensated ac-
cordingly (see Appendix). Also, for all the datasets
used in the research, we stick to the ethical stan-
dards giving credit to the original author. We en-
courage future work that take advantage of these
resources, to cite also the original sources of the
data.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
361,"Recent work has highlighted issues of stylistic bias
in text generation systems, speciﬁcally machine
translation systems (Hovy et al., 2020). We ac-
knowledge these issues, and consider style transfer
and style-controlled generation technology as an
opportunity to work towards ﬁxing them (for in-
stance, gender neutralization as presented in Sec-
tion 6.2). Note that it is important to tread down
this path carefully — In Chapter 9, Blodgett (2021)
argue that style is inseparable from social meaning
(as originally noted by Eckert, 2008), and humans
may perceive automatically generated text very dif-
ferently compared to automatic style classiﬁers.
Our models were trained on 32 Google Cloud
TPUs. As discussed in Appendix A, the UR&
UR-INDIC model take roughly 18 hours to train.
The DIFFUR -* and DIFFUR -MLT models are much
cheaper to train (2 hours) since we ﬁnetune the
pretrained UR-* models. The Google 2020 en-
vironment report mentions,15“TPUs are highly
efﬁcient chips which have been speciﬁcally de-
signed for machine learning applications”. These
accelerators run on Google Cloud, which is car-
bon neutral today, and is aiming to “run on
carbon-free energy, 24/7, at all of Google’s data
centers by 2030” ( https://cloud.google.
com/sustainability ).
15https://www.gstatic.com/
gumdrop/sustainability/
google-2020-environmental-report.pdf
",bias,"bias, environmental impact",bias,2022,statement
362,"The idea of unbiased journalism has always been
challenged12because journalists will make their
own editorial judgements that can never be guar-
anteed to be completely bias-free. Therefore, we
propose to generate a comprehensive summary of
articles from different political leanings, instead of
trying to generate a gold standard “neutral” article.
One of the considerations is the bias induced
by the computational approach. Automatic ap-
proaches replace a known source bias with another
bias caused by human-annotated data or the ma-
chine learning models. Understanding the risk of
uncontrolled adoption of such automatic tools, care-
ful guidance should be provided in how to adopt
them. For instance, an automatically generated neu-
tral summary should be provided with reference to
the original source instead of standing alone.
We use news from English-language sources
only and largely American news outlets through-
out this paper. Partisanship from this data refers
to domestic American politics. We note that this
work does not cover media bias at the international-
level or in other languages. In future work, we
12https://www.allsides.com/blog/does-unbiased-news-
really-exist
will explore the application of our methodology to
different cultures or languages. However, we hope
the paradigm of NEUS, providing multiple sides
to neutralize the view of an issue, can encourage
future research in mitigating framing bias in other
languages or cultures.",bias,bias,bias,2022,"statement, suggestions, concerns"
363,"TheQAConv benchmark proposed in this work
could be helpful in creation of more powerful con-
versation retrieval and QA on conversations. How-
ever, QAConv benchmark only covers a few do-
mains as background conversations. Furthermore,
even with our best efforts to ensure high quality and
accuracy, the dataset might still contain incorrect
labels and biases in some instances, which could
be the inherent mistakes from the original dialogue
datasets. This could pose a risk if models that are
evaluated or built using this benchmark are used
in domains not covered by the dataset or if they
leverage evidence from unreliable or biased dia-
logues. Thus, the proposed benchmark should not
be treated as a universal tool for all domains and
scenarios. We have used only the publicly available
transcripts data and adhere to their guideline, for
example, the Media data is for research-purpose
only and cannot be used for commercial purpose.
As conversations may have biased views, for ex-
ample, speciﬁc political opinions from speakers,
the transcripts and QA pairs will likely contain
them. The content of the transcripts and summaries
only reﬂect the views of the speakers, not the au-
thors’ point-of-views. We would like to remind our
dataset users that there could have potential bias,
toxicity, and subjective opinions in the selected
conversations which may impact model training.
Please view the content and data usage with discre-
tion.
",bias,bias,bias,2022,"statement, concerns"
364,"We paid our expert workers fairly, based on the
monthly minimum wage in Indonesia. All workers
were made aware that the submitted stories would
be distributed, and used for research purposes. No
sensitive information about the workers will be
released.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
365,"Our data comes primarily from crosswords pub-
lished in established American newspapers and
journals, where a lack of diversity among puzzle
constructors and editors may influence the types
of clues that appear. For example, only 21% of
crosswords published in The New York Times have
at least one woman constructor (Chen, 2021) and a
crossword from January 2019 was criticized for in-
cluding a racial slur as an answer (Graham, 2019).
We view the potential for real-world harm as lim-
ited since automated crossword solvers are unlikely
to be deployed widely in the real world and have
limited potential for dual use. However, we notethat these considerations may be important to re-
searchers using our data for question answering
research more broadly.
",misuse,misuse,misuse,2022,statement
366,"Analyzing state legislation is a sensitive task, where
unexpected results of research and deployed ML
systems can create misguided beliefs on the gov-
ernment policies on important topics (e.g., health,
education). Thus, we would like to discuss some
ethical aspects related to our work in terms of data
and model (considering potential scenarios sug-
gested by Chandrabose et al., 2021):
1. Selection of data sources . While there can
be different inherent imbalances in the state legisla-
ture (e.g., gender and party distribution), we were
not able to identify that our data sources adding
systematic political and social biases to our study,
e.g., towards demographic populations of legis-
lators. All our data sources (e.g., LegiScan and
FollowTheMoney) are publicly available and have
been used by the political science community over
the years. LegiScan (LegiScan, 2019) is a nonpar-
tisan and impartial legislative tracking and report-
ing service for state bills. FollowTheMoney (Fol-
lowTheMoney, 2019) is a nonpartisan, nonproﬁt
organization revealing the inﬂuence of campaign
money on state-level elections and public policy
in all US states. Finally, Ballotpedia (Ballotpedia,
2019) is a nonpartisan, nonproﬁt organization pro-
viding a brief introduction, biography, committee
assignment, and general information on legislators
across different years. Our study combined these
data sources for analyzing state bills in a broad
context, thus contributing to reduced bias for
all models evaluated in this paper.
2. Selection of states . In addition, to help mit-
igate the risk of data collection bias or topic pref-
erence that can be introduced through the choice
of speciﬁc state legislatures, we randomly picked
a “red”, a “blue”, and a “purple” state (indicating
a signiﬁcant majority for Republicans, Democrats
or more balanced state legislature, respectively).
There were some restrictions in terms of collecting
the data from the above sources (e.g., FollowThe-
Money and Ballotpedia). These data sources and
services often limit the number of API calls and
queries for retrieving the data for educational insti-
tutions. Besides this, annotating the data through
Amazon MTurk was expensive for us so we con-
ducted our study on four highly discussed topics
in state bills (i.e., health, education, agriculture,
and law). We will explore ways of expanding our
dataset to more states and topics over time.
3. Disguised winners and losers . In theory, the
authors of state bills (e.g., interest groups selling
ﬁll-in-the-blank bills to legislators) may try to re-
frame bills (disguise winners or losers) to further
their political aims. At the ﬁrst glance, this could
pose a challenge to our bill annotation, dataset, and
stakeholder analysis. As described in Section 3, the
state legislative process has a multi-stage review-
ing process in two chambers (e.g., ﬁrst reading,
second reading, and third reading). Thus, we have
observed that it is hard to hide the impact of bills on
their relevant stakeholders from our qualiﬁed anno-
tators, i.e., the authors and multiple vetted MTurk
workers for each example, in practice. In addition,
our work on MTurk maps the impact of policies
suggested by bills to winners and losers. Thus, it
already considers those stakeholders that are not
mentioned in the text explicitly (More details in
Appendix A.1).
4. Winners and losers analysis . The analysis,
aligning demographic cleavages with winners and
losers preferences, is done at an aggregate level
based on the data we annotated. These preferences
could be inﬂuenced by other factors beyond demo-
graphics. Deriving conclusions from this analysis
could require longitudinal studies, capturing the
change of these patterns over time, for example
when analyzing policies intended to help correct
inequities towards marginalized groups. Our goal
is to provide a tool for domain experts that would
point at nuanced, stakeholder speciﬁc, legislative
preferences that can be studied further in order to
determine their signiﬁcance.
5. Handling abstain votes . There are abstain
(absent and N/A) votes in our dataset. However, we
did not include them in our study due to their ex-
tremely low frequency (for our proposed model and
other baseline models). We leave this evaluation asa future work.
6. Handling other countries and languages.
While our dataset is speciﬁc to the US, the the
problem we studied, stakeholder analysis, can be
generalized to legislation from other countries and
in different languages. Although we have not eval-
uated such bills (due to lack of data sources), we
expect such legislation to produce winners or losers
to provide practical solutions to their local prob-
lems. In particular, our framework offers a multi-
relational graph abstraction and prediction models
to analyze stakeholders of bills (winners/losers)
and the voting behavior of legislators. These tech-
niques can support non-US national and state-level
legislative processes. To accommodate other lan-
guages, one could adopt cross-lingual embedding
models, e.g., XLM-R (Conneau et al., 2019) in-
stead of RoBERTa, in our architecture.",bias,bias ,bias,2022,"statement, concerns, suggestions"
367,"This paper proposes a knowledge distillation frame-
work that leverages multi-granularity structural
knowledge to compress a large and powerful lan-
guage model into a small one with minimum perfor-
mance degradation, which is beneficial to energy-
efficient NLP applications. The research will notpose ethical problems or negative social conse-
quences. The datasets used in this paper are all
publicly available and are widely adopted by re-
searchers as the general testbed for natural lan-
guage understanding evaluation. The proposed
method doesn’t introduce ethical/social bias or ag-
gravate the potential bias in the data.
",no_ethical_concerns,no_ethical_concerns,bias,2022,statement
368,"A key limitation of our work is the limited coverage
of eight higher-resource languages. As such, we
are unable to test our approach in a genuinely low-
resource scenario. We must also consider the risk
of over-generalization to dominant dialects within
each language as we lack an evaluation of addi-
tional dialects (e.g. our English dataset is represen-
tative of American English but not Indian English).
We hope that such issues can be addressed with
additional data collection.
Our training requirements are detailed in Ap-
pendix A. We hope our work contributes to fur-
ther usage and development of singular multilin-
gual models as opposed to learning Nmonolingual
models forNlanguages.
",no ethical concerns,English-centric,no ethical concerns,2022,"statement, concerncs"
369,"Our study was conducted with the approval of
the Internal Review Board. We informed work-
ers about the risk of being exposed to the hate
content through the HIT title visible to workers
before accepting the HIT on Amazon Mechanical
Turk. The paper’s theme is important as online hate
speech and microaggressions continue to increase;
therefore, there is a need for combating hate au-
tomatically. We hope that our corpus encourages
further studies on this topic. We acknowledge the
limitations that the corpus is only in English and
that the hate speech contents are not fully up-to-
date, such as dealing with the increasing amounts
of hate speech against Asians due to the COVID
pandemic.
",no ethical concerns,English-centric,no ethical concerns,2022,"statement, concerns"
370,"This work does not involve any sensitive data, but
only public unlabeled corpora, i.e., BookCorpus
(Zhu et al., 2015) pre-processed by Zhou et al.
(2021b), and crowd-sourced datasets released in
previous works, including ART (Bhagavatula et al.,
2020), TIMETRA VEL (Qin et al., 2019), APSI
(Zhang et al., 2020a), MCNC (Li et al., 2018),
ROCStories (Mori et al., 2020).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
371,"This work is primarily an effort toward retrieval
models that generalize better while performing
reasonably efﬁciently in terms of space consump-
tion. Strong out-of-the-box generalization to small
domain-speciﬁc applications can serve many users
in practice, particularly where training data is not
available. Moreover, retrieval holds signiﬁcant
promise for many downstream NLP tasks, as it
can help make language models smaller and thus
more efﬁcient (i.e., by decoupling knowledge from
computation), more transparent (i.e., by allowing
users to check the sources the model relied on when
making a claim or prediction), and easier to update
(i.e., by allowing developers to replace or add doc-
uments to the corpus without retraining the model)
(Guu et al., 2020; Borgeaud et al., 2021; Khattab
et al., 2021a). Nonetheless, such work poses risks
in terms of misuse, particularly toward misinforma-
tion, as retrieval can surface results that are relevant
yet inaccurate, depending on the contents of a cor-
pus. Moreover, generalization from training on
a large-scale dataset can propagate the biases of
that dataset well beyond its typical reach to new
domains and applications.
While our contributions have made ColBERT’s
late interaction more efﬁcient at storage costs, large-
scale distillation with hard negatives increases sys-
tem complexity and accordingly increases train-
ing cost, when compared with the straightforward
training paradigm of the original ColBERT model.
While ColBERTv2 is efﬁcient in terms of latency
and storage at inference time, we suspect that un-
der extreme resource constraints, simpler model de-
signs like SPLADEv2 or RocketQAv2 could lend
themselves to easier-to-optimize environments. We
leave low-level systems optimizations of all sys-
tems to future work. Another worthwhile di-
mension for future exploration of tradeoffs is re-
ranking architectures over various systems withcross-encoders, which are known to be expensive
yet precise due to their highly expressive capacity.
Research
","bias, misuse","misuse, bias","bias, misuse",2022,"statement, concerns"
372,"Social Impact of Dataset
The scope of this work is to provide an evalua-
tion framework along with extensive experiments
to further study fairness within the legal domain.
Following the work of Angwin et al. (2016), Dres-
sel and Farid (2018), and Wang et al. (2021b),
we provide a diverse benchmark covering multi-
ple tasks, jurisdictions, and protected (examined)
attributes. We conduct experiments based on pre-
trained transformer-based language models and
compare model performance across four represen-
tative group-robust algorithm, i.e., Adversarial Re-
moval (Elazar and Goldberg, 2018), Group DRO
(Sagawa et al., 2020), IRM (Arjovsky et al., 2020)
and REx (Krueger et al., 2020).
We believe that this work can inform and help
practitioners to build assisting technology for legal
professionals - with respect to the legal framework
(jurisdiction) they operate -; technology that does
not only rely on performance on majority groups,
but also considering minorities and the robustness
of the developed models across them. We believe
that this is an important application ﬁeld, where
more research should be conducted (Tsarapatsa-
nis and Aletras, 2021) in order to improve legal
services and democratize law, but more impor-
tantly highlight (inform the audience on) the vari-
ous multi-aspect shortcomings seeking a responsi-
ble and ethical (fair) deployment of technology.
Credit Attribution /Licensing
We standardize and put together four datasets:
ECtHR (Chalkidis et al., 2021), SCOTUS (Spaeth
et al., 2020), FSCS (Niklaus et al., 2021), and CAIL
(Xiao et al., 2018; Wang et al., 2021b) that are al-
ready publicly available under CC-BY-(NC-)SA-
4.0 licenses. We release the compiled version of
the dataset under a CC-BY-NC-SA-4.0 license to
favor academic research, and forbid to the best of
our ability potential commercial dual use.15All
datasets, except SCOTUS, are publicly available
and have been previously published. If datasets
or the papers where they were introduced in were
not compiled or written by ourselves, we have ref-
erenced the original work and encourage FairLex
users to do so as well. In fact, we believe that
this work should only be referenced, in addition
to citing the original work, when jointly experi-
15https://creativecommons.org/licenses/
by-nc-sa/4.0/menting with multiple FairLex datasets and using
the FairLex evaluation framework and infrastruc-
ture, or use any newly introduced annotations (EC-
tHR, SCOTUS). Otherwise only the original work
should be cited.
Personal Information
The data is in general partially anonymized in
accordance with the applicable national law. The
data is considered to be in the public sphere from
a privacy perspective. This is a very sensitive mat-
ter, as the courts try to keep a balance between
transparency (the public’s right to know) and pri-
vacy (respect for private and family life). ECtHR
cases are partially annonymized by the court. Its
data is processed and made public in accordance
with the European data protection laws. SCOTUS
cases may also contain personal information and
the data is processed and made available by the
US Supreme Court, whose proceedings are public.
While this ensures compliance with US law, it is
very likely that similarly to the ECtHR any process-
ing could be justiﬁed by either implied consent or
legitimate interest under European law. In FSCS,
the names of the parties have been redacted by the
courts according to the o ﬃcial guidelines. CAIL
cases are also partially anonymized by the courts
according to the courts’ policy. Its data is processed
and made public in accordance with Chinese Law.
",no_ethical_concerns,no_ethical_concerns,"privacy, bias, transparency",2022,statement
373,"The authors foresee no ethical concerns with the
work presented in this paper.
13See App. E for further experiments.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
374,"While DEM IXoffers new opportunities to reduce
the influence of unwanted training domains (e.g.,
those that contain hatespeech) at inference time,
shared parameters in the LM may prevent the
model from fully forgetting the unwanted domain
after expert removal. Therefore, DEM IXLMs may
still be prone to producing harmful generations
when deployed, and further research is required to
understand the bounds on the probability of toxic
degeneration after expert removal.
While we partially anonymize our corpus with
simple regexes, it is difficult to guarantee that sen-
sitive information is not exposed in large datasets.
To protect data authors and subjects, we do not
publicly release our models or data, although we
provide instructions and code to replicate them to
support reproducibility.
","harmful content, sensitive data","misuse, privacy",misuse,2022,concerns
375,"Due to the sensitivity of the mental health related
data, additional consideration needs to be taken
into account when accessing and analyzing such
data, as highlighted by Benton et al. (2017). All
datasets used in this research were obtained accord-
ing to each dataset’s respective data usage policy.
We did not interact with users in any way, and we
refrained from showing any direct excerpts of the
data in this manuscript to prevent risks from identi-
fying users’ pseudonyms. (All excerpts have been
paraphrased.) Similarly, we made no attempt to
identify, deanonymize, or link users to other social
media accounts. These precautions ensure we do
not draw attention to specific users who may be
suffering from depression.
All models proposed in this research were
trained on social media data. Thus, they are likely
to fail on data coming from other sources (e.g., clin-
ical notes), and there are no accuracy guarantees
even within social media data. Our models are not
intended to replace clinicians. Instead, we envision
the approaches we describe being used as assistive
tools by mental health professionals.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
376,"The usual approach to building a system for an AI
task is to design the task (e.g., for AER, identify
the precise emotion task to be automated, identify
the emotions of interest, etc.), compile appropriate
data (e.g., label some of the data), train ML model
(method) to capture relevant patterns of languagefrom the data, and evaluate the model by examining
their predictions on a held-out test set. There are
ethical considerations associated with each step of
this development process. Below is a template of
50 considerations grouped by the associated stage:
Task Design, Data, Method, Impact, Privacy & So-
cial Groups (this final category is particularly im-
portant and cuts across Task Design, Data, Method,
and Impact). I present only a high-level summary
for each category below. See Mohammad (2022)
for an instantiation of this generic template for the
task of automatic emotion recognition (AER). It in-
cludes details on how these considerations manifest
in AER. One can use the template below as a guide
(in part or full), skip the considerations that do not
apply, and describe how the relevant considerations
manifest for their chosen task. One should notably
include details of key considerations for their task
whether it is included in this template or not. One
can also cite specific issues already discussed in
the ethics sheets for other tasks.
TASK DESIGN
Summary: This section discusses various ethical
considerations associated with the choices involved
in the framing of the focus task and the implications
of automating the focus task. For AER, important
considerations included: whether it is even possible
to determine one’s internal mental state; whether
it is ethical to determine such a private state; and
who is often left out in the design of existing AER
systems. Mohammad (2022) also discusses how
it is important to consider which formulation of
emotions is appropriate for a specific task/project;
while avoiding careless endorsement of theories
that suggest a mapping of external appearances to
inner mental states.

Summary: This section has three broad themes: im-
plications of using datasets of different kinds, the
tension between human variability and machine
normativeness, and the ethical considerations re-
garding the people who have produced the data.
Notably, Mohammad (2022) discusses how on the
one hand is the tremendous variability in human
representation and expression of language and emo-
tions, and on the other hand, is the inherent bias
of modern machine learning approaches to ignore
variability. Thus, through their behaviour (e.g., by
recognizing some forms of emotion/language ex-
pression and not recognizing others), AI systems
convey to the user what is “normal""; implicitly
invalidating other forms of emotion/language ex-
pression.",no_ethical_concerns,no_ethical_concerns,bias,2022,statement
377,"Our comparison experiments and proposed formu-
lation are intended to encourage model sharing in
source-free domain adaptation while avoiding the
risk of privacy leakage caused by direct data shar-
ing. The data we use in this experiment are publicly
available and from a shared task, however some of
that data is from health institutions and requires a
data use agreement to work with the data. Though
recent research has found it difﬁcult to recover pro-
tected information from trained models (Lehman
et al., 2021), there is still some small risk that more
complex models may be able to do so. However,
as our research is a comparative study, we are not
directly releasing models, and thus not risking any
release of protected health information.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
378,"We used freely available data and a freely avail-
able pre-trained model for our experiments. Our
experimental setup required fine-tuning many large
language models, but we ran preliminary experi-
ments on a few languages to determine whether we
could achieve sufficient performance with a small
model size. As this indeed was the case, environ-
mental impact was limited compared to the larger
model size. Moreover, to limit the need for future
fine-tuning efforts for this task, we release all of
the fine-tuned models.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
379,"MetaDistil focuses on improving the performance
of knowledge distillation and does not introduce ex-
tra ethical concerns compared to vanilla KD meth-
ods. Nevertheless, we would like to point out that
as suggested by Hooker et al. (2020), model com-
pression may lead to biases. However, this is not
an outstanding problem of our method but a com-
mon risk in model compression, which needs to be
addressed in the future.
",bias,bias,bias,2022,"statement, concerns"
380,"We aim to accelerate scientiﬁc progress on robust
general question answering, which could translate
downstream to useful tools. We are not looking
at possible sources of social bias, although this is-
sue should be highly relevant to those considering
sources to use as training data for applied systems
(Li et al., 2020; Parrish et al., 2022). We are us-
ing Amazon Mechanical Turk despite its history of
sometimes treating workers unfairly (Kummerfeld,
2021), especially in recourse for unfair rejections.
We make sure that our own pay and rejection poli-
cies are comparable to in-person employment, but
acknowledge that our study could encourage others
to use Mechanical Turk, and that they might not be
so careful. This work passed review or is exempt
from the oversight of the internal review boards of
the authors’ institutes.
",bias,no_ethical_concerns,bias,2022,statement
381,"We conducted a qualification test to select workers
based on their performance. The workers were paid
a bonus of USD 0.10for taking the qualification
text. We paid USD 0.25for a batch of 10examples,
each batch taking 45-60seconds on average. This
amounts to USD 15−20/hour. We displayed a
warning on the task that said that the task might
contain potentially offensive language. We didn’t
collect any personal identifying information of the
workers other than their worker ID for assigning
qualifications. We restricted the workers location
to the USA with minimum of 5,000approved HITs
and98% HIT approval rate.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
382,"Now that we have described how the corpus was
created, we need to discuss ethical considerations
around the creation and use of such resources.
The process of creating language technologies for
any community of speakers should be guided by
the goals and interests of the respective commu-
nity. Natural language processing (NLP) research
should directly involve the language communities
for which the technologies are being designed, as
it will directly impact the speakers of the language.
Further, the process of constructing these technolo-
gies should be clear to the community so there is
an understanding of the data required for the model
and how it will be used. For example, communi-
ties may wish to see language technologies such as
text-to-speech to honor an oral tradition. However,
these systems require an underlying model trained
on corresponding audio and text for the language,
which may or may not be in accordance with a
community’s wishes.
In direct terms, the existence of this corpus initself is not an invitation to make Indigenous lan-
guage models and technologies independently and
without consultation. As discussed by Pine and
Turin (2017), successful Indigenous language re-
vitalization projects must be “grounded in local
understandings of impact and success, rooted in
the lived experiences and aspirations of Indigenous
communities.”
An important consideration when developing
language technologies using corpora and language
models is the nature of the language used to
train those models. For example, language mod-
els trained on Internet texts (e.g., GPT-3) have
been subject to scrutiny following the revelation
of racist and generally offensive outputs (Floridi
and Chiriatti, 2020). Those who use the devel-
oped nêhiyawêwin corpus should note the potential
for problematic outcomes when the data is used
to support certain types of language technologies.
This potential comes from the inclusion of bib-
lical texts. While biblical texts are widely used
for tasks such as machine translation (Mohler and
Mihalcea, 2008), they could advance the harmful
legacies of Christianity-related efforts and govern-
ment policies that used religion to control and harm
Indigenous groups (Bradford and Horton, 2016).
The translation of bibles into local Indigenous lan-
guages was a means of furthering colonization
(Pine and Turin, 2017). In addition, there are cer-
tain bible passages within our corpus that may be
considered violent or aggressive in nature, e.g.,
“May sinners be destroyed from the earth. . . may
the wicked be no more” (Psalm 1). This kind of
text, paired with the history between the church and
Indigenous peoples, should be used with caution,
especially when designing language technologies
that produce language (e.g., machine translation).
An additional important note is that, between
aligning texts and automatically extracting text
from varied sources, it is possible for there to be
mistakes or inconsistencies. This should be taken
into consideration when using the corpus. We also
welcome edits and contributions.
Beyond the above considerations, each of the
choices that we made during data cleaning has the
potential to have normative effects on the language.
Some may view norming and standardization as a
benefit (Mager et al., 2018; United Nations, 2019).
However, it also risks the loss of language vari-
ety that is often valued by community members.
Consequently, we include our data cleaning scripts
within the repository so that others may adapt them
and transform the data into the version of SRO or
syllabics that meets their needs.","offensive language, inconsistencies",misuse,misuse,2022,statement
383,"Our work empirically validated the long-tail theory
in the context of NLP, offering an alternative view
to the relationship between memorization and gen-
eralization. This will help NLP researchers see the
value of memorization. However, previous work
showed that neural networks can be vulnerable to
privacy attacks such as membership inference at-
tacks because these models are able to memorize
training instances, and sometimes sensitive private
information may be contained in the training in-
stances (Shokri et al., 2017; Zhang et al., 2017;
Feldman and Zhang, 2020). Thus, there is a trade-
off between the accuracy of a model and the privacy
of the data. In other words, although memorization
can help reduce generalization error (as we showed
in this paper), it also increases the vulnerability of
the system to privacy attacks, which raises ethical
concerns.
The computation of influence functions used in
our work is massive because of the computation ofinverting the hessian matrices. To reduce the com-
putation costs, i.e., power consumption, we may
adopt other influence estimators like TracIn (Pruthi
et al., 2020), which is hessian-free and thus faster.
Acknowledgment
This research is supported by the Singapore Min-
istry of Education (MOE) Academic Research
Fund (AcRF) Tier 1 grant.
",security,privacy,security,2022,"statement, concerns"
384,"This work focuses on quality checking and re-
annotations of DocRED, a publicly available
dataset constructed from Wikipedia pages. All
source documents and types of relationships are
provided and utilized in the original DocRED
dataset, and no additional annotation rule that may
involve unexamined ethical concerns was intro-
duced. Annotators receive a competitive pay of
100 yuan per hour (more than 4 times the local
minimum wage) under the approval of the institute,
and both the annotation and discussion stage count
towards the paid working time. Annotators are
required to read the ACM Code of Ethics before
annotating and report any document that violates
the code. These documents are removed from the
sampled documents. However, there may still be
sentences or entities that are from Wikipedia pages
with potentially improper content. The possible
adoption of such content is not a decision of the au-
thors, and all content in the dataset does not reflect
the views or stances of the authors. The resulting
re-annotations from the agreement of two expert
annotators form a decent approximation of the gold
labels, but may still not be the ground truth due
to natural error rates. Further use of the dataset
should be aware of the limitations and other possi-
ble issues, and we are not responsible for issues in
further model training processes using our data.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
385,"This paper has no particular ethic consideration.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
386,"This paper has no particular ethic consideration.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
387,"This paper has no particular ethic consideration.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement 
388,"Two of the three datasets employed in this work,
Regulation Room andChange My View , are
publicly available along with the annotation used
in our experiments. Europolis has been kindly pro-
vided to us and the splits will be relased at the terms
and conditions of its owners.
Our experiments and analysis do not employ
any author-speciﬁc feature. We cannot exclude,
however, that the reporting of very speciﬁc personal
experiences may facilitate the identiﬁcation of the
author of a post.
As far as the societal impact of the use of reports
of personal experiences and stories is concerned,
we are fully aware that they can be a two-edged
sword. On the one hand, as pointed out in the
paper, the use of personal experiences triggers em-
pathy and promotes inclusivity. On the other hand,
however, the non veriﬁable nature of such reports
makes them an easy vehicle to generate and spread
fake news, and this, coupled with the emotional
loading reports tend to have, makes them a useful
tool highlight certain episodes for propagandistic
purposes. Robust computational models to identify
human-generated reports of personal experiences
and a deeper understanding of the linguistic fea-
tures of these contributions can also serve as a ﬁrst
step into the identiﬁcation of false stories gener-
ated by automatic systems and of those human-
generated ones that are best candidates to serve
manipulation purposes.
",manipulation,"privacy, fake news, propaganda, manipulation",manipulation,2022,"statement, concerns"
389,"Content moderation is a critical application with
potential of significant benefits, but also harms to
human well-being. Therefore, ethics-related issues
in content moderation have been actively studied
in NLP and other disciplines (Vidgen et al., 2019;
Wiegand et al., 2019; Kiritchenko et al., 2021; Vid-
gen and Derczynski, 2020). These include sam-
pling and annotation biases in data collection, al-
gorithmic bias amplification, user privacy, system
safety and security, and human control of technol-
ogy, among others. Our work aims to address the
aspects of system safety and fairness by adapting
the model to newly emerged or not previously cov-
ered types of online abuse, often directed against
marginalized communities. We employ existing
datasets (with all their limitations) and use them
only for illustration purposes and preliminary eval-
uation of the proposed methodology. When de-
ploying the technology care should be taken to
adequately address other ethics-related issues.",bias,"sampling bias, annotation bias, algorithmic bias, privacy, safety, security","bias, misuse",2022,"statement, concerns"
390,"Transformer models that attend to long contexts,
to improve their generation quality, need large
amounts of computation and memory to perform
self-attention. In this paper, we propose an exten-
sion to a transformer model that makes the attention
complexity independent of the length of the con-
text being attended to. This can lead to a reduced
number of parameters needed to model the same
context, which can, consequently, lead to gains in
efﬁciency and reduce energy consumption.
On the other hand, the ∞-former, as well as the
other transformer language models, can be used on
questionable scenarios, such as the generation of
fake news (Zellers et al., 2019), defamatory text
(Wallace et al., 2019), or other undesired content.
",environmental_impact,"fake news, defamatory text",environmental_impact,2022,"statement, concerns"
391,"This research project uses open datasets and mod-
els, which are used in accordance with correspond-
ing licenses to the best of our knowledge. For the
downstream task in question (NER), we used the
MasakhaNER dataset, which is constructed from
newspaper data. Where this newspaper data in-
cludes mentions of individuals, the individuals are
public figures. The domain of this NER data is lim-
ited to the newspaper/news domain, which should
be kept in mind while considering the applicability
of the methods presented.
In terms of compute, the work presented here
required approximately 200 pre-training or fine-
tuning jobs tracked via ClearML. Each run lasted
no more than 1-2 hoursfor finetuning, but gener-
ally much longer for pretraining (on the order of a
day), and only consumed one GPU resource at a
time (either an A100 or P100). This computation
sums up to around 5-6 GPU-weeks on the A100,
about one gpu-week on the Titan RTX, and several
compute-days each for the other GPUs. Additional
exploratory work and debugging consumed another
few GPU-days on Google Colab.
",no ethical concerns,environmental impact,no ethical concerns,2022,statement
392,"As described in Section 4.3, the methodology used
in this paper has a number of limitations that affect
how these results may be generalized. Most promi-
nently, the stakeholder group that I interviewed was
small and represented only a small subsection of
the perspectives of Armenian speakers.
Additionally, the group of participants described
in this paper comprises speakers of only one
low-resourced language; speakers of other low-
resourced languages would likely have very differ-
ent needs and concerns. This case study is meant
only to provide examples of the concerns of speak-
ers of a particular low-resourced language. It is
important to avoid generalizing low-resourced lan-
guages and their speakers.
This paper does not cover all of the potential
harms of machine translation; further efforts are
needed to uncover other concerns for individual lan-
guage communities. If only the harms I described
in this paper were taken into consideration in the
development of a machine translation system, it
is certain that other important concerns would be
missed, which could cause substantial harms to
speaker populations.9 Conclusion
Using Value Scenarios, this paper illustrates some
potential harms that a general-purpose machine
translation system could have for speakers of a low-
resourced language. Avoiding these harms requires
direct collaboration with stakeholders before the
creation of a machine translation system intended
for low-resourced languages. To do so, machine
translation for low-resourced languages should be
undertaken as a language-specific task.
",no_ethical_concerns,exclusivity,misuse,2022,"statement, concerns, suggestions "
393,"The dialogue models we use in this work utilize
large language models, and therefore have similar
concerns as in other work, in particular concerns
about toxic language, bias and other issues dur-
ing language generation (Bender et al., 2021). For
open-domain dialogue in particular, see Xu et al.
(2020); Dinan et al. (2021) for reviews of the liter-
ature and evaluation of recent methods that try to
mitigate these safety issues.
Our work focuses on models with long-term
memory and open-domain conversations wherein
speakers may divulge personal interests. We re-
mark that, during data collection, crowdworkers
were specifically playing roles with given personal-
ity traits, not talking about themselves, and hence
not identifying any personal information. During
conversations with our trained models, the models
will store information they learn from the exchange.
In contrast to current standard language models,
our models have the capability of storing this in
the long-term. This information is stored in the
memory of the model, private to the individual’s
conversation, and hence is not shared with anyone
else.
","toxic language, bias","toxic language, bias","privacy, bias",2022,"statement, concerns"
394,"The datasets used in all experiments are derived
from previously published scientific papers, and
to our knowledge, there are no privacy or ethical
issues.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
395,"The annotators for CICERO were hired through
a data annotation service. The compensation was
derived based on the country of residence of the
annotators, as deemed by the company. The study
has been categorized as “exempt” by the IRB. An-
notators were strictly asked not to write any toxic
content (hateful or o ﬀensive toward any gender,
race, sex, religion). They were asked to consider
gender-neutral settings in dialogues whenever pos-
sible.
The source dialogue datasets – DailyDialog, Mu-
Tual, and DREAM are high quality multi-turn di-
alogue datasets manually annotated by experts in
dialogue, communication theory and linguistics.
All three datasets have been extensively used and
studied in the natural language processing literature.
The three source datasets and our annotations in
CICERO do not contain any personal data or any
information that can uniquely identify individual
people or groups.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
396,"Application. Our framework does not present di-
rect societal consequence and is expected to benefit
the defense against the fake news issue. It can
serve as a detection module for fake news detection
systems, especially when the given post is closely
related to the events that happened recently, with
no need to wait for the accumulation of user re-
sponses or query to knowledge sources. Due to
the requirement of real-time access to open news
sources (source list can be determined as needed),
it might be easier to deploy for service providers
(e.g., news platforms) and media outlets.
Data. Our data is mostly based on existing datasets,
except the news items for constructing news envi-
ronments. All news items (or headlines) are open
and accessible to readers and have no issues with
user privacy. The media outlets in the English
dataset might be considered “biased”, so we care-
fully select a left, a center, and a right outlet (whose
headlines are available) according to the AllSides
Media Bias Chart. In China, a media outlet might
be state-run (e.g., CCTV News), local-government-
run (e.g., The Paper), or business-run (e.g., Toutiao
News). With no widely recognized bias chart of
Chinese media as a reference, we select media out-
lets based on their influence (e.g., number of fol-
lowers) on Weibo from the three categories for the
sake of representativeness.",bias,bias,bias,2022,"statement, concerns"
397,"To study compositional generalization for data-to-
text tasks, we introduce data splits based on the
already existing, publicly available, and widely
used compositional weather dataset (Balakrishnan
et al., 2019). We release our data splits to facili-
tate the development of new methods and consis-
tent evaluation of them in comparison with exist-
ing works. In terms of use-case scenarios, we fo-
cus on task-oriented dialogue generation by using
large pre-trained language models. These models
are known to exhibit and potentially amplify so-
cial biases found in the training data, such as gen-
der biases (Dinan et al., 2020), and are capable of
generating toxic or otherwise unsafe content (Wei-
dinger et al., 2021). Our method helps these models
generate higher quality responses than considered
baselines when evaluated in terms of grammati-
cality, naturalness, informativeness, and accuracy.
However, our work does not explicitly focus on
mitigating social biases, unsafe content, or other
potential ethical or social harms that might result
from dialogue generation. Therefore, we caution
against the deployment of our system in environ-
ments where any such biases can negatively impact
the individuals interacting with our system without
further assessment of the safety of this system in
that environment.","bias, toxic content, unsafe content","bias, toxic content, unsafe content","bias, misuse",2022,"statement, concerns, suggestions"
398,"The focus of this work is text classiﬁcation. Poten-
tial risks that apply to text classiﬁcation in general
also apply to this work. Nonetheless, we present
alternatives to commonly used pretrained language
models, which suffer from various sources of bias
due to the large and poorly manageable data used
for pretraining (Bender et al., 2021). In contrast,
the presented alternatives render full control over
the training data and, thus, contribute to circumvent
the biases otherwise introduced during pretraining.
",bias,no_ethical_concerns,bias,2022,statement
399,"Intended Use The chief purpose of our dataset is
to examine a dialogue model’s capacity in selecting
proper knowledge with the help of personal mem-
ory. The dataset is mainly for research propose
and it is not supposed to be directly used to train
a production system. And researchers should be
aware of the possible ethic issues before exploiting
our dataset.
Data Collection All the examples in our dataset
are in English and no human annotators are in-
volved in the data collection process. As men-
tioned in Sec.4.1, our dataset is built on the basis
of the Reddit dumps from Pushshift (Baumgartner
et al., 2020), which is a publicly available resource
widely used in more than a hundred peer-reviewed
publications. Our data collection is in consistent
with the term of use and the research is granted
ethical approval by an external institutional review
board. To avoid potential abuse, the dataset is avail-able upon request to the authors. Contact the au-
thors (by email) and clearly state your intended use
if you believe the dataset might be helpful in your
research.
User Privacy Although our dataset includes user-
specific utterance history as personal memory, no
user account names will be revealed or inferred
from the dataset. Besides, the utterance histo-
ries are paraphrased during our procession of the
dataset such that they can not be traced back to the
real users in Reddit. In conclusion, There is no
personally identifiable information in our dataset
or underlying leakage of personal information.
",no_ethical_concerns,no_ethical_concerns,privacy,2022,statement
400,"In this paper we have introduced an annotated
dataset and models for detecting unassimilated bor-
rowings in Spanish. The dataset is openly-licensed,
and detailed annotation guidelines are provided
(Appendix B). Appendix C includes a data state-
ment that provides information regarding the cura-
tion rationale, annotator demographics, text char-
acteristics, etc. of the dataset we have presented.
We hope these resources will contribute to bringing
more attention to borrowing extraction, a task that
has been little explored in the ﬁeld of NLP but that
can be of great help to lexicographers and linguists
studying language change.
However, the resources we have presented
should not be considered a full depiction of either
the process of borrowing or the Spanish language
in general. We have identiﬁed four important con-
siderations that any future systems that build off
this research should be aware of.
The process of borrowing. Borrowing is a com-
plex phenomenon that can manifest at all linguistic
levels (phonological, morphological, lexical, syn-
tactic, semantic, pragmatic). This work is exclu-
sively concerned with lexical borrowings. Further-
more, in this work we have taken a synchronic
approach to borrowing: we deal with borrowings
that are considered as such in a given dialect and at
a given point in time. The process of borrowing as-
similation is a diachronic process, and the notion of
what is perceived as unassimilated can vary across
time and varieties. As a result, our dataset and
models may not be suitable to account for partially
assimilated borrowings or even for unassimilated
borrowings in a different time period.
Language variety. The dataset we have pre-
sented is exclusively composed of European Span-
ish journalistic texts. In addition, the guidelines
we have described were designed to capture a very
speciﬁc phenomena: unassimilated borrowings in
the Spanish press. In fact, the annotation guidelines
rely on sources such as Diccionario de la Lengua
Española , a lexicographic source whose Spain-
centric criteria has been previously pointed out
(Blanch, 1995; Fernández Gordillo, 2014). Con-
sequently, the scope of our work is restricted to
unassimilated borrowings in journalistic European
Spanish. Our dataset and models may not trans-
late adequately to other Spanish-speaking areas or
genres.The preeminence of written language. In our
work, the notion of what a borrowing is is heav-
ily inﬂuenced by how a word is written. Accord-
ing to our guidelines, a word like meeting will
be considered unassimilated, while the Spanish
form mitin will be considered assimilated. These
preferences in writing may indirectly reveal how
well-established a loanword is or how foreign it
is perceived by the speaker. But it is questionable
that these two forms necessarily represent a dif-
ference in pronunciation or linguistic status in the
speaker’s mental lexicon. How a word is written
can be helpful for the purpose of detecting novel an-
glicisms in written text, but ideally one would not
establish a deﬁnition of borrowing solely based on
lexicographic, corpus-derived or orthotypographic
cues. These are all valuable pieces of information,
but they only represent an indirect evidence of the
status that the word holds in the lexicon. After all,
speakers will identify a word as an anglicism (and
use it as such), regardless of whether the word is
written in a text or used in speech.
On the other hand, the lexicographic fact that
a word came from another language may not be
enough as a criterion to establish the notion of bor-
rowing. Speakers use words all the time without
necessarily knowing where they came from or how
long ago they were incorporated into the language.
The origin of the word may just be a piece of trivia
that is totally irrelevant or unknown to the speaker
at the time of speaking, so the etymological origin
of the word might not be enough to account for the
difference among borrowings. In fact, what lies
at the core of the unassimilated versus assimilated
distinction is the awareness of speakers when they
use a certain word (Poplack et al., 1988). The no-
tion of what a borrowing is lies within the brain of
the speaker, and in this work we are only indirectly
observing that status through written form. There-
fore our deﬁnition of borrowing and assimilation
cannot be regarded as perfect or universal.
Ideas about linguistic purity. The purpose of
this project is to analyze the usage of borrowings
in the Spanish press. This project does not seek to
promote or stigmatise the usage of borrowings, or
those who use them. The motivation behind our
research is not to defend an alleged linguistic purity,
but to study the phenomenon of lexical borrowing
from a descriptive and data-driven point of view.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
401,"It’s crucial for an open-domain dialogue system
to be able to automatically detect and discover the
underlying structural pattern of a sentence. With
the ability to handle a variety of expression styles,
whether positive or negative, serious or casual, our
work suggests that we are getting closer to the goal
of creating an artificial intelligent dialogue system
that can freely communicate with humans, which
is beyond the wildest dreams of most AI and NLP
researchers. However, a detailed survey should be
undertaken in advance to consider the immediate
audience’s and developers’ interests, as well as any
potential stakeholder groups.
Furthermore, knowledge-grounded dialogue sys-
tems have the potential to fabricate facts and dis-
tribute rumors and false information, particularly
when the source of external background knowl-
edge is unreliable. If the knowledge candidate set
is contaminated by fake news, the response gener-
ated by the dialogue system is likely to suffer from
the “hallucination” issue. Controlling the source
of knowledge sentences, such as paragraphs ex-
tracted from the wiki, authoritative news sites, or
authoritative product documents, is a necessary and
practical strategy.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
402,"While the contrastive visual semantic objective of
CLIP produces semantically rich representations of
natural language, we caution that the model is also
known to encode harmful societal biases. Goh et al.
(2021) find that the CLIP image encoder forms
representations which reflect biases against com-
munities marginalized based on religion and on
immigration status, and Wang et al. (2021a) and
Agarwal et al. (2021) report biases of underrepre-
sentation and stereotypical associations which dis-
proportionately affect women. Moreover, Radford
et al. (2021) state that they use frequency-based
heuristics to construct the WebImageText corpus
on which CLIP trains. Other research on language
models has shown that similar techniques can ex-
acerbate biases against marginalized groups, who
are often underrepresented in such datasets (Wolfe
and Caliskan, 2021). Thus, while our findings are
promising for the future of visual semantic AI sys-
tems, models like CLIP must be studied further to
understand how they represent people, and what
the ramifications of such representations are for
society.
",bias,bias,"bias, misuse",2022,statement
403,"The claims in this paper match the experimental re-
sults. This work focuses on DST in task-oriented di-
alogue systems, and the improvements could have
a positive impact on helping humans to complete
goals more effectively in a more intelligent way of
communication.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
404,"In this work, we used a binary deﬁnition of gender
while investigating gender bias in pre-trained lan-
guage models. While we fully recognize gender
as non-binary, our survey closely follows the origi-
nal methodology of the techniques explored in this
work. We believe it will be critical for future re-
search in gender bias to use a more ﬂuid deﬁnition
of gender and we are encouraged by early work in
this direction (Manzini et al., 2019; Dinan et al.,
2020b). Similarly, our work makes use of a narrow
deﬁnition of religious and racial bias.
We also note we do not investigate the extrinsic
harm caused by any of the studied pre-trained lan-
guage models, nor any potential reduction in harm
by making use of any of our studied debiasing
techniques. In other words, we do not investigate
how biases in pre-trained language models effect
humans in real-world settings.
Finally, we highlight that all of the intrinsic bias
benchmarks used in this work have only positive
predictive power. In other words, they can iden-
tify models as biased, but cannot verify a model
as unbiased. For example, a stereotype score of
50% on StereoSet or CrowS-Pairs is not indica-
tive of an unbiased model. Additionally, recent
work demonstrated the potential unreliability of the
bias benchmarks used in this work (Blodgett et al.,2021). Because of this, we caution readers from
making deﬁnitive claims about bias in pre-trained
language models based on these benchmarks alone.
","bias, unreliability","bias, unreliability","bias, misuse",2022,"statement, concerns"
405,"Our paper describes an enabling technology that
can expedite a dialogue system’s response for a
better user experience. It could also assist people
who have trouble interacting with the system by
reducing their effort in completing the query utter-
ance.
Caution must be taken when pre-executing pro-
gram calls before the user intent is fully revealed,
as there may be an unacceptable cost to mistak-
enly executing state-changing programs (for ex-
ample, sending emails or scheduling meetings)
without user conﬁrmation. In this work, we only
pre-execute “safe” function calls, which retrieve or
compute information without changing the state of
the environment.
Another concern, if training on real user data, is
leaking private information to other users. This
is especially pressing when predicting with in-
complete intent, as the model is encouraged to
hallucinate, and may hallucinate information that
it has memorized from other users’ data. For
PREFIX TOGRAPH , we use an explicit MASK token
for unrevealed future tokens, and force the model
to copy MASK to the predicted program instead
of freely generating text. We could easily com-
pletely remove the model’s ability to hallucinate
free text. LMC OMPLETE , on the other hand, can
and will leak text from the training data directly
into an utterance completion, which can then be
copied into a string literal in the predicted program.
Thus PREFIX TOGRAPH may be closer to suitable
for production use.
",hallucinations,hallucinations,privacy,2022,"statement, concerns"
406,"The nature of text generation leads to multiple ethi-
cal considerations when considering applications.
The main failure mode is that the model can learn
to mimic target properties in the training data that
are not desirable.
Faithfulness and Factuality Since models cre-
ate new text, there is the danger that they may nei-
ther be faithful to the source material nor factual.
This can be exacerbated when the data itself has
highly abstractive targets, which require the model
to generate words not seen in the source material
during training. This often leads the model to gen-
erate content inconsistent with the source mate-
rial (Maynez et al., 2020; Kryscinski et al., 2020;
Gabriel et al., 2021).
Trustworthy Data If the data itself is not trust-
worthy (comes from suspect or malicious sources)
the model will naturally become untrustworthy as
it will ultimately learn the language and topics of
the training data. For instance, if the training data
is about Obama birther conspiracies, and the model
is asked to generate information about the early life
of Obama, there is a risk that false claims will be
predicted by the model.
Bias in Data Similarly, biases in the data around
gender, race, etc., risk being propagated in the
model predictions, which is common for most NLP
tasks. This is especially true when the models are
trained from non-contemporary data that do not
represent current norms and practices (Blodgett
et al., 2020).
The above considerations are non-malicious, in
that the model is merely learning to behave as its
underlying source material. If users of such models
are not aware of these issues and do not account for
them, e.g., with better data selection and evaluation,
then the generated text can be damaging.
Generation models can also be misused in mali-
cious ways. These include generating fake news,
spam, and other text meant to mislead large sec-
tions of the general population.
","faithfulness, factuality, trustworthiness, bias, misuse","faithfulness, factuality, trustworthiness, bias, misuse","bias, misuse, manipulation",2022,"statement, concerns"
407,"Lexical substitution can be useful in various natural
language processing (NLP) tasks such as textual
data augmentation, paraphrase generation and text
simplification. The results that we present in this
paper suggest that contextual word embeddings
models, such as our framework (LexSubCon), can
be a valuable tool for providing accurate substitu-
tion candidates that can be further used in a variety
of down-stream tasks.
We believe that there are many benefits of us-
ing our contextual embeddings models. For exam-
ple, LexSubCon can be used as a data augmenta-
tion tool to provide artificial training data for tasks
where the lack of sufficient training data may hurt
the performance of the model. However, there are
potential risks of over-relying on any lexical sub-stitution tool. Particularly, a lexical substitution
model can unintentionally change the meaning of
the original text thus leading to erroneous conclu-
sions.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, concerns"
408,"In this work, we present a table pretraining method
leveraging spreadsheet formulas.
Dataset. Our pretraing corpus is built upon public
English spreadsheet files crawled from webs via the
search engine (Wang et al. , 2021b), covers various
domains, and has been checked by a compliance
team in a company to ensure that does not contain
sensitive names or uniquely identifies individual
people or offensive content. All datasets used for
evaluation are licensed public datasets, e.g., for
formula prediction, Enron (Hermans and Murphy-
Hill) is a public spreadsheet dataset consisting of
over17K spreadsheet files, and we re-purpose it for
formula prediction following (Chen et al. , 2021a).
Application. Our model shows its effectiveness in
three representative table-related tasks. Formula
prediction helps spreadsheet end-users to write for-
mulas which could be tedious and error-prone. Ta-ble QA enables users to query on the table without
the need of domain background knowledge. Cell
type classification assists interpreting fine-grained
table semantic structures, which help users to bet-
ter understand table structures and contents. There
may be risks that crooks use tabular models to au-
tomatically parse tables/forms to obtain private per-
sonal or company data in bulk, which should be
prevented.
",privacy,privacy,offensiveness,2022,"statement, concerns"
409,"The authors foresee no ethical concerns with the
research presented in this paper.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
410,"The advancement of deep neural network architec-
tures and the availability of large pre-trained lan-
guage models has led to significant improvements
for the multi-document summarization task, which
has applications in high-impact domains, particu-
larly in the medical one. Here, systematic literature
reviews play an essential role for the medical and
scientific community, and for that reason, they re-
quire strong guarantees about the factuality of the
output summary. Current state-of-the-art NLP solu-
tions cannot establish such assurance, so we do not
believe our solution, like previous ones, is ready to
be deployed. The research should explore more ef-
fective evaluation measures for text summarization
to make it happen, and large-scale accuracy guar-
antees by medical experts are still needed. Finally,
if the method will be applied to sensitive data such
as medical patient records, it should also include
privacy-preserving policies (da Silva et al., 2006).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
411,"We envision that our training strategy will broaden
the availability of task-oriented agents for tasks
and populations not currently covered by existing
large-scale datasets, due to its low annotation re-
quirement. We have open-sourced tool set designed
around our representation for bootstrapping afford-
able contextual semantic parsers for new domains.
Our agent was tuned and evaluated on the Multi-
WOZ benchmark. MultiWOZ is a crowdsourced
Wizard-of-Oz dataset; WOZ datasets are known not
to fully represent real-world conversations (Ganho-
tra et al., 2020). Further research is needed before
a dialogue agent based on our methodology can
be deployed in the real world. Additionally, the
current version of the agent was tuned for English;
future work should investigate techniques to au-
tomatically localize a contextual semantic parser,
analogous to prior research done for single-turn
semantic parsers (Moradshahi et al., 2020).
Our training strategy replaces manual annotation
of data with automatically obtained data, which re-
quires some additional amount of computation time.
In practice, such additional compute is small: data
synthesis runs in 5 hours on a single machine with
no GPUs; the paraphrase dataset can be obtained
in about 5 hours on a machine with 4 Nvidia T4
GPUs; training completes within 8 hours on a ma-
chine with one Nvidia V100; self-training requires
2 hours on a single Nvidia T4 GPU, and ﬁne-tuning
is another 1.5 hours on one Nvidia V100. Overall,
the whole process is done with about 22 hours of
compute time, well below the cost of human an-
notation of equivalent amounts of data. We note
that the large amount of synthetic data poses no
challenge to convergence in practice, so training
models with a large amount of synthesized data has
little effect on the compute cost.
The manually annotated portion of our dataset
was obtained from the previously released Multi-
WOZ 2.1 dataset, a crowdsourced dataset. No
crowdsourcing was employed in this paper; the
data was annotated by the authors.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, suggestions, concerns"
412,"In this paper, we report on the creation of a new
dataset for readability assessment. The data col-
lection process did not involve any human par-
ticipants. So, no ethics board approval was nec-
essary. Both the websites are available under
permissive licenses that allow sharing and redis-
tributing. The released dataset will follow the
same procedures and is freely available at https:
//zenodo.org/record/6327828 . An im-
portant point to note in the use of the dataset is that
the length of texts is much shorter in the ""simple""
versions compared to regular Wikipedia articles,
which may affect the quality of results in some use
cases of the dataset.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
413,"This paper is concerned with possible approaches
to domain adaptation, especially for situations
where training data cannot be shared, such as for
reasons of privacy or copyright. However, it is
important to note that domain adaptation will be
most effective when model producers are able to
make their training data publicly available, and we
strongly encourage all researchers to do so, where
possible, along with following other best practices
for open and reproducible science.
Although we found signiﬁcant improvements on
out-of-domain data in multiple domains, we only
evaluated these techniques on text classiﬁcation
tasks here, and they should therefore be applied
with caution. As emphasized throughout the paper,
validation is important, especially when using text
classiﬁcation as a form of measurement, and any
inferences based on such measurements should be
properly contextualized when reporting ﬁndings.
Our experiments are all based on pre-established
datasets, which do not pose any serious ethical con-
cerns. We also facilitate replication of our results
by making code available.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
414,"Annotation was completed by two non-algorithm
developers (one with expertise in linguistics, the
other with expertise in psychosocial moral indica-
tors), compensated appropriately for their work. A
two-level institutional review board (at both the
sponsored site and at the sponsoring site) deemed
this work as “research not involving human sub-
jects,” as it does not involve a living individual
about whom an investigator conducting research
obtains information through intervention or inter-
action with the individual, or obtains, uses, studies,
analyzes, or generates identifiable private informa-
tion.
The data used for the software development
is provided from Kaggle’s existing public
research Twitter dataset, focusing on an En-
glish subset for the 2017 French Elections
of 1.9 million tweets (Daignan, 2017) at
https://www.kaggle.com/jeanmidef/
french-presidential-election . Kag-
gle data contain user names, but the dataset is open
and publicly available.
Potential risks may emerge from language bi-
ases in standard resources on which some of our
work is built. For example, cultural idioms like
fluctuat nec mergitur may be translated from Latin
into the correct literal meaning of tossed [by the
waves], but not sunk , but the culturally distinct
values (Paris’ coat of arms and motto with a deep
affective history) will get lost in translations into
English, and with it, its cultural meaning. Similarly,
while English WordNet provides one of the most
comprehensive semantic ontology of words in En-
glish, embedded biases are still present today, e.g.,
offensive, racist, and misogynistic slurs (Crawford
and Paglen, 2021). These issues need to be ad-
dressed within the language resource community.
Risk of misuse of this technology is mitigated
by the transparent nature of concern detection, ow-
ing to the propositional representations that under-
lie and inform algorithmic decisions. In contrast
to ML approaches, misuse within the technology
would be easily discoverable. The technology fur-
ther serves as a framework within which cultural
distinctions may be studied and better understood,
thus mitigating the potential for cross-culturally
undetected misuse.",bias,"bias, misuse","bias, misuse",2022,"statement, concerns, suggestions"
415,"Although tackling online hatred through CNs in-
herently protects the freedom of speech and has
been proposed as a better alternative to the detect-
remove-ban approaches, automatization of CN gen-
eration can still raise some ethical concerns and
some measures must be taken to avoid undesired
effects during research. Thus, we address the rel-
evant ethical considerations and our remedies as
follows:
Annotation Guidelines: The well-being of the
annotators was our top priority during the whole
study. Therefore, we strictly followed the guide-
lines created for CN studies (Fanton et al., 2021)
that were adapted from (Vidgen et al., 2019). The
human evaluations have been conducted with the
help of two expert annotators in CNs. These ex-
perts were already trained for the CN generation
task and employed for the work presented by (Fan-
ton et al., 2021). We further instructed them in the
aims of each experiment, clearly explained the eval-
uation tasks, and then we exempliﬁed proper eval-
uation of <HS, CN> pairs using various types
of CNs. Most importantly, we limited the expo-
sure to hateful content by providing a daily time
limit of annotation. Concerning the demographics,
due to the harmful content that can be found in the
data, all annotators were adult volunteers, perfectly
aware of the objective of the study.Dataset. We purposefully chose an expert-based
dataset in order to avoid the risk of modeling the
language of real individuals to (i) prevent any pri-
vacy issue, (ii) avoid to model inappropriate CNs
(e.g. containing abusive language) that could be
produced by non-experts. The dataset also focuses
on the CN diversity while keeping the HSs as
stereotypical as possible so that our CN genera-
tion models have a very limited diversity on the
hateful language, nearly precluding the misuse.
Computational Task. CN generation models are
not meant to be used in an autonomous way, since
even the best models can still produce substandard
CNs containing inappropriate or negative language.
Instead, following a Human–computer cooperation
paradigm, our focus is on building models that can
be helpful to NGO operators by providing them di-
verse and novel CN candidates for their hate coun-
tering activities and speed up the manual CN writ-
ing to a certain extent. This approach also gives
ground to some of the measures we used during
evaluation (namely choose-or-not and is-best).
Model Distribution. In addition to the limited
and simpliﬁed hateful content in the dataset we
selected, we further reduce the risk of misuse by
choosing a speciﬁc distribution strategy: i) we only
make available the non-autoregressive models in
order to eliminate the risk of using over-generation
for hate speech creation, ii) we distribute such mod-
els only for research purposes and through a re-
quest based procedure in order to keep track of the
possible users.
","privacy, inappropriate language","privacy, inappropriate language","misuse, offensiveness",2022,"statement, concerns, suggestions"
416,"As mentioned, we collected our data from IWSLT
and WMT that all are public to academic use, and
they contain no sensitive information. The legal
advisor of our institute confirms that the sources of
our data are freely accessible online without copy-
right constraint to academic use. Our human exper-
iments (Section 7 and Section 8) involves manual
annotation. Annotators were asked to give prompts,
post-edit machine translation and evalaute transa-
tions, which do not involve any personal sensitive
information. We hired 4 annotators who have de-
grees in English Linguistics or Applied Linguistics.
Before formal annotation, annotators were asked
to annotate a few samples randomly extracted from
the dataset, and based on average annotation time
we set a fair salary (i.e., 30 dollars per hour) for
them. During their training annotation process,
they were paid as well.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
417,"Data collection and Annotation. We made sure
that the sources we use to collect stories do not
prevent any kind of copyright infringement. The
content distribution licenses were checked before
any use. Additionally, we manually examined the
stories and the created questions to ensure there are
no privacy or ethical concerns, e.g., toxic language,
hate speech, or any bias against underrepresented
groups. EyeRead has outreach programs in place
to recruit writers from diverse populations, incor-
porate their writing into the online system, and
properly compensate them for their work. Writers
that created questions earned comparable hourly
wages to those earned by salaried teachers in a
summer program. We estimated the amount of
time AMT workers need to finish a HIT and then
we compensated them so that the payment rate was
higher than the local living wage per hour. Each
AMT worker received $0.41 USD for completingone HIT, which we estimated would take 1 minute.
Bias in Language Models. Recently, many re-
search works found that language models have sev-
eral types of bias, e.g. gender, race, religion, etc.,
and this is due to the data used to train them (Liang
et al., 2021). Removing bias from language models
completely is difficult, if not impossible (Gonen
and Goldberg, 2019). Thus, here we acknowledge
that the QG model we trained might cause ethical
concerns, e.g. generating biased questions about
stories’ characters. EyeRead is keenly aware of this,
and continues to monitor both teacher and model-
generated questions before they are integrated into
their system.
",bias,bias,bias,2022,"statement, actions"
418,"The experiments reported in this work rely on pre-
viously published datasets described in Section 2.4.
We used the CAMeLBERT models along with mor-
phosyntactically annotated datasets to build our
morphosyntactic taggers, which is in line with their
intended use. Our work is on core and generic NLP
technologies that can be potentially used with mali-
cious intent, for example, as part of the pipeline. To
ensure reproducibility, we make our code publicly
available. The details on the datasets and train-
ing are described in Appendix A. Given the focus
of this paper and the available resources, we rec-
ognize the limitations of our findings in terms of
applicability to different genres, styles, and other
languages.
Acknowledgment
This work was carried out on the High Performance
Computing resources at New York University Abu
Dhabi. We thank anonymous reviewers for their
insightful suggestions and comments. We thankBashar Alhafni and Ossama Obeid for their assis-
tance with the codebase and the helpful discus-
sions.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
419,"In recent years, deep learning approaches have
been the main models of choice for practical ma-
chine reading systems. However, these systems
are often overconﬁdent in their predictions. A cal-
ibrated conﬁdence score would help system users
better understand the system’s decision making.
Our work introduces a simple and general way for
calibrating these systems. While our models are
not tuned for any speciﬁc application domain, our
methods could be used in sensitive contexts such
as legal or healthcare settings, and it is also es-
sential that any work using our method undertake
additional quality assurance and robustness testing
before using it in their setting. The datasets used in
our work do not contain any sensitive information
to the best of our knowledge.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
420,"The datasets introduced in this paper for machine
readable training and evaluations are extracted
from previous specialized linguistic work. We stick
to the ethical standards giving credit to the original
author in the spirit of fair scientific usage . We fur-
ther strongly encourage future work that use these
resources to cite also the original sources of the
data. Additionally we found another ethical risks
of this work: for the down-stream task of MT, a
translation system should not be deployed with low
quality translations, as it can mislead the user, and
have implicit biases. Finally, want to state that the
authors of this paper have a long record of working
with the studied indigenous languages. Some have
conducted field studies with the communities in
the past, and Manuel Mager is part of the Wixarika
community. This allows the authors to have a better
understanding of the concerns of the communities
that speak the discussed languages.
",bias,bias,bias,2022,statement
421,"In this work, we have focused on domain special-
ization of task-oriented dialog models. While the
limited scope of this manuscript prevents us from
an in-depth discussion of potential ethical issues
related to conversational AI in general, we would
like to highlight the sensitivity of many of the en-
visioned future applications and the corresponding
potential harms, e.g., unfair stereotypical biases
encoded in pretrained language models, both gen-
eral purpose (Nadeem et al., 2021; Lauscher et al.,
2021) and conversational PLMs (Barikeri et al.,
2021) alike and exclusion and misrepresentation
within the large spectrum of users’ (gender) identi-
ties (Dev et al., 2021; Lauscher et al., 2022), which
might arise depending on the sociotechnical de-
ployment environment. Further, in light of poten-
tial environmental harms arising from training large
PLMs (Strubell et al., 2019), we would like to point
the reader to the computational efficiency of our
adapter-based specialization approaches, specifi-
cally in a multi-domain setup.
","bias, exclusion, misrepresentation","bias, exclusion, misrepresentation","bias, misuse",2022,"statement, concerns"
422,"We introduce a new approach for controllable text
generation by extracting vectors from a pretrained
language model, leveraging information that is al-
ready encoded in the language model. Large pre-
trained models are known to be biased and our
method of extracting steering vectors can reflect
biases already present in these large pretrained lan-
guage models (Bender et al., 2021). The methods
we present for controllable text generation could po-
tentially be used for many downstream tasks such
as unsupervised style transfer, abstractive summa-
rization, and offensive content removal. Unfortu-
nately, this also means that this technology has
the potential to be misused to perpetuate biases or
generate offensive or toxic text.
Our technology does not guarantee removal of
toxic content, even in the case of unsupervised style
transfer from toxic to nontoxic text. To use this
method, we encourage readers to first take steps to
address biases that are already present in the un-
derlying language model. Further we recommend
that this technology not be used in high-stakes set-
tings, especially those where deployment of this
technology could cause harm.
","misuse, bias, offensive language","misuse, bias, offensive language","bias, misuse, offensiveness",2022,"statement, concerns"
423,"We honor and support the ACL Code of Ethics. We
have used only the publicly available news articles
from the CNN website and adhere to their only-for-
research-purpose guideline. Meanwhile, to make
sure the downstream usage of the data will not
break the permission of CNN website, we only
release the URLs of these articles along with a
script to download and process them.
The content of the news and summaries only re-
flect the views of the media, and should be viewed
with discretion.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
424,"Ethical Treatment of Annotators Annotators
were a part of the study team and were not ad-
ditionally compensated for their annotations.
Political Afﬁliation Classiﬁcation The models
in 5 make inferences about the political afﬁliations
of users. Given the increasing importance of po-
litical identity in American society (Finkel et al.,
2020) and inter-party hostility (Miller and Conover,
2015), these models could come with some risk if
a user is mislabeled with an afﬁliation they do not
have, e.g., a public mislabeled political identity
could cause a user to be socially ostracized for
their supposed political beliefs. However, as we
demonstrate, these inference models offer moder-
ate performance at best and are not likely to be
reliable in practice. As a result, we hope our mod-
els discourage future use of such inference on Red-
dit, mitigating the potential risk. Further, our work
aims to highlight the issue of inaccurately labeled
and biased datasets in computational social science
research, which are often inequitably felt in down-
stream harms (Olteanu et al., 2019; Mehrabi et al.,
2021). Our results show that self-reported political
identity is highly noisy and, when used to train
classiﬁers, likely misrepresents any classiﬁed pop-
ulation used due to a high model error rate.
Ethical Risks in Behavioral Analyses Our
study includes multiple behavioral analyses that
look at how political users engage with each other.
These studies have focused on broad characteri-
zations of populations, rather than individuals, to
maximize privacy. Yet the relatively simple meth-
ods in these studies could still be considered as
dual-use with risks for users. For example, the
relatively simple methods could be used to target
certain users, e.g., identifying users who are po-
tentially open to changing party afﬁliation (§B.3).
Given that political targeting is wide-spread in prac-
tice by a variety of groups (e.g., Speicher et al.,
2018; Ribeiro et al., 2019), we view the additional
risk caused by our study as being minimal; how-ever, we do acknowledge that our study could con-
tribute to an additional focus on Reddit users.
As another risk, our study identiﬁes two-faced
users which could prompt those users themselves
to change their behavior to avoid detection (e.g.,
making more subtle indications of their politics).
We view this risk as being out-weighed by show-
ing these users are (or were) active on the platform
and potentially highlighting their behaviors for plat-
forms to examine more closely.
Data Collection and Privacy Our data collec-
tion is in compliance with Reddit’s terms of service
and matches previous publications. In accordance
with Reddit’s content policy, any rule violation
such as hate speech leads to the deletion of the
comment. If the moderators of a community fail
to comply with the content policy, or violations are
running rampant, the subreddit will be banned. The
deletion of hateful comments ensures our data does
not contain any banned content. No identifying
user characteristics are used in the paper, which
minimizes privacy risk. Although Reddit data is
public, in releasing our data, we share comment
IDs rather than raw data. This format allows users
to delete their data, while still other researchers to
retrieve comments (if they are public).
","bias, misuse","bias, misuse","bias, misuse",2022,statement
425,"IRB Approval. Prior to collection of the IN-
SPIRED dataset, we obtain IRB (Institutional Re-
view Board) approval at our institution. This data
collection is considered Exempt Research, mean-
ing that our human subjects are presented with no
greater than minimal risk by their participation.
Participants’ personal information is not collected,
aside from minimal demographic information in-
cluding their native language, which is used to en-
sure native-speaker level proficiency in the dataset.
No identifying information is included. Further, all
participants are required to read and agree to an
informed consent form before proceeding with the
task. AMT automatically anonymizes crowdwork-
ers’ identities as well.
Compensation to Crowdworkers. In order to en-
sure both quality data collection and fair treatment
of our crowdworkers, we carefully review our pay-
ment plan for the AMT task. After a pilot study
we gauge the average amount of time we expect
a task to require and adjust the payment amount
per task according to the minimum wage amount
in our state, resulting in a 70 cent payment per task.
Further, we ensure compensation for the time spent
on the tutorial and qualification task by awarding
$10 bonuses after completion of their first 10 tasks.
They also receive $10 bonuses upon every 100
tasks they complete. In total, the cost of creating
the INSPIRED dataset is approximately $13,300.",no_ethical_concerns,no_ethical_concerns,no ethical concerns,2022,statement
426,"The HYPO-XL dataset is collected from a public
website Sentencedict.com, and we have asked the
website owners permission to use their data for
research purposes. There is no explicit detail that
leaks a user’s personal information including name,
health, racial or ethnic origin, religious affiliation
or beliefs, sexual orientation, etc.
Our proposed method MOVER utilizes the pre-
trained language model, which may inherit the bias
in the massive training data. It is possible that
MOVER is used for malicious purposes, since it
does not explicitly filter input sentences with tox-
icity, bias or offensiveness. Therefore, the output
generated by MOVER could potentially be harm-
ful to certain groups or individuals. It is important
that interested parties carefully address those biases
before applying the model to real-world situations.
","bias, misuse, harmful output",harmful content,"privacy, bias, misuse",2022,"statement, concerns"
427,"We use the PushShift API to collect data from
Reddit.5Our collection process is consistent with
Reddit’s Terms of Service. The data are accessed
through the data dumps on Google’s BigQuery us-
ing Python.6
Reddit can be considered a public space for dis-
cussion which differs from a private messaging
service (Vidgen et al., 2021). Users consent to
have their data made available to third parties in-
cluding academics when they sign up to Reddit.
Existing ethical guidelines state that in this situa-
tion explicit consent is not required from each user
(Procter et al., 2019). We obfuscate user names as
User_A or User_B to reduce the possibility of iden-
tifying users. In compliance with Reddit’s policy,
we would like to make sure that our dataset will be
reused for non-commercial research only.7
The Reddit comments in this dataset were an-
notated by annotators using Amazon Mechanical
Turk. We have followed all requirements intro-
duced by the platform for tasks containing adult
content. A warning was added in the task title. An-
notators need to pass the Adult Content Qualiﬁca-
tion Test before working on our tasks. Annotators
were compensated on average with $8 per hour.
We paid them regardless of whether we accepted
their work. Annotators’ IDs are not included in the
dataset.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
428,"Whether recruiting annotators through Amazon
Mechanical Turk or campus, we paid them 15 dol-
lars per hour, more than the local average mini-
mum wage. We removed all content in the dataset
that might contain personal information about the
annotators.
", no_ethical_concerns,no_ethical_concerns,privacy,2022,statement
429,"Data Privacy and Bias: This research mainly fo-
cuses on translation using the Europarl (Koehn,
2005) corpus, which is widely adopted in the com-
munity. There are no data privacy issues or bias
against certain demographics with regard to this
dataset.
Potential Use: The potential use of this study is
to improve future MT practice in terms of both
evaluation and training.
Generalizability: Most conclusions in this study
are language-agnostic and potentially help MT in
all language pairs, although due to the limitations
of available data, the study mainly uses the com-
mon languages, English, German, French, and
Spanish, in a relatively low-resource setting of
around 100K to 200K data. It will be meaning-
ful to explore whether the conclusions of this paper
can generalize to higher-resource data and a wider
variety of languages. There is a possibility that
the ﬁndings of the study will need to be further
adjusted in different settings, which we strongly
encourage future work to explore.",no_ethical_concerns,no_ethical_concerns,"privacy, bias",2022,"statement, suggestions"
430,"Constructs like sexism and hate speech detection
are often depicted as neutral or objective, but
they are deeply contextual, subjective, and am-
biguous (Vidgen et al., 2019; Jurgens et al., 2019;
Nakov et al., 2021). Promoting features like iden-
tity terms can increase the risk of misclassifying
non-hateful content with such terms, such as dis-
closures or reports of facing hate speech, leading to
unintended bias (Dixon et al., 2018) that can cause
harm (Blackwell et al., 2017). Following up on
this subjectivity and based on recommendations by
Blodgett et al. (2020), we motivate our analyses
of unintended bias on normative grounds, situated
in the context of the harms wrought by misclassiﬁ-
cation of content containing identity terms despite
being non-sexist or non-hateful. We acknowledge
that we only study one type of unintended bias and
there are other aspects that require further investi-
gation (Blackwell et al., 2017).
",no_ethical_concerns,no_ethical_concerns,"bias, misuse",2022,statement
431,"We are aware of the discussions around the risks
related to unintended harmful effects and uses, en-
vironmental consequences, fairness and privacy
considerations of large language models in gen-
eral (Bender et al., 2021), and machine translation
models speciﬁcally (Wang et al., 2021). We note
here that our models constitute a primarily theo-
retical contribution and were trained and tested
on standard datasets. Before deployment in a pro-
duction setting our methodology is subject to re-
training with data pre-processed in the appropriate
way (as our model is not equipped with relevant
security, privacy and fairness mechanisms), system-
atic debugging, extensive simulation, testing and
validation under the supervision of experts.
","environmental consequenses, fairness, privacy","harmful effects, environmental impact, fairness, privacy","bias, misuse",2022,"statement, concerns, suggestions"
432,"In this work, we have presented MULTI2WOZ , a
robust multilingual multi-domain TOD dataset, and
focused on the multilingual conversational special-
ization of pretrained language models. Although
the scope of this manuscript does not allow for
an in-depth discussion of the potential ethical is-
sues associated with conversational artificial intel-
ligence in general, we would still like to highlight
the ethical sensitivity of this area of NLP research
and emphasize some of the potential harms of con-
versational AI applications, which propagate to
our work. For instance, issues may arise from un-
fair stereotypical biases encoded in general pur-
pose (Lauscher et al., 2021) as well as in conver-
sational (Barikeri et al., 2021) pretrained language
models and from exclusion of the larger spectrum
of (gender) identities (Lauscher et al., 2022). Fur-
thermore, (pre)training as well as fine-tuning of
large-scale PrLMs can be hazardous to the environ-
ment (Strubell et al., 2019): in this context, the task-
agnostic intermediate conversational specialization
for the target languages that we introduce, which
allows for highly sample-efficient fine-tuning for
various TOD tasks can be seen as a step in the pos-
itive direction, towards the reduction of the carbon
footprint of neural dialog models.
","bias, environmental impact",bias,"bias, misuse",2022,"statement, concerns"
433,"In this work, we used the ECB+ corpus (Cybulska
and V ossen, 2014) which consists of news articles
from the open domain. Our use was consistent with
the intended use of the dataset. Our model does
not contain any intentional biases. As discussed
in §3.4, §4.4, we ran our experiments on a single
p3dn.24xlarge with 8 V100 32GB GPUs. Model
training and inference was relatively short and does
not present ethical issues.
",no_ethical_concerns,no_ethical_concerns,bias,2022,statement
434,"The annotation task described in Section 4.3 was
carried out by two researchers regularly employed
at Fondazione Bruno Kessler as part of their work.
Overall, we do not foresee any speciﬁc ethical
concern related to this work. On the contrary, our
goal is to propose artifacts statement as a desir-
able practice for documenting potential biases in
newly released datasets, and improve current debi-
asing methods by distinguishing among different
types of lexical artifacts. However, the (ﬁnite set
of) identity-related and offensive tokens consid-
ered in this work are all in English and centered
around Western cultural context. We leave the eval-
uation of our methodology to assess whether there
are language- or more broadly culture-dependent
changes for future work, following recent work on
biases in geo-cultural contexts (Ghosh et al., 2021).
",bias,English-centric,bias,2022,statement
435,"Our work in understanding the role of maximizing
the similarity between question and evidence pairs.
Therefore, there is a limited risk associated with
the quality of annotated evidence sentences in the
dataset, as there is no guarantee that our model
will always generate non-biased and factual con-
tent. Therefore, caution must be exercised when the
model is deployed in practical settings, where the
evidence quality is vague and cannot be veriﬁed.
",bias,bias,bias,2022,"statement, concerns"
436,"Data Usage. To minimize real-world harm, we
choose WikiText-2 since it is already public and
widely used, and synthesize the dialogs as well
as the personal information in the CUSTOMER SIM
datasets in our experiments. For future research, we
plan to continue using public or synthetic datasets
to prevent real-world data leakage.
Application. Our work addresses the problem of
data privacy protection and can be applied in dif-
ferent applications. The attacks used in our study
are well-known standard attacks tailored for our
specific tasks, so it’s hard to generalize and misuse
them to attack other language models. We will re-
lease the code so that people can have access to the
various algorithms and protect their own data.
A.2
",no_ethical_concerns,no_ethical_concerns,"privacy, misuse, security",2022,"statement, actions"
437,"To the best of our knowledge, the datasets used
in our work do not contain sensitive information,
and we foresee no further ethical concerns with the
work.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
438,"Dataset Biases The underlying models of the
metrics presented in this work are trained on doc-
uments in English and thus mainly represent the
culture of the English-speaking populace. Politi-
cal or gender biases may also exist in the datasets,
and models, and subsequently the metrics, trained
on these datasets may propagate these biases. We
did not stress test these metrics for such biases and
request that the users of these metrics be aware of
these potential issues in applying them.
Misuse Potential and Failure Mode When prop-
erly used, the metrics described in this paper can
be a useful tool for detecting summarization model
errors. However, the current metrics fail to detect
all factual inconsistencies, which must be remem-
bered when applying these metrics as a filter for
downstream applications. Factual inconsistencies
in summaries could contribute to misinformation
on the internet.
Environmental Cost The experiments described
in the paper primarily make use of A100 GPUs.
Most of the metrics have already been trained, in
which case we simply ran inference using the ex-
isting models. We typically used a single GPU
per experiment. Training learned answer overlap
components can take a couple of hours, while ex-
periments for learned metrics on SummaC takeless than 10 minutes. These are the base mod-
els used in these experiments, with the number
of parameters, in millions, in parentheses: BERT-
base (110), BART-large (400), Electra-base (110),
Electra-large (335), RoBERTa-large (355), T5-base
(220), T5-large (770). Future work may analyze
the effect of using distilled backbone models on
factual consistency evaluation.
","bias, misinformation","factual inconsistencies, bias","bias, misuse, misinformation",2022,"statement, concerns"
439,"In this work, we present a method for automatically
generating offensive spans using the pre-trained
generative language model GPT-2. While the sole
purpose of the proposed method is to enhance the
performance of the offensive content detection sys-
tems in social networks, such a generative model
can also be misused by someone to automatically
make offensive posting continuously without much
effort. Prior to our discussion on our measures to
mitigate the potential harms of this research, we
first justify the risk of this harm. First, as shown
in the experiments, employing generation-based
models can improve the offensive span detection
performance by exposing the model to more di-
verse patterns of offensive content. Second and
more importantly, automatically generating train-
ing data for this task reduces the need to expose
annotators to a large amount of offensive content.
More specifically, since the GPT-2 generated data
is effective for training an OSD model, less offen-
sive content is needed to be annotated by human.
Thereby, the risk of harmful effects on the annota-
tors is decreased. However, as mentioned before,
there is still room for misuse of the findings of this
research to automatically generate offensive con-
tent. As such, to mitigate the potential harms of
this method, we take extra measures into account.
In particular, first, we don’t release the fine-tuned
GPT-2 model on the offensive data, therefore, no
one can directly use the artifacts of this research
for harmful purposes. Second, since this research
demonstrates the potential of the GPT-2 for generat-
ing natural-looking offensive content, in return, we
also study the effectiveness of a defensive method
in which a classifier is employed to identify con-
tents generated by GPT-2 from contents posted by a
human. More specifically, we train a BERT model
on a dataset consisting of 7,939 human-generated
and the same number, i.e., 7,939, automaticallygenerated offensive posts2. The input content, i.e.
[CLS ]w1w2. . . w n[SEP ]where wiis the i−th
word of the post, is encoded using the BERT base
model. The representation of the [CLS ]vector ob-
tained from the final layer of the BERT baseis sent
to a binary classifier3to identify human-generated
and automatically generated texts. We evaluate the
performance of the trained binary classifier on a
test set of 4,000 offensive posts, with a ratio of
50% human-generated content. The accuracy of
the classifier on the test set is 92.7% (note that a
random baseline would have an accuracy of 50%).
Given the simplicity and the high performance of
the classifier to recognize the automatically gen-
erated posts, we expect that one can directly use
this defensive model to automatically and quickly
identify the model-generated offensive contents in
social networks, thereby mitigating the potential
harms of the findings of this research. Also, in
future work, with a more comprehensive classifier,
better defensive performance is expected. One po-
tential improvement is to incorporate the context of
the postings. In particular, while this work shows
that GPT-generated content is helpful to improve
OSD performance, it does not show the degree to
which the generated offensive content is related to
the context of the posting. Finally, although this re-
search is conducted on a publicly available dataset
of offensive content, in order to prevent disclos-
ing the identity of people mentioned in the dataset,
both in the training of the GAOSD and GPT-2 mod-
els, we hire 5 undergrad students to double-check
and anonymize the SemEval 2021 Task 5 dataset.
We expect by anonymizing the data, fewer human
subjects can be targeted by automatically generated
offensive text.
","misuse, offensiveness","misuse, offensive content","misuse, offensiveness",2022,"statement, concerns, suggestions"
440,"This investigation partially uses data from the field
of medicine. Specifically, it includes genes, dis-
eases and phenotypes that contribute to rare dis-
eases. Although the present work does not include
any patient information, it is translational in nature
and its broader impacts are first and foremost the
potential to improve the well-being of individual
patients in the society, and support clinicians in
their diagnostic efforts, especially for rare diseases.
Our work can also help Wikipedia curators and
content generators in finding relevant concepts.2213
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
441,"We list a number of ethical concerns related to envi-
ronmental impact, explainability, and transparency
in this section. We employ BERT ﬁne-tuning with
small modiﬁcations, therefore the environmental
impact can be considered small. Our work focuses
on well-known OOS detection task with established
use cases, therefore there would be no risk for un-
intended use. We use publicly available datasets
with licences suitable for academic research.
To assure explainability and transparency, we
report the length of utterances and domains of
datasets in Section 4.1. We report the statistics
of datasets and ﬁguratively report the split strategy
used in the experiments in Section 4.1. There are
two setups in our study. The zero-shot setting does
not include any training. In the supervised setup,
the complexity of our method is quite similar to
regular ﬁne-tuning procedure of BERT. The thresh-
olding hyperparameter is decided by maximizing
the Youden’s J statistic as explained in Section 4.2.
In Section 4.3, we also report the hyperparameters
of the baseline methods. We employ a modiﬁed
10-fold cross-validation strategy as explained in
Section 4.4 and apply t-test with Bonferroni correc-
tion to all experimental results.
6 Conclusion
We propose an OOS detection pipeline with a dis-
tance calculation between classiﬁer prediction and
uniform distribution, called D2U. In the zero-shot
setup, D2U serves as an architecture-agnostic post-
processing step to emphasize the distinction be-
tween INS and OOS. In the supervised setup, we
bring closer OOS predictions to uniform distribu-
tion with a modiﬁed loss function. Experimental
results, supported by statistical tests, show that
D2U outperforms existing baselines in zero-shot,
and has challenging performance in the supervised
setup. We plan to extend our study to different
architectures and deep learning tasks in the future.",transparency,transparency,transparency,2022,
442,"Like many other pretrained language representa-
tion models, the proposed model may also have
learned patterns associated with exposure bias. In-
terpretability associated with the output is rather
limited, hence users should use the outputs care-
fully. The proposed model ranks possible response
candidates, and does not filter out any “problem-
atic” candidates. Thus, for applications, where
candidate responses could be problematic, (e.g.,
offensive, hateful, abusive, etc.), users should care-
fully filter them out before providing them as input
to our model.
All the datasets used in this work are publicly
available. We did not collect any new dataset as
part of this work.
Banking77 Casanueva et al., 2020
has been obtained from https:
//github.com/PolyAI-LDN/
task-specific-datasets . It is avail-
able under a Creative Commons Attribution 4.0
International license with details here12.
SWDA Stolcke et al., 1998: The dataset
has been obtained from http://compprag.
christopherpotts.net/swda.html .
This work is licensed under a Creative Commons
Attribution-NonCommercial-ShareAlike 3.0
Unported License.
E-Intent Welivita and Pu, 2020: The dataset was
downloaded from https://github.com/
anuradha1992/EmpatheticIntents .
The original dataset is available at https:
//github.com/facebookresearch/
EmpatheticDialogues which is under the
Creative Commons Attribution 4.0 International
license.
MuTual and MuTual-plus Cui et al., 2020: The
datasets have been downloaded from https://
github.com/Nealcly/MuTual . Licensing
is unclear; the authors do not mention any license
information or terms of use.
DailyDialog++ Sai et al., 2020: The dataset
was downloaded from https://github.com/
iitmnlp/DailyDialog-plusplus . The
data is available under the MIT License.
12https://github.com/PolyAI-LDN/
task-specific-datasets/blob/master/
LICENSE
rMax or Reddit-727M conversational-
data Henderson et al., 2019: the
dataset has been obtained from https:
//github.com/PolyAI-LDN/
conversational-datasets/tree/
master/reddit . The dataset is available under
the Apache License Version 2.0.",bias,bias,bias,2022,"statement, concerns"
443,"Improvements in the detection of synthetic text
could be used by an adversary to improve the qual-
ity of generated text or to help them avoid detec-
tion (Darmetko, 2021). False positives are another
source of potential negative consequences of auto-
mated detectors. For example, incorrectly ﬂagged
human-written content could be a source of misin-
formation, and could additionally lead to a loss of
trust in the detection system. Care should be taken
that false positives do not affect certain demograph-
ics disproportionately (Bommasani et al., 2021,
§5.2). Finally, widespread awareness of the mere
possibility of synthetic scientiﬁc text can further
undermine public trust in genuine science (Makri,
2017).
",manipulation,false positives,manipulation,2022,"statement, concerns"
444,"Since this paper involves subjects related to human
conversation, we have ensured that all the experi-
ments will cause no harm to humans. The dataset
EmpatheticDialogues is collected by (Rashkin
et al., 2019), all the participants join the data collec-
tion voluntarily. Also, the dataset provider filters
all personal information and obscene languages.
Therefore, we believe that the dataset Empathetic-
Dialogues used in our experiments are harmless to
users, and the model trained on this dataset is not
dangerous to humans.
",no_ethical_concerns,no_ethical_concerns,"privacy, misuse",2022,statement
445,"Risks, limitations, and intended use We con-
sider the main risk of this work that the evaluation
suite may be used to make overstating claims about
model abilities in the future. In particular, should
future models achieve very high or even perfect
accuracy on the evaluation suite, then such results
may be seen as evidence of human-like abilities
of discourse entity processing. We therefore want
to emphasize that achieving high accuracy on this
evaluation suite is a necessary but not necessar-
ily sufficient requirement for a model to exhibit
human-like entity tracking abilities.
Further, it seems likely that models fine-tuned on
similar examples would perform a lot better on this
evaluation suite, and therefore researchers should
only use this dataset for out-of-domain evaluations
in which the model has not been trained on similar
examples.
Finally, we only evaluated models trained on
English data in this work and it is conceivable that
entity tracking abilities of models trained on other
languages differ from the results reported here.
Human subject experiments As we mentioned
in Section 4.1, we recruited crowdworkers from
Prolific to validate the experimental stimuli. Par-
ticipants were based in the US and on average re-
ceived compensation of about $14/hour, which is
almost twice the minimum wage in most states in
the US. The experiment has been pre-approved by
the New York University IRB, and there were no
risks associated with participation.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
446,"Models such as the one proposed in this paper,
which output toxicity classifications of text or
speech and reasoning behind such classifications
should be used with care. Considerations of al-
gorithmic fairness should be taken into account
(Corbett-Davies et al., 2017), as well as cultural
differences (Oliva et al., 2020) and racial biases
(Xia et al., 2020) which can lead to misclassifica-
tions of offensiveness. Care should be taken to
avoid political bias in training datasets, when train-
ing these models for deployment purposes (Wich
et al., 2020). Finally, concerns about censorship
should be taken seriously (Heins, 2014).
","fairness, bias, censorship",bias,bias,2022,"statement, suggestions, concerns"
447,"It is essential to consider potential ethical issues
in knowledge-grounded dialogue systems. In our
work, PLUG is pre-trained on a large-scale dataset
Reddit Conversation, which is crawled from the
internet. We follow Galley et al. (2018) to ﬁlter out
dialogues that have profanity content. However,
it is still possible to include inappropriate content
in the pre-training dataset. In processing the Red-
dit Conversation dataset during pre-training, we
have carefully designed rules to remove knowl-
edge that has profanity words. Additionally, the
T5 model may have seen inappropriate content in
its pre-training tasks, and it may generate wrong
responses even if we input appropriate knowledge.
Considerable additional work is needed to detect
profanity content when we generate with a pre-
trained language model. In addition to these ethical
considerations, we have sought to better conduct
our human evaluation by transparently communi-
cating with crowd-workers about data use and study
intent and compensating workers at a reasonable
hourly wage.",inappropriate content,no_ethical_concerns,no ethical concerns,2022,statement
448,"When we apply large-scale corpora from the Web,
alleviating bias issues is necessary. We make efforts
from two perspectives: (1) For input reviews, we
have filtering steps to remove harmful contents,
and ensure that they do not have user privacy
information like age and gender (“Data Collection
and Filtering” of § A.1); (2) For output ad texts,
we are cautious before online deployment with a
risk control procedure (“Post-Processing before
Deployment” of § A.1). (3) Our model does not use
user privacy information like age and gender.
",no_ethical_concerns,no_ethical_concerns,"bias, misuse, offensiveness",2022,statement
449,"Reproducibility and ethics are inherently related,
since ensuring that research is reproducible by
members of the community that are not its orig-
inal authors contributes to making the field more
inclusive (e.g. providing the code and hyperpa-
rameters needed to replicated a state-of-the-art ML
model can help researchers build and expand upon
it). Furthermore, being transparent about the costs
of the model, both in terms of the computational
power need to train it as well as the data involved,
helps members of the community be more equi-
table in evaluating it: for instance, if two models
achieved similar accuracy on the same dataset, with
one requiring 10x more computation than the other,
2https://reproducedpapers.org/8
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
450,"We use utterances taken from 100 subcommuni-
ties (i.e., subreddits) of the popular online platform
Reddit to train style representations with differ-
ent training tasks and compare their performance.
With our work, we aim to contribute to the devel-
opment of general style representations that are
disentangled from content. Style representations
have the potential to increase classiﬁcation perfor-
mance for diverse demographics and social groups
(Hovy, 2015).
The user demographics on the selected 100 sub-
reddits are likely skewed towards particular demo-
graphics. For example, locally based subreddits
(e.g., canada, singapore) might be over-represented.
Generally, the average Reddit user is typically
more likely to be young and male.11Thus, our
representations might not be representative of (En-
glish) language use across different social groups.
However, experiments on the set of 100 distinct
subreddits should still demonstrate the possibil-
ities of the used approaches and methods. We
hope the ethical impact of reusing the already pub-
lished Reddit dataset (Baumgartner et al., 2020;
Chang et al., 2020) to be small but acknowledge
that reusing it will lead to increased visibility of
data that is potentially privacy infringing. As we
aggregate the styles of thousands of users to cal-
culate style representations, we expect it to not be
indicative of individual users.
We conﬁrm to have read and that we abide by
the ACL Code of Ethics.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
451,"In this work, we presented a new dataset compiled
automatically from Wikipedia, Wikinews and Wiki-
data. After the initial collection process, we per-
form rigorous post-processing steps to reduce po-
tential errors in our dataset. Our dataset is multi-
lingual with texts from 44 languages. In our main
paper, we state these languages as well as their in-
dividual representation in our dataset. As we high-
light in the paper, the proposed linking systems
only work for specific class of events (eventive
nouns) due to the nature of our dataset.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
452,"There are ethical considerations to take into ac-
count when using NER technology. For example,
the technology may disproportionately work worse
for some populations with uncommon name struc-
ture. This could have a negative impact on these
groups, as their names may not be accurately recog-
nized and classified by the software. It is important
that we are aware of potential biases in our data
and algorithms, so that we can avoid unfairly dis-
criminating against certain groups of people.
",bias,bias,bias,2022,"statement, concerns, suggestions"
453,"Informed Consent. Participants of our
user study participated voluntarily and
anonymously with a self-chosen, unique
study key that allows them to request
the deletion of their provided data at a
later point. Upon registration, they are
informed about the data presented and
collected in the study, its further use,
and the purpose of the study. Before col-
lecting any data, participants are explic-
itly asked for their informed consent.
We do not collect any personal data in
our study. If participants do not provide
their informed consent, their study key
is deleted immediately. For publication,
the study key is further replaced with a
randomly generated user id.
Use of Twitter Data. The CovidLies corpus
(Hossain et al. 2020) we used to generate
the instances for our annotation study
consists of annotated tweets. To protect
the anonymity of the user who created
the tweet, we only display the text (re-
moving any links) without any meta-
data like Twitter user id or timestamps to
our study participants. We only publish
the tweet ids in our study data to con-
form with Twitter’s terms of service and
hence, all users retain their right to delete
their data at any point.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,"statement, actions"
454,"The usual approach to building an AER system is to design the task (identify the
process to be automated, the emotions of interest, etc.), compile appropriate data (label
some of the data for emotions—a process referred to as human annotation), train ML
models that capture patterns of emotional expression from the data— the method , and
evaluate the models by examining their predictions on a held-out test set. There are
ethical considerations associated with each step of this development process. Consid-
erations for privacy and social groups are especially pertinent for AER and cut across
task, design, data, and evaluation.
This section describes ﬁfty considerations grouped under the themes: Task Design,
Data, Method, Impact and Evaluation , and Implications for Privacy and Social Groups . First I
present an outline of the considerations along with a summary for each grouping. This
is followed by ﬁve sub-sections (§3.4.1 through §3.4.5) that present, in detail, the ethical
considerations associated with the ﬁve groups.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
455,"The usual approach to building an AER system is to design the task (identify the
process to be automated, the emotions of interest, etc.), compile appropriate data (label
some of the data for emotions—a process referred to as human annotation), train ML
models that capture patterns of emotional expression from the data— the method , and
evaluate the models by examining their predictions on a held-out test set. There are
ethical considerations associated with each step of this development process. Consid-
erations for privacy and social groups are especially pertinent for AER and cut across
task, design, data, and evaluation.
This section describes ﬁfty considerations grouped under the themes: Task Design,
Data, Method, Impact and Evaluation , and Implications for Privacy and Social Groups . First I
present an outline of the considerations along with a summary for each grouping. This
is followed by ﬁve sub-sections (§3.4.1 through §3.4.5) that present, in detail, the ethical
considerations associated with the ﬁve groups.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
456,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
457,"This work is not related to any speciﬁc real-world
application. All the datasets used in our experi-
ments are collected by crowdsourcing ( Anantha
et al. ,2021a ), especially through Amazon Mechan-
ical Turk ( Reddy et al. ,2019 ;Choi et al. ,2018 ;
Elgohary et al. ,2019a ), and they are publicly avail-
able. As the nature of the task, the data collection
of CQA and QR is done anonymously and does
not involve any privacy or intellectual property con-
cern.
",no_ethical_concerns,no_ethical_concerns,intellectual property,2022,statement
458,"Our paper discusses an approach for attending toindividual annotator’s judgements in training a su-
pervised model. In doing that, our multi-annotator
approach better preserves minority perspectives
that are usually sidelined by majority votes. Our
intended use case for this approach is in subjec-
tive NLP tasks, such as identifying affect, abusive
language, or hate speech, where generating a sin-
gle true answer does not capture the nuances.
While our method likely preserves minority per-
spectives, a misuse of this technique might happen
upon weighting individual annotator’s labels dur-
ing prediction. Such an alternation aimed solely to
improve the majority label prediction performance
may adversely impact the representation of dif-ferent perspectives in the model. In fact, such an
optimization may cause further marginalization to
under-represented perspectives than the current
majority vote–based approaches. For instance,
identifying annotator heads that significantly dis-
agree with the majority vote might cause their
perspectives to be at higher risk of being excluded.
It is also important to consider the number
of annotators in the annotator pool when apply-
ing this method, in order to protect the privacyand anonymity of annotators, since our approach
attempts to model their personal subjective pref-
erences and biases. This is especially critical in
the case of sensitive tasks such as hate speech an-
notations, where associating individual annotators
with such representations may be undesirable.
","bias, misuse",misuse,"bias, misuse",2022,"concerns, suggestions"
459,"While promising, the results in this work should
not be interpreted as a definitive assessment of the
performance of hate speech detection in Italian. We
are unsure if our model can maintain a stable and
fair precision across the different targets and cate-
gories. HATE-ITA might overlook some sensible
details, which practitioners should treat with care.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
460,"This work explores the privacy implications of a
text classiﬁcation setting in which training is per-
formed on sensitive and private data. We inves-
tigate whether data leakage is feasible under this
setting. We believe that this work is a ﬁrst step
in determining the susceptibility of the underlying
text classiﬁcation model to privacy leakage and de-
tecting unauthorized use of personal data. Both the
dataset and the model are publicly available.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
461,"A limitation of this work is that it did not consider
linguistic characteristics of the pre-trained models
(Bai et al., 2020). We used an artificial teacher in
MT and did not deeply examine hybrid MT-AL
strategies, although we used an AL approach as
teacher in the MT setup. Still, this work may stim-
ulate NLP researchers to consider the benefits of
AL and MT, especially for challenging subjective
NLP tasks such as text-based emotion prediction
(Alm, 2011). Additionally, continued work can ex-
plore how the findings apply in the context of other
corpora, including with multimodal data.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
462,"This work was completed following the ACL code
of ethics. Secure access to the shared task dataset
was provided with IRB approval under University
of Maryland, College Park protocol 1642625 and
approval by the Biomedical and Scientific Research
Ethics Committee (BSREC) at the University of
Warwick (ethical application reference BSREC
40/19-20). Each member of the team completed
an NDA and a data usage agreement, ensuring that
the data used for this work would not be misused,
distributed, or otherwise compromised. Due to the
sensitive nature of this data, dataset creators andthe shared task organizers were de-identified, and
each team member agreed to make no attempt to
identify the individuals whose data was used for the
task. We completed our analyses using the secure
NORC Data Enclave to further protect the data.3
",no_ethical_concerns,no_ethical_concerns,misuse,2022,statement
463,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
Before being granted access, we signed a Non-
Disclosure Agreement (NDA) and a Data Enclave
Use Agreement (DUA).",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
464,"Secure access to the shared task dataset was pro-
vided with IRB approval under University of Mary-
land, College Park protocol 1642625 and approval
by the Biomedical and Scientific Research Ethics
Committee (BSREC) at the University of Warwick
(ethical application reference BSREC 40/19-20).
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
465,"NSSI communities and their members tend to re-
fer to NSSI as “self harm."" In this paper, we use
“self-injury"" or the acronym NSSI as it is more spe-
ciﬁc to the behavior in question (“self harm"" could
also include suicide attempts) as well as the term
most frequently found in recent literature. How-
ever, papers we cite may use the terms “deliberate
self harm"" (DSH) or “self-mutilation."" For a review
and discussion of the most appropriate language to
use when referring to people who self-injure, see
(Hasking et al., 2021).
There are a number of ethical considerations
when using sensitive data. Since Reddit data is pub-
licly available, this study was deemed non-human
subjects research and exempt from approval of an
Institutional Review Board. Despite this ofﬁcial
classiﬁcation, the data used throughout is indeed
human generated and reﬂects the lived experiences,
intimate feelings, and personal struggles of the au-
thors. Related, there are issues regarding informed
consent when using public data. Online communi-
ties such as r/selfharm are intimate and personal
spaces, where consensual sharing happens between
community members and not with researchers who
collect the data. For a full discussion of related
issues, we recommend the work of Chancellor et al.
(2019), which identiﬁes conﬂicting representationsof humans in “human centered machine learning.""
There are also issues of privacy; while Reddit is
anonymous, there are risks of revealing sensitive
information or the identities of the accounts used
in the study (Proferes et al., 2021). As such, we
only report aggregate information throughout the
manuscript, and we have chosen to not publicly
release any of the data used in this study. Finally,
there are some egregious use cases with this data.
For example, given the cross posting between NSSI
and mental health forums, one could imagine ads
for anti-depressants being targeted to the redditors
in this study.
One must also consider researchers’ well-being
when working with data of this type. Spending time
with sensitive and potentially triggering data can be
emotionally challenging for researchers. As such,
researchers should also consent to working with
this type of data and continue to consent through-
out the life of the project. To help with these issues,
our research group held one-on-one and group ses-
sions to discuss triggering content and mental and
emotional fatigue experienced while working on
this and similar projects.
","privacy, consent","privacy, consent",misuse,2022,"statement, concerns, actions"
466,"We have proposed an approach for AD using TN-
based methods with the aim of learning effective
representations for data. We have used SciBERT
trained on scientific publications and BioBERT
trained on biomedical domain corpora (PubMed
abstracts and PMC full-text articles) for our exper-
iments. Instead of finetuning all the layers in the
pre-trained language models, we have finetuned
only the last encoder layer by freezing the first
11 encoder layers thereby bringing the latest deep
learning advances to AD in a computationally ef-
ficient way. However, the mnetwork architecture
despite its smaller number of parameters has m
architectures. This has resulted in more updates in
the parameters increasing the computational time
in the training stage.
The proposed approaches have been tested and
validated on three datasets: SDU dataset, CASI
dataset, and MeDAL dataset. According to the
National Statement on Ethical Conduct in Human
Research (2007) — Updated 2018 (National Health
and Medical Research Council, 2018), a new ethics
approval is not required for our experiments and,
to the best of our knowledge, the three original
datasets have been created ethically. All the three
datasets are publicly available (see Appendix B).
Identifying the correct expansion of acronyms
is important in improving the understandability
of scientific/medical text due to the prevalence of
technical acronyms which are shorthanded for ease
of use. For people with limited expertise knowl-
edge, understanding scientific/medical documents
can be difficult, stressful and cause misunderstand-
ings. The proposed methods can be used in scien-
tific/medical text simplification tasks to provide lay
people with better understanding of text through
the disambiguation of acronyms.",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
467,"Because of legal and institutional concerns arising
from the sensitivity of clinical data, it is difficult for
the NLP community to gain access to relevant data
except for MIMIC (Johnson et al., 2016). Despite
its large size (covering over 58khospital admis-
sions), it is only representative of patients from a
particular clinical domain (the intensive care unit)
and geographic location (a single hospital in theUnited States). Such a sample is not representative
of either larger population of patient admissions or
other geographical regions/hospital systems. We
have tried to address the second issue by collecting
data across multiple practices in the US. However,
it is impossible to predict whether our models will
generalize to the entire patient population with-
out actually evaluating on allthe different radiol-
ogy practices. Thus we have to be extra careful
about out-of-distribution data since the actionable
insights we generate from our models can be poten-
tially faulty and can lead to severe consequences.
Finally, we recognize the need to minimize ethi-
cal risks of AI implementation which can include
threats to privacy and confidentiality, informed con-
sent, and patient autonomy. We strongly believe
that stakeholders should be encouraged to be flexi-
ble in incorporating AI technology, most likely as
a complementary tool and not a replacement for a
physician. Thus, we develop our workflows, anno-
tation guidelines and generate actionable insights
by working in conjunction with a varied group of
radiologists and medical professionals.
",no ethical concerns,"privacy, confidentiality",no ethical concerns,2022,"statement, concerns "
468,"Because of legal and institutional concerns arising
from the sensitivity of clinical data, it is difficult for
the NLP community to gain access to relevant data
except for MIMIC (Johnson et al., 2016). Despite
its large size (covering over 58khospital admis-
sions), it is only representative of patients from a
particular clinical domain (the intensive care unit)
and geographic location (a single hospital in theUnited States). Such a sample is not representative
of either larger population of patient admissions or
other geographical regions/hospital systems. We
have tried to address the second issue by collecting
data across multiple practices in the US. However,
it is impossible to predict whether our models will
generalize to the entire patient population with-
out actually evaluating on allthe different radiol-
ogy practices. Thus we have to be extra careful
about out-of-distribution data since the actionable
insights we generate from our models can be poten-
tially faulty and can lead to severe consequences.
Finally, we recognize the need to minimize ethi-
cal risks of AI implementation which can include
threats to privacy and confidentiality, informed con-
sent, and patient autonomy. We strongly believe
that stakeholders should be encouraged to be flexi-
ble in incorporating AI technology, most likely as
a complementary tool and not a replacement for a
physician. Thus, we develop our workflows, anno-
tation guidelines and generate actionable insights
by working in conjunction with a varied group of
radiologists and medical professionals.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
469,"We explore the potential security issues of text re-
trieval systems in this paper and propose TRAt-
tack that is experimentally veriﬁed to be effective
to many text retrieval models. Hope that our ap-
proach and discussions could inspire more explo-
rations and designs of advanced defense methods
and security policies.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
470,"This paper studies clinical NER with a small
strongly labeled and a large weakly labeled dataset.
Our investigation neither introduces any social or
ethical bias to the model nor amplifies any bias
data that allows downloading and using it given that
any publication/distribution states and describes
any modifications made to the content of the data.
It is public data that anyone can download and re-
produce the outcomes with the code made available
on Github.
",no_ethical_concerns,no_ethical_concerns,bias,2022,statement
471,"Vision-language systems have many potential ap-
plications beneficial for society:
•Aiding visually impaired users in un-
derstanding their surroundings (Human:
What is on the shelf above
the microwave? AI: Canned
containers. ),
•Teaching children through interactive de-
mos (AI captioning a picture of Dall
Sheep: That is Dall Sheep. You
can find those in Alaska. ),
•Aiding analysts in processing large quan-
tities of visual surveillance data (An-
alyst: What kind of car did the
man in red shirt leave in? AI:
Blue Toyota Prius. ),
•Interacting with in-home physical robots (Hu-
man: Is my laptop in my bedroom
upstairs? AI:Yes. Human: Is the
charger plugged in? ),
•Making visual social media content more ac-
cessible (AI: Your friend Bob just
uploaded a picture from his
Hawaii trip. Human: Great, is
he at the beach? AI:No, on a
mountain. ).
But like most other technology, such vision-
language systems could also be used for potentially
harmful applications such as:
•Invasion of individual’s privacy by using
vision-language systems to query streams of
video data being recorded by CCTV cameras
at public places.
•Visually impaired users often need assistance
with parsing data containing personal informa-
tion (Ahmed et al., 2015), such as credit cards,
personal mails etc. Vision-language systems
providing such assistance could be configured
to leak / retain such personally identifiable
information.
In addition to the above potentially harmful ap-
plications of vision-language systems, there existethical concerns around fairness and bias. The
vision-language models, as other deep learning
based models (Zhao et al., 2017b), could poten-
tially amplify the biases present in the data they are
trained on. Since the training data (images and lan-
guage) captures stereotypical biases present in the
society (e.g, the activity of cooking is more likely
to be performed by a woman than a man), am-
plification of such stereotypes by vision-language
systems is concerning as it has the potential to harm
the users in the relevant groups (based on gender,
race, religion etc.) by entrenching existing stereo-
types and producing demeaning portrayals (Brown
et al., 2020).
To raise awareness about such ethical concerns
and to promote discussions among researchers, the
last part of the tutorial (“Beyond statistical learn-
ing in vision-language”) will focus on such short-
comings of existing models and we will discuss
some methods that aim to tackle some of these
challenges.
","fairness, bias, misuse","bias, misuse","bias, misuse",2022,"statement, concerns, suggestions"
472,"We do not anticipate any ethical issues related to
the topics of the tutorial.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
473,"Non-autoregressive sequence generation can im-
prove the inference speed of various sequence gen-
eration tasks in text and speech. Unfortunately,
this technology may be misused to generate deep-
fake content (Thies et al., 2016) such as mimicking
one’s writing style or speaking style. However,
great attempts have been made to detect the deep-
fake content (Kaggle, 2019), which can minimize
or avoid its potential negative impact.
",misuse,misuse,misuse,2022,"statement, concerns"
474,"Non-autoregressive sequence generation can im-
prove the inference speed of various sequence gen-
eration tasks in text and speech. Unfortunately,
this technology may be misused to generate deep-
fake content (Thies et al., 2016) such as mimicking
one’s writing style or speaking style. However,
great attempts have been made to detect the deep-
fake content (Kaggle, 2019), which can minimize
or avoid its potential negative impact.
",misuse,deep fake content,misuse,2022,"statement, concerns, actions"
475,"Although automatic deception detection has the po-
tential to benefit society, there are several ethical
concerns within this line of research. Automatic
deception detection has varying degrees of severity
depending on the application area. The impact of a
false positive is substantially lower when detecting
deceit in informal activities such as gaming. How-
ever, when detecting dishonesty in a criminal in-
vestigation, a false positive can have serious impli-
cations. In general, automatic deception detection
should be employed with caution, especially when
there is no manual human verification involved.
For the case of cross-domain deception detec-
tion applications, it is important to test the model
on the target domain before deploying it, as men-
tioned in Section 1. To understand the differences
between deception domains, a linguistic feature
analysis should be performed, as we mention in
Section 3.1. Finally, to increase transparency in
multi-modal deception detection, it is critical to
compute importance scores for each modality as
mentioned in Section 3.3. As automatic deception
detection across domains, languages and modali-
ties becomes a more widely studied subject, it is
important to be aware of the ethical considerations
and also take the necessary precautions to avoid
harm to society.","misuse, transparency, manipulation","transparency, manipulation","misuse, transparency, manipulation",2022,"statement, concerns"
476,"We declare no conﬂict of interests with the authors
of DPText, we do not even know them personally.
The purpose of this paper is strictly scientiﬁc.
",no ethical concerns,no_ethical_concerns,no ethical concerns,2022,statement
477,"The addition of proprietary data to existing datasets
to fine-tune NLU models can often insert confi-
dential information into datasets. The proposed
attack could be misused to extract private infor-
mation from such datasets by an adversary with
open-box access to the model. The objectives of
this work are to (1) study and document the actual
vulnerability of NLU models against this attack,
which shares similarities with existing approaches
(Fredrikson et al., 2014; Song and Raghunathan,
2020); (2) warn NLU researchers against the pos-
sibility of such attacks; and (3) propose effective
defense mechanisms to avoid misuse and help NLU
researchers protect their models.
Our work demonstrates that private information
such as phone-numbers and zip-codes can be ex-
tracted from a discriminative text-based model,
and not only from generative models as previ-
ously demonstrated (Carlini et al., 2020). We advo-
cate for the necessity to privatize such data using
anonymization (Ghinita et al., 2007) or differential
privacy (Feyisetan et al., 2020). Additionally, in
case the training data continues to contain some
private information, practitioners can prevent the
extraction of sensitive data by using the defense
mechanisms described in Section 6, which reduces
the attack performance to a random guess.
","misuse, security","misuse, privacy","misuse, privacy",2022,"statement, concerns"
478,"The next generation of voice assistants need to be more naturally interactive and accessible for everyone, especially as SDSs are increasingly applied in the healthcare setting. In order to make informed design decisions and effectively evaluate new dialogue systems with specific user groups in mind, potentially sensitive data must be collected. Offthe-shelf audio recorders are not secure and cannot be ethically approved for use, creating a barrier to complete crucial research. This work will not only enable us to design dementia-friendly assistants and social robots in the future. We hope other researchers use the CVRSI to make a positive impact with similar goals in mind, and in more general settings to ensure data privacy.",no_ethical_concerns,no_ethical_concerns,"privacy, sensitive data",2022,statement
479,"ENTIGEN is proposed for evaluating the change in the model generations in the presence of ethical interventions. We limit our work to selected categories (such as profession and objects) within the gender and social axis even though there might be other categories such as politics where equal representation is desired. Even though there are a wide range of groups within the gender and skin color axis, we only consider categorizing individuals into {man, woman} and {light-skinned, dark-skinned}. We are aware of the negative impact brought from limited binary categories. It is offensive for underrepresented groups and possibly causes cyclical erasure of non-binary gender identities. However, assessing any individual’s gender identity or sex is impossible based on their appearance; hence we limit our work on classifying individuals into man/woman based on the perceptual bias and gender assumptions of the human annotators and the CLIP model. We also emphasize that our analysis is based on generated images not the images containing real individuals. We also understand that there are numerous skin colors but we limit our study to classify individuals into light-skinned or dark-skinned. Additionally, we do not instruct the annotators to use Fitzpatrick scale (Fitzpatrick, 1986) to determine skin-color, rather the decision is left to their own perception. The imperfect image-to-text generative modeling can run into the hazard of missing certain data modes that eventually compound the social biases
present in the pre-trained dataset (Saharia et al.,
2022). There are harms associated with the models
ability to change predictions drastically based on
the prompts as it can lead to the generation of objectionable contents. We encourage the practice of
having sophisticated Not Safe For Work (NSFW)
filters before image generations. A CLIP-based
filter used by Stable Diffusion implementations is
a positive step in this direction.
Extensions of our work can focus on increasing the representation of more groups as well as
designing text-to-image generative models that output images of people belonging to diverse groups
conditional on the neutral prompt.
As we annotate a new dataset ENTIGEN, we
compensate annotators with a fair rate. We recruit
annotators from Amazon MTurk. We provide a
fair compensation rate with $10 per hour and spent
around $60 in total to the annotators on human evaluation. Each HIT costs several seconds according
to the statistics in Amazon MTurk.","harmfulness, objectionable content, non-binary_erasure, offensiveness","harmfulness, objectionable content, bias, non-binary gender, offensiveness","harmfulness, objectionable content, non-binary_erasure, offensiveness",2022,"statement, concerns, suggestions"
480,"Ethics reviewing in NLP was first implemented in
June 2021 for NAACL 2021. In this paper we addressed the following questions about papers published at ACL 2021 in August 2021. 1) What percentage of papers include an ethical considerations
section (ECS)? 2) Are there some tracks that stand
out, either positively or negatively? 3) What types
of ethical questions are addressed in ECSs, and in
what proportion? 4) Are the papers with ECSs that
went through ethics review different from those that
did not? 5) Are there differences between countries with respect to the ECSs?. We also describe
common obstacles and arguments regarding ECSs
and illustrate papers that have overcome them insightfully. Potential harms of our paper include
over-generalizations of the empirical results we
show here so we want to make our limitations explicit and we do so in the next section. One of our
reviewers pointed out that a potential harm is that
this paper raises opinions about the Ethics Consideration Section, which is to some extent sensitive,
and that may affect the point of view of other authors toward ECS. To this we can only say: trying
to raise awareness and stimulate open discussion
of ECSs in the NLP community seems better than
leaving them unexamined..
We believe this paper might benefit NLP researchers who are authors, reviewers or conference
organizers in different ways. Authors might find
in this paper tools to come up with better ECSs.
Ethics reviewers might see the impact of their effort.
And organizers could have a glimpse at questions
addressed and not addressed by ECSs. Our goal
is to contribute to the ongoing debate on what is
the current situation of the broader societal impact
discussions of NLP research.","over-generalizations, sensitive opinions","Sensitive opinions, overgeneralisation","over-generalizations, sensitive opinions",2022,"concerns, statement"
481,"When developing an automated system for clinical
note generation from doctor-patient conversations,
it is crucial to consider various ethical considera-
tions. One such consideration is the privacy and
confidentiality of patient information. The system
must be designed to comply with regulations and
guidelines for protecting patient data. Additionally,
there must be explicit consent processes, ensuring
that patients understand how their data will be used
and allowing them to opt-out if desired. The system
must also be developed fairly and transparent, en-
suring it does not perpetuate biases or contribute to
health disparities. Moreover, the system must be ac-
curate and reliable, as errors or inaccuracies could
lead to incorrect diagnoses or treatments. Overall,
it is essential to approach the development of an au-
tomated system for clinical note generation with a
solid ethical framework to ensure that it aligns with
the highest standards of patient care and ethical
conduct.","privacy, fairness, transparency, accuracy, reliability",,"privacy, fairness, transparency, accuracy, reliability",2023,suggestions
482,"This paper proposes a TIT model and a multi-stage
training framework. We take ethical considera-
tions seriously and ensure that the methods used
in this study are conducted in a responsible and
ethical manner. We also release a Chinese-English
TIT dataset named OCRMT30K, which is anno-
tated based on five publicly available Chinese OCR
datasets, and are used to support scholars in doing
research and not for commercial use, thus there
exists not any ethical concern.
",no ethical concerns,,no ethical concerns,2023,actions
483,"We mention several ethical issues related to text
prediction in Sections 1 and 2. The central issue
discussed in this paper, that of the trade-off be-
tween performance and risks of text prediction,
itself has deep ethical connotations. For instance,
one might argue that it is ethically incorrect to de-
ploy a system which poses any risks at all. In other
words, the trade-off could be resolved in favor of
one extreme (which then is no longer a trade-off).
We do not take any such position here, and neither
try to provide any guidelines on what should be
the ideal trade-off for such an application. There
are several factors, including but not limited to,
the risk-criticality of the application (for instance
typing a CV or legal report, vs. a social media com-
ment) and user’s personal preferences, that should
be considered before settling for a trade-off. In-
stead, what we would like to highlight through this
work is that such a trade-off exist and current tech-
nology is unable to completely eradicate harmful predictions. Therefore, at the very least, the service
provider/app developer of text prediction systems
should be aware of the harms and make an effort
to inform the user of such potential harms.
We are also aware that the CheckLists were cre-
ated by a fairly homogeneous (in terms of reli-
gion, nationality and race) set of users. Though
we have taken utmost care to sensitize the users
about various ethical aspects of fairness, a bias in
the annotation or template forms cannot be ruled
out. Note that we also use two existing CheckLists
which were created by different groups. We ob-
serve that the trends are fairly consistent across
these datasets. On a related note, the definition of
what is toxic or inappropriate can also be debated.
Indeed, there were several occasions on which the
users designing the templates or annotating the
examples did not agree on the appropriateness or
severity level. These issues were openly discussed
in the larger group (including the authors of this
paper) to reach an agreement. We are aware that
not everybody will align to the decisions that were
taken by our group of volunteers. Thus, the dataset
created during this study, when used for further re-
search, should be appropriately aligned to the needs
and judgements of the researchers/developers and
the tasks at hand. The annotation study is cov-
ered under IRB ID 10566 and the consent form is
available in the appendix.
","harmful predictions, bias",,"harmful predictions, bias",2023,"actions, concerns"
484,"Our work focuses on improving the commonsense
reasoning ability of pre-trained language models. It
probably does not introduce extra ethical concerns.
However, in commonsense knowledge extraction,
the neural commonsense knowledge model may
generate unexpected (e.g., biased) commonsense
inferences, and training with these inferences may
lead to additional bias in the pre-trained model.
Nevertheless, all pre-trained language models con-
tain bias and should be examined.
","bias, unexpected responses",,"bias, unexpected outputs",2023,concerns
485,"This paper investigates the pre-training for word
alignment, which will not lead to a negative social
impact. The data used in this paper are all publicly
available and are widely adopted in previous liter-
ature, avoiding any copyright concerns. The pro-
posed method does not introduce ethical bias. On
the contrary, our aim is to advance word alignment
techniques to enhance their utility for low-resource
language communities, promoting inclusivity and
equitable access to language resources.
",no ethical concerns,,no ethical concerns,2023,actions
486,"The authors believe that their work does not raise
any ethical questions of harm or discrimination.
Moreover, they acknowledge that the benchmark
has a wide range of potential applications and want
to make it clear that they are not responsible for
any unethical applications of their work.
",no ethical concerns,,no ethical concerns,2023,statement
487,"The authors foresee no ethical concerns with the
work presented in this paper, in particular concern-
ing any kind of harm and discrimination. Since
the presented architecture can have a wide range
of usages, the authors are not responsible for any
unethical applications of this work.
",no ethical concerns,,no ethical concerns,2023,statement
488,"We honor the ethical code set out in the ACL Code
of Ethics.
",no ethical concerns,,no ethical concerns,2023,statement
489,"In considering the ethical aspects of this study, we
strive to avoid any potential harm to individual In-
ternet users or publishing outlets, to protect their
privacy, and to respect their right to the created
texts. These considerations motivated the follow-
ing practical decisions. First, we used the publi-
cations that were publicly available at the time of
collection. Second, the corpus is made available
only as a list of links augmented with non-revealing
attributes, such as date, media type, source (web-
site or platform), region and engagement score (for
social networks subcorpus)5, with the actual tex-
tual content deleted from this version of the cor-
pus. It is done to protect the users’ right to take
down their content and to avoid violating their copy-
right. While these restrictions imply reduced repli-
cability of our results and additional efforts for
researchers associated with the necessity to recol-
lect the data, they were considered ethical to avoid
potential harm to individuals. Third, we do not
distribute any metadata or publish any considerable
parts of the collected texts that can be used to iden-
tify individuals with particular political beliefs at
the moment or in the future. This is particularly im-
portant, given the current scale of prosecution for
anti-war publications and reactions expressed on-
line in Russia. Finally, while we admit that research
on propaganda strategies can be used to improve
ways of information manipulation, we think that
uncovering and describing these practices serves
the greater social good of raising the awareness of
the public about types of disinformation and po-
tentially delusive environments that can be created
online. When presenting the results of the analyses,
care was taken to avoid any wording that can be
interpreted as promoting particular political beliefs,
where possible.",research misuse,,research misuse,2023,actions
490,"In this work, we have actively engaged with the
fact that the actions of machine learning and NLP
engineering can change the world and affect both
society and individuals. The use of computer tech-
nologies may produce new or reproduce existing
discrimination, and we, therefore, strive towards
being as inclusive as possible. Not only do we
wish to draw attention to social biases inherent in
contemporary Danish language technology but we
hope that our work can be used directly by other
researchers when deciding on tools usages in their
scholarly pipeline, particularly for those working
with cultural heritage data.
9
",no ethical concerns,,no ethical concerns,2023,statement
491,"In this paper, we have created a GPT-3 fine-tune
that is capable of producing synthetic news. As it
may be possible to use it for malicious purposes,
the fine-tuned model will not be available to anyone
besides the authors. Per January 4, 2024, the au-
thors will also lose access to the model as OpenAI
announced all davinci models, including fine-tunes,
will depreciate.7Nonetheless, we acknowledge
that this paper demonstrates the ease of producing
such a model, but also how it may be detected.
Finally, we recognize that the synthetic news
produced for this paper could potentially contain
societal biases from GPT-3’s training data or from
the real news articles used for fine-tuning.
7https://openai.com/blog/gpt-4-api-general-availability
","misuse, bias",,"misuse, bias",2023,"statement, actions"
492,"When elaborating this study, we took into consider-
ation the following elements :
•Informed consent: We made sure to elaborate
a consent form stating the goals of the study
and the precise actions the participant will
have to accomplish in order to finish it. The
consent form also included information about
the risks and benefits of participating in this
research. Indeed, some articles are describing
the violent interaction the victim had with the
police, and mention systemic racism. We dis-
closed that some articles were taken from ex-
tremely conservatives news sources and could
make an apology of white supremacy as well.
A full review of the consent form was done be-
forehand with the participants to answer any
potential questions they might have.
•Confidentiality: At no point during the study
were the participants asked to disclose any
information, whether name, age, gender, oc-
cupation, or any other potentially identifying
data. We used the Qualtrics survey software’s
anonymous link, and did not include any iden-
tifiable question in the survey itself.
•Participants welfare: Participants were told
that they could withdraw from the study at
any time, without any consequences, and that
the choice to participate or not was their own.
•Ethical review: This study was approved by
the Institutional Review Board of our univer-
sity before any data collection began. The
members of this research team were asked to
pursue an ethics training before being able to
submit the study protocol.
",no ethical concerns,,no ethical concerns,2023,actions
493,"It is worth noting that the behavior of the our down-
stream smaller models is subject to biases inherited
from the larger teacher LLM. We envision that the
same research progress in reducing anti-social be-
haviors in LLMs can also be applied to improve
smaller language models.
",bias,,bias,2023,"statement, suggestion"
494,"Participation in our study was voluntary and fully
anonymous. We did not collect any personal data
that would allow us to identify people and did not
exclude any participants unless they specifically
requested their participation be ignored in the last
open commentary field of our survey or they statedtechnical difficulties. Concerning our rewriting
models, we did not filter the publicly available data
to exclude harmful content. However, since our
models mainly learn to copy text, we do not believe
they will hallucinate such text of their own accord.
",harmful data,,harmful data,2023,actions
495,"We take ethical considerations very seriously, and
strictly adhere to the ACL Ethics Policy. This pa-
per focuses on empirical evaluations on large-scale
datasets and scaled NAT models, which can be seen
as a reality check. Both the datasets and models
used in this paper publicly available and have been
widely adopted by studies of machine translation.
We ensure that the findings and conclusions of this
paper are reported accurately and objectively.
",no ethical concerns,,no ethical concerns,2023,actions 
496,"The construction of HRDsAttack involves human
annotations on AMT. The Turkers are provided
with clear annotation instructions and are informed
of the conditions where they would be qualified or
disqualified. We compensate the Turkers with a
final paid rate of $15.00 per hour which is over the
US national minimal wage of $7.50.
",no ethical concerns,,no ethical concerns,2023,actions
497,"The collection of BUMP involves human annota-
tions. The human annotators are provided with
clear task instructions and informed of the condi-
tions where they would be qualified and disqual-
ified. We compensate annotators with $3.00per
assignment in the qualification task and $0.50per
assignment in the full task for both Tasks 1 and 2.
The final paid rate is $15per hour which is over the
US national minimum wage4of$7.25. We are also
aware that our shared datasets could be potentially
misused as training samples, albeit a small number,
to develop models to generate unfaithful content.
",misuse,,data misuse,2023,actions
498,"Previous studies have shown that large pre-trained
models embed biases and might create harm to
certain populations. While MUFASSA is built with
large pre-trained models, we do not study if the
faithfulness estimation by MUFASSA is biased
towards any population in this work (e.g., produce
higher scores for texts including a population than
text including another population). As recent work
finds that BERTScore which is also based on large
pre-trained models has biases (Sun et al., 2022),
we suggest users carefully investigate the potential
biases in the model before applying it in real-world
situations.
",bias,,"bias, harmfulnsess",2023,"statement, suggestion"
499,"Collection of BIGVIDEO .We comply with the
terms of use and copyright policies of all data
sources during collection from the YouTube and
Xigua platform. User and other sensitive informa-
tion is not collected to ensure the privacy of video
creators. The data sources are publicly available
videos and our preprocessing procedure does not
involve privacy issues. For all annotation or human
evaluation mentioned in the paper, we hire seven
full-time professional translators in total and pay
them with market wage. All of our annotators are
graduates.
Potential Risks of BIGVIDEO and our model.
While BIGVIDEO consists of high-quality paral-
lel subtitles, we recognize that our data may still
contain incorrect samples. Our model may as well
generate degraded or even improper contents. As
our dataset is based on YouTube or Xigua videos,
models trained on our dataset might be biased to-
wards US or Chinese user perspective, which could
yield outputs that are harmful to certain popula-
tions.
","accuracy, bias, harmful predictions",,"data accuracy, bias, harmful predictions",2023,"actions, concerns"
500,"Our work involves the development of a platform
for annotating Sanskrit text. We believe that this
platform will be useful for people who are willing
to work with Sanskrit for research and educational
purposes. We have ensured that our platform is de-
signed ethically and responsibly. We do not foresee
any harmful effects of our platform on any com-
munity. However, we caution users to use the plat-form carefully as our pretrained models are not per-
fect, and errors can occur in the annotation process.
All our systems are built using publicly available
benchmark datasets, and we have released all our
pretrained models and source codes publicly for
future research. We are committed to transparency
and open access in our work, and we believe that
sharing our resources will benefit the wider NLP
community. We also acknowledge that NLP re-
search can have potential ethical implications, par-
ticularly in areas such as data privacy, bias and
discrimination. We are committed to continuing to
consider these ethical implications as we develop
our platform, and we welcome feedback from the
community on how we can improve our ethical
practices.
","inaccuracies, harmful predictions, privacy, bias",,"inaccuracies, harmful predictons, bias",2023,"statement, suggestion, actions"
501,"We could not find any potential harm that might de-
rive from this work. However, we understand that
translation as a whole can impact the cultural and
social life of the people that use it. This has been
used in the past in a harmful way, i.e., to spread
colonial views (Mbuwayesango, 2018). Therefore,
we call the final user to use this work ethically.
Regarding the annotation process, all manual an-
notations were made by a subset of the authors of
this paper. Therefore, no hiring of external workers
was necessary.
",research misuse,,research misuse,2023,"statement, concerns, actions, suggestion"
502,"We could not ﬁnd any speciﬁc Ethical issue for this
paper or potential danger. Nevertheless, we want
to point to the reader that working with indigenous
languages (in this case, MT) implies a set of ethical
questions that are important to handle. For a deeper
understanding of the matter, we suggest specialized
literature to the reader (Mager et al., 2023; Bird,
2020; Schwartz, 2022).",no ethical concerns,,no ethical concerns,2023,"stamement, suggestions"
503,"When collecting data in an Indigenous language,
it becomes vital that the process does not exploit
any member of the community or commodify the
language (Schwartz, 2022). Further, it is important
that members of the community benefit from the
dataset. While the creation of a word alignment
dataset will not directly impact community mem-
bers, we believe that it can contribute to the devel-
opment of tools, such as translation systems, that
can be directly beneficial, and that increasing the
visibility of these languages within the research
community will further spur the creation of useful
systems. Our annotations were created by either
co-authors of the paper or by native speakers of the
languages, who were compensated at a rate chosen
with the minimum hourly salary in their respective
countries taken into account.
",no ethical concerns,,no ethical concerns,2023,"actions, suggestions"
504,"Related to the limitations of this work, while this
work increases research potential for low-resource
languages, this comes with the main ethical risk
of potential of propagating the anglocentric bias of
some of the source datasets further.
",misuse,,misuse,2023,statement
505,"The proposed attack aims to cause misbehaviors of
neural code search models. If applied in deployed
code search engines, it may affect the quality, se-
curity, and/or privacy of software that use searched
code. Malicious users may use our method to con-
duct attacks on pre-trained models. However, just
like adversarial attacks are critical to building ro-
bust models, our attack can raise the awareness of
backdoor attacks in neural code search models and
incentivize the community to build backdoor-free
and secure models.",research misuse,,research misuse,2023,concerns
506,"The intended use of our proposed approach is re-
lated to sequence labelling tasks where there are
latency constraints and limited labelled data avail-
able. While it is not impossible to identify potential
misuses of this technology, it is not immediately
clear what those malicious uses would be. On the
contrary, this paper contributes to the body of work
investigating efficient solutions for deployment of
live systems.
",no ethical concerns,,no ethical concerns,2023,statement
507,"As the OpenAI team pointed out, GPT-2 does not
distinguish fact from fiction, so it can not support
use cases that require the generated text to be true.
Additionally, GPT-2 reflect the biases inherent to
the systems they were trained on, so it can not be
deployed into systems that interact with humans
unless the deployers first carry out a study of bi-
ases relevant to the intended use case. Though
ourMIXCE-finetuned GPT-2 gets improved per-
formance with respect to the metrics we used, the
above statement still holds. At this point, we are
not sure whether MIXCEcan help improve fac-
tuality or lead to less biased generations, but we
are sure that the generations still have non-factual
content and biases.
",bias,,bias,2023,statement
508,"The annotated dataset reported in this paper in-
volved manual effort. This is an output of the anno-
tation campaign conducted with students of linguis-
tics and informatics in order to aid in learning about
sentiment analysis as part of their coursework. The
students were compensated with course credits at
the end of the campaign.
",no ethical concerns,,no ethical concerns,2023,"statement, mention of action taken"
509,"Our proposed task aims to motivate research to-
wards understanding how social media users per-
ceive and discuss various health conditions online.
To that end, we created a dataset consisting of per-
sonal experiences, claims, and questions of Reddit
users along with key clinical elements related to the
claims. Given the nature of the dataset and related
privacy concerns, we made sure that any individual
user could choose not to be included in our corpus.
First, we notified every Reddit user whose post we
scraped, informed them about the corpus and the
intended purpose, and provided them an option to
opt-out within a period of 30 days. Second, instead
of releasing the dataset directly, we only provide
Reddit post identifiers, related annotations, and a
script to download and combine them. The script
ensures that if a user deletes their posts they can no
longer be retrieved.
",no ethical concerns,,no ethical concerns,2023,actions
510,"This work has the potential to contribute to human
well-being by supporting development of language
technologies for processing health-related social
media posts. Such models might in turn provide in-
sights about patient experiences and viewpoints in
general, and more specifically may help community
moderators identify and remove posts containing
medical misinformation.
Realizing these potentially positive contributions
requires annotated data with which to train relevant
models; such data is the main contribution on of-
fer in this work. However, releasing an annotated
corpus of health-related social media posts raises
concerns regarding individual privacy. The Reddit
posts we have assembled and collected annotations
were posted publicly on the Internet (almost always
under pseudonyms), but nonetheless we have taken
steps to ensure that individuals can choose not to
be represented in this dataset.
Specifically, we sent a message to every user in
theRedHOT explaining our intent to construct and
release this dataset and offering the option to “opt
out”. In addition, although this is not required by
Reddit, we have decided not to release the collected
posts directly. Instead we release a script that will
download the posts comprising our data on-demand
and align these with the collected annotations. This
means that if a user chooses to delete their post(s)
from Reddit, they will also effectively be removed
from our dataset. Further, we require anyone ac-
cessing this data to self-certify that they have obtain
prior approval from their own IRB concerning the
use-cases of their research.",no ethical concerns,,no ethical concerns,2023,actions
511,"Upon entering the competition, all participants of
the shared task accepted the following terms and
conditions of the competition:
•All participants agree to compete in a fair and
honest manner in the shared task and not use
any illegal, malicious, or otherwise unethical
methods to gain an advantage in the shared
task.
•All participants agree to not distribute or share
the test data obtained during the shared task
with any third parties.
•All participants agree to make their solutions
publicly available upon the completion of the
shared task in order to facilitate knowledge
sharing and developments of the Ukrainian
language.
To the best of our knowledge, the shared task par-
ticipants followed these terms and conditions.
",no ethical concerns,,no ethical concerns,2023,actions
512,"Our study focuses on the development of a neural
architecture for text editing tasks. The research
was conducted in accordance with ethical princi-
ples, and no sensitive or personal data was used or
collected during the study. The UA-GEC dataset
and corpora presented on lang.org.ua used in the
study have been obtained from public sources, and
their authors assure the privacy and confidentiality
of the original texts. The results of the study are
intended to improve the efficiency and accuracy of
text writing and may be useful for other NLP tasks.
We ensure that the study does not raise any ethical
concerns or has no negative impact on individuals
or groups.
",no ethical concerns,,no ethical concerns,2023,actions
513,"We do not expect any ethical risks caused by our
work. The datasets we use are all publicly available.
We do not annotate any data on our own. All our
datasets have user reviews in the English language.
We performed human evaluation experiments on
Amazon Mechanical Turk (AMT). The human an-
notators were compensated at a rate of $15 per hour.
During the evaluation, human annotators were not
exposed to any sensitive or explicit content.
8
",no ethical concerns,,no ethical concerns,2023,actions
514,"The work is conducted in compliance with ethical
principles. The datasets introduced in this paper
only used publicly available data. The annotation in
human evaluation was conducted by two authors of
the paper, and thus there are no associated concerns,
e.g. regarding compensation. Therefore, there are
no potential risks associated with the research.
",no ethical concerns,,no ethical concerns,2023,actions
515,"In this paper, we present a two-stage CQG frame-
work (SG-CQG), which was trained on CoQA
(Reddy et al., 2019), a published large-scale dataset
for building Conversational Question Answering
systems. Our framework is potentially helpful for
building chatbot systems, which can serve differentstreams such as educational, medical, or commer-
cial purposes.
Through human evaluations, we observe that our
proposed method does not generate any discrimina-
tory, insulting responses (questions and answers).
We validate the proposed method and baseline mod-
els on human evaluation which involves manual
labor. We hire three annotators to score 125 gen-
erated questions in total. The hourly pay is set to
S$15, which is higher than the local statutory min-
imum wage. Therefore, we do not anticipate any
major ethical concerns.
",no ethical concerns,,no ethical concerns,2023,actions
516,"We recognize and take seriously the ethical prin-
ciples of avoiding harm, trustworthiness, fairness
and non-discrimination, and privacy. We take steps
to minimize the potential negative impacts of our
research and we are committed to ensuring that the
use of our findings and technology is done in an eth-
ical and responsible manner. We are committed to
ensuring that our research and the use of machine
translation technology do not perpetuate language
biases, discrimination or any form of inequality.
",no ethical concerns,,no ethical concerns,2023,statement
517,"We address the efficiency of data annotation by
investigating learning curves to estimate the neces-
sary training sample size to reach a desired model
performance. However, it is imperative to take into
consideration the potential biases that may exist
in the model predictions when utilizing a reduced
amount of labeled data in the system construction
process. Furthermore, when addressing complex
tasks such as machine translation and text summa-
rization, it is essential to guarantee the factuality
of output generated by the system trained with the
suggested data sample size.
","bias, inaccuracies",,"bias, inaccuracies",2023,concerns
518,"We take ethical considerations seriously and strictly
adhere to the ACL Ethics Policy. This paper fo-
cuses on the higher efficiency of dynamic networks,
e.g., the mixture of experts. Both the datasets and
models used in this paper are publicly available and
have been widely adopted by researchers. We en-
sure that the findings and conclusions of this paper
are reported accurately and objectively.
",no ethical concerns,,no ethical concerns,2023,actions
519,"We strictly adhere to the ACL Ethics Policy. This
paper focuses on reducing the false positives prob-
lem of unsupervised dense retrieval. The datasets
used in this paper are publicly available and have
been widely adopted by researchers. We ensure
that the findings and conclusions of this paper are
reported accurately and objectively.
",no ethical concerns,,no ethical concerns,2023,actions
520,"We take ethical considerations very seriously, and
strictly adhere to the ACL Ethics Policy. All pro-
cedures performed in this study are in accordance
with the ethical standards. This paper focuses on
improving automatic NLG evaluations with an er-
ror analysis framework. Our proposed metric relies
on reference translations as signals and produces
scores for translations indicating their quality. Both
the datasets and models used in this paper are pub-
licly available and have been widely adopted by
researchers. Our model will not learn from user
inputs or cause potential risks to the NLP commu-
nity. We ensure that the findings and conclusions of
this paper are reported accurately and objectively.
Informed consent was obtained from all individual
participants included in this study.",no ethical concerns,,no ethical concerns,2023,actions
521,"The debiased models in our work apply to the
same general ethical considerations as other de-
biased dialogue models and normal dialogue mod-
els, which still run the risk of generating unsafe
responses. There is a development process for our
work, which includes collecting and labeling data.
In the data collection process, we collect sentences
by matching keywords to data over a manually
defined period, which has a certain degree of ran-
domness. We use three annotators to annotate the
data, and although it has some diversity, this level
of diversity does not necessarily provide true cross-
demographic fairness.
",fairness,,fairness,2023,actions
522,"This work aims at improving user experiences with
voice assistants. By allowing users to refer to enti-
ties on screen, it reduces user friction and enables
a smoother and more natural experience. No voice
assistant usage log data was used and all requests
were collected by recruited annotators.
",no ethical concerns,,no ethical concerns,2023,statement
523,"With our work, we wish to encourage further ana-
lysis of bias in machine learning models. To this
end, we provide data that enables an assessment of
a number of potential manifestations of bias. We
acknowledge that the images harbor a multitude
of different stereotypes that cannot be taken to be
representative of the various groups. Moreover, we
acknowledge that the pairings of classes of peo-
ple adopted thus far in our work leaves out other
groups of people, e.g., further forms of faith and
belief, and also further pairings. We view our work
as a step towards a more inclusive bias assessment
resource that should keep growing in the future.
",bias,,bias,2023,statement
524,"This paper focuses on the task of few-shot natural
language understanding and conducts experiments
on open datasets. The implementation details are
described in Appendix for reproduction.
",no ethical concerns,,no ethical concerns,2023,statement
525,"In this section, we briefly discuss the ethical aspects
of our experiments. We do this with regard to our
experiment as a whole.
Ethical Review
Prior to our experiment, materials and methodol-
ogy underwent ethical review by our institution’s
Ethics Board. The proposal was flagged as ethically
compliant and accepted without major revisions.
Risks
Our work inspects the annotation differences be-
tween laypeople and experts in the counselling do-
main (MI and reflections in particular). With these
premises, it could be seen as a message that therapy
can be fully automated, laypeople can replace ther-
apists in creating such systems and generative mod-
els could act as ""virtual counsellors"". We acknowl-
edge that past work inspected similar options (Fiske
et al., 2019; D’Alfonso, 2020; Saha et al., 2022),
but we take distance from it. Our work is framed
as modelling technological advancements that are
solely directed at therapist training. We foresee the
use of neural NLG as promising in counselling, but
only for supporting trainees. We also point out pre-
vious work showing why replacing mental health
practices with language models (or AI in general)
should not be considered (Le Glaz et al., 2021).
Information and Consent
Prior to starting the annotation, both laypeople and
experts received an electronic information sheet
containing details on the task, purpose of research,
workload and pay. This also included the fact that data would be made available for future research, in
accordance with data anonymisation requirements.
Upon starting the annotation, annotators were
prompted with a mandatory consent form to con-
firm their understanding of the terms and conditions
and their willingness to take part in the annotation.
Annotators were also given an email contact in
case of problems during the annotation or any other
query. Annotators were automatically prevented
from doing the annotation if they did not provide
consent.
Demographic Information of Annotators
All annotators were highly proficient in English,
which is the language of the dialogues. 5 out of the
9 laypeople were based in the Netherlands while
the other 4 resided in Italy. Among the experts, 4
were based in the UK, 1 in the Netherlands, 1 in
Hungary, 1 in Italy and 2 in Sweden.
We recruited laypeople who were known to us,
as this allowed active monitoring of the annota-
tion task, hence ensuring high quality. While this
approach is different from other standard ones
(such as using crowdsourcing platforms), we ar-
gue that the focus of this work is to understand if
fully committed laypeople can be valid annotators,
which can be challenging considering the annota-
tion quality issues that crowdsourcing platforms
suffer from (Dennis et al., 2020).
We also note that the group of laypeople is
diverse in demographics and educational back-
grounds. Specifically, the group includes people of
5 nationalities in their 20s, 30s and 40s who range
from bachelor’s student to professional with a PhD.
To verify the generalisability of our laypeople-
based evaluation, future work may replicate our
setup on crowdworkers and compare the resulting
annotations with ours.
Remuneration
The annotation workload was made explicit in the
task (a total of 5 annotation batches in each stage,
with a detailed description of what a batch consists
of). Annotators were given 30 minutes to com-
plete each annotation batch: laypeople received
19.5 USD/h, while experts received 21.6 USD/hour.
This difference is motivated by the generally higher
hourly pay of experts. The remuneration is con-
siderably (>50%) higher than the minimum wage
levels of the countries of residence of the annota-
tors. It also took most annotators much less than
30 minutes (e.g., 10 to 15 minutes) to complete
a batch, so the effective hourly remuneration was
higher than 19.5/21.6 USD.
Data Anonymisation
No personal data about the annotators was kept
stored at the end of the experiment. During the
annotation process, no annotator ever got in touch
with anyone involved in the experiments except for
the researchers.",no ethical concerns,,no ethical concerns,2023,actions
526,"Our work provides a deeper understanding of why
a transformer language model can still perform
well without positional embeddings, potentially
enabling the application of developing future trans-
formers that are greener and more cost-efﬁcient.
Inappropriate usage of our technique might have
negative societal impacts though. These include
the ethical challenges of improper text generation
and privacy issues inherent in the data collection
process. These implications apply to any natural
language processing research and are not unique to
this speciﬁc work.

","research misuse, improper predictions, privacy",,"research misuse, improper predictions, privacy",2023,concerns
527,"Our work advances the understanding of positional
embeddings adopted in almost all transformer mod-
els. In addition, our proposed new positional em-
bedding signiﬁcantly reduces energy consumption
and training cost thanks to its length extrapolation
property. Finally, our work lays the groundwork for
developing future transformers that are greener and
more cost-efﬁcient enabled by improved length ex-
trapolation. Inappropriate usage of our technique
might have negative societal impacts. These in-
clude the ethical challenges of improper text gen-
eration and privacy issues inherent in the data col-
lection process. These implications apply to any
natural language processing research and are not
unique to this speciﬁc work.

","research misuse, improper predictions, privacy",,"research misuse, improper predictions, privacy",2023,concerns
528,"Increasing the inference speed of MT can positively
impact society by giving people a fast and good
translation. This will enable people from differ-
ent language backgrounds to communicate with
each other and help remove cultural and trade bar-
riers. As demonstrated by comparing the number
of FLOPs in Table 3, our method uses fewer re-
sources compared to alternatives and thus has a
smaller carbon footprint, making it a more sustain-
able choice (Strubell et al., 2019). Furthermore,
since our method does not involve training proce-
dures or change the quality of results, we do not
introduce any societal bias (e.g. racism, sexism,
homophobia) into the translations. The latter, how-
ever, can be introduced through data in the training
of the backbone autoregressive models and NATs.
It is the task of those who train these models to
mitigate this problem. DDG vizcan also help inves-
tigate and visualize some potential harmful biases
encoded in the model like in Figure 4.
",no ethical concerns,,no ethical concerns,2023,actions
529,"We do not envisage any ethical concerns. The
dataset does not contain any personal, or person-
ally identifiable, information, the source data is
already open source, and there are no risks or harm
associated with its usage.
",no ethical concerns,,no ethical concerns,2023,statement
530,"People often express sentiment in unique and in-
teresting ways. Thus, there is large amounts of
person–person variation. Therefore, any automatic
method for sentiment analysis will achieve differ-
ent results on data from different people, from dif-
ferent domains, etc. We do not recommend the use
of automatic methods of sentiment analysis (based
on individual instances of text) to make important
decisions that can impact an individual. Instead,
it is often better to use automatic sentiment analy-
sis to determine broad trends of sentiment across
large amounts of data. Sentiment analysis, like
many other AI technologies, can be used not just
for beneficial purposes, but also to cause harm such
as using it to identify and suppress dissent. There
are several such ethical considerations that should
be accounted for when developing and deploying
sentiment analysis systems. We refer to Moham-
mad (2022, 2023) for a comprehensive discussion
of ethical considerations relevant to sentiment and
emotion analysis.
",research misuse,,research misuse,2023,suggestions
531,"Experiment 1 complied with the standards of re-
search involving human subjects. Their participa-
tion was voluntary, all of them were informed of
the nature of the task, and provided informed con-
sent before starting the experiment. With respect
of C02 consumption for the computational exper-
iments (Experiments 2 and 3), it should be noted
that we used pre-trained models and hence the im-
pact of the calculations obtained is expected to be
minimal. The experiments were run on a NVIDIA
A100 GPU, and the results were obtained in a few
minutes. Since this work is circumscribed within
basic research on artificial language modelling, no
applications or tools are to be directly derived by it
and hence, we do not think of any potential harms
or bias that can be derived from our work.
",no ethical concerns,,no ethical concerns,2023,actions
532,"This work leverages dialogues and annotations de-
veloped exclusively by English-speakers. This in-
troduces an English-centric bias with respect to the
notion of quality (and subqualities) in dialogues.
Although not evaluated in depth in this work, there
could be a chance that the models erroneously yield
lower scores to responses not conforming to En-
glish notions of quality responses.
The original dialogue dataset and generated re-
sponses were checked for personally identifiable
information or offensive content by the original
authors. Although highly unlikely, we acknowl-
edge the translations may contain offensive content
resulting from decoding.
The post-editing conducted in this work used a
crowdsourcing platform that awarded users a fair
wage according to their location.
","english-centric, bias, offensive content",,"bias, harmful predictions",2023,"concerns, actions"
533,"We state that this work complies with the ACL
Code of Ethics. We believe this work could help the
research community with new information about
clinical and biomedical PLM. The data for this
work will not be released and will be used accord-
ing to the ethical and confidentiality standards pro-
vided by the clinical institutions.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
534,"Despite having positive effects, such as promot-
ing solidarity, bringing people together, and cre-
ating social acceptance and approval, humor can
also be harmful, as it can be used as a form of
social control, a correction for deviant behaviors,
or as a way to legitimize social prejudice and
stereotypes against marginalized groups (Craw-
ford, 2003; Kuipers, 2008; Bemiller and Schneider,
2010).
Additionally, there are jokes which are not suit-
able for specific vulnerable groups, such as chil-
dren, due to their possibly problematic content, e.g.
sexual relations, pedophilia, harassment, xenopho-
bic stereotypes, etc.
Therefore, it is important to bear these aspects
of humor in mind throughout the whole research,
including the data collection and the development
of our methods. This will help to bring awareness
and raise questions about how these systems, cor-
pora, and resources might affect society not only
in a positive light but also from a critical point of
view.
",research misuse,,research misuse,2023,concerns
535,"ITCC contains 24 documents from METU Turkish
Corpus (Say et al., 2002). All necessary
permissions have been obtained to use anddistribute these documents.
",no ethical concerns,,no ethical concerns,2023,statement
536,"
The undergraduate students in the annotation pro-
cess are Vietnamese native speakers; have at least
12 years of studying Vietnamese with average
scores on the Vietnam National Exam on Liter-
ature of 6.5; have at least three years of using social
network platforms. They were explicitly warned
that their tasks will display hateful and offensive
content and if they became overwhelmed, they were
also urged to stop labeling. These undergraduate
students were paid $0.1 per comment, which takes
an average of 6.44 seconds to complete (excluding
the time used by workers who took exceptionally
lengthy comments).
All the comments in ViHOS originated in the
study of Luu et al., 2021, which preserved users’
anonymity by removing all of them when creat-
ing the ViHSD. As a result, the comments in our
dataset do NOT reflect our thoughts or viewpoints.
ViHOS is available to the public under a usage
agreement for research and related purposes only.
",no ethical concerns,,no ethical concerns,2023,actions
537,"The ethical considerations of this work are re-
lated to the data that we used and the models we
built. The data was extracted from administrative
and clinical records from an insurance and health
provider that specialized in labor accidents. Within
this data, it is possible to find personal and sensitive
information such as personal and company names,
addresses, health information, pre-existing condi-
tions, and diagnoses, among others. An anonymiza-
tion process was not carried out since the model
will be used for internal purposes and will not be
released. As a process of memorization can occur
in the PLM, we believe it is best to keep the model
private because privacy attacks can extract personal
and sensitive information.
We did not test the models for any bias under any
protected field. Therefore, the trained models could
benefit certain patients or accidents over others
in the insurance decision. If a biased model is
deployed in this provider’s systems, it could harm
patients with their insurance coverage decisions.
","privacy, bias",,"privacy, bias",2023,"concerns, actions"
538,"We use a freely available dataset under a Creative
Commons license to create our new dataset. The
dataset has been used only for academic purposes,
and in complete compliance with the license. The
dataset created in this work will be made available
only after filling and signing an agreement declar-
ing that the data will be used only for research pur-
poses. The annotation for manual evaluations was
done by human experts, who are regular employees
of our research group and are paid in accordance
with the institute’s policy. There are no other issues
to declare.",no ethical concerns,,no ethical concerns,2023,actions
539,"We recognize that there are substantial stakes in-
volved in developing computational models for use
in counseling applications and this fact necessitates
careful attention to ethical issues. It is crucial for a
counseling dialogue system to render support and
suggestions to the clients while maintaining their
privacy. Thus, proper care has been taken to main-
tain the client’s privacy. Despite the fact that we
have used publicly available counseling conversa-
tional datasets, the annotators pledged not to con-
tact or deanonymize any of the users or share the
data with others. Further, it is to be noted that this
paper does not make any therapy recommendations
or clinical diagnostic claims.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
540,"We have conducted an IRB for KAGAS human
evaluation.
",no ethical concerns,,no ethical concerns,2023,statement
541,"We consider our approach to have low ethical risks
since we do not utilize any biases. Our ap-
proach could be extended to any natural language
generation task and does not constraint the in-
put/output structure. We therefore conclude that
our method would not bring any harmful ethical
impact.
",no ethical concerns,,no ethical concerns,2023,statement
542,"To ensure that our dataset does not involve any
potential risks, we ask crowd-workers to check if
the generated utterances contains any of following:
1. offensive, sexist, or racist comments, 2. toxic
words, 3. sexual behavior. Detailed description,
payment, and the interface to collect all human
evaluations for this work can be found in Section F
and Figure 8 in the Appendix.
",no ethical concerns,,no ethical concerns,2023,actions
543,"All datasets and models used in the experiments
are from the publicly available website or Github.
",no ethical concerns,,no ethical concerns,2023,statement
544,"Our work is approved by our institution’s human re-
search ethics committee to conduct human-centric
or ethics-related experiments, e.g., crowdsourc-
ing and human evaluations. Topic-wise, our re-
search develops a knowledge graph of common-
sense knowledge about personas to augment un-
derstanding of characters and their interactions in
diverse narratives. Given that some of the attributes
are extracted from previous KGs or generated by
LMs, we cannot guarantee our knowledge graph
does not contain attribute alignments with negative
connotations that could provide undesired informa-
tion to a downstream system. However, we took
the following steps to mitigate this effect. First,
the set of personas we include in PEACOKwas
manually filtered to not include stereotypical and
harmful roles, thereby limiting the negative asso-
ciations of the personas themselves. Second, we
explicitly prompted the LM to generate optimistic
attributes about personas, which has been shown in
prior work to reduce the toxicity of outputs (Schick
et al., 2021). Finally, each attribute in PEACOK
is explicitly validated by two human workers for
toxicity, providing a final opportunity for workers
to flag problematic content. However, we acknowl-
edge that none of these safeguards are perfect, as
language models may still produce toxic outputs
and annotators may have differing opinions on what
constitutes toxic content (Sap et al., 2022).
","harmful predictions, annotation bias",,"harmful predictions, annotation bias",2023,"concerns, actions"
545,"While we do not believe the data we use to train the
error-correcting models to be sensitive, the models
can be applied in sensitive settings where an incor-
rect edit may cause an issue. As such, corrections
may introduce both stylistic or semantic changes
based on either the biases found in the pretrained
models or the curated error corpora.
In particular, we have noticed a bias in the
subword-based models for entities, such as loca-
tions, being overcorrected to a different entity if
there is a spelling mistake in the input.
The stylistic changes found in the curated Ice-
landic corpora may reflect on the socio-economic
background of the annotators and writers of the
data. While we don’t believe this to be a large issue
in this particular setting, one can easily imagine
this to be more complex in regions where language
use is connected to disputes or oppression. As
such, a text correction or improvement tool could
be used to homogenize discourse or otherwise limit
freedom of expression, knowingly or unknowingly.
",bias,,bias,2023,"concerns, actions"
546,"There are several significant limitations of the
MASSIVE dataset and of our modeling. Starting
with the dataset, the per-language data quantities
are relatively small at 19.5k total records and 11.5k
records for training. Second, there are some low-
quality utterances, both in the seed data and in
the translations. For the most part, these are sur-
faced through the judgment scores we provide for
each record, but if a user does filtering based on
these judgments, then the data size decreases even
further. Third, the data were originally created
through crowd-sourcing, not from a real virtual as-
sistant, which introduces artificialities. Relatedly,
allowing the worker to decide on translation versus
localization of slot entities added further noise to
the dataset, although we try to store this decision
in the metadata. Fourth, our labeling schema is
relatively simple when compared with hierarchical
labeling schemata or flat schemata with more intent
and slot options. Fifth, our collection system did
not have a robust method to preserving or denot-
ing native tokenization practices—some languages
do not separate with whitespace, while others do
but there is no set practice. This results in poten-
tially easier (larger chunks to predict slot labels) or
harder (each character individually predicted) tasks.
Sixth, it’s possible, though unlikely, that some of
our new crowd-sourced records may contain toxic
or otherwise objectionable content. We performed
analyses to check for such malicious activities and
did not find any as such. Regarding modeling, we
have only investigated base-sized models in rela-
tively standard setups, leaving room for much more
sophisticated modeling. The risks associated with
this dataset and work are relatively low, given that
we have released a research dataset meant to pro-
mote better multilinguality in NLP systems.
",no ethical concerns,,toxic content,2023,"concerns, actions"
547,"While our proposed V AG system involves genera-
tion, it does not have the general ethical concern
of generation, i.e., outputting biased or discrimina-
tive texts, because the final output of the system
is retrieved from the label pool which is highly
controllable. For our experiments, we use public
datasets and believe that none of them contains
offensive contents. Also, although the training of
the V AG system requires computational resources,
the CIL paradigm is resource-efficient because the
model preserves the previously learned knowledge
and continually learns new classes.
",no ethical concerns,,no ethical concerns,2023,actions
548,"This paper presents a large-scale multi-turn dia-
logue dataset towards multi-modal open-domain
conversation MMDialog. Data in MMDialog are
collected from a public worldwide social media
platform on which users can converse with each
other and share their daily lives messages freely in
multiple modalities, including plain text, photos,
or even videos. The data-collecting API is only
available for academic purposes. And to protect
the privacy and security of data, users and platform,
MMDialog is just released under strict terms for
academic people only. This action fully aligns with
the data using and sharing regulations of source
data providers. Thus, there will not be any ethical
problems or negative social consequences from the
research. The proposed method does not introduce
ethical/social bias in the data.
",no ethical concerns,,no ethical concerns,2023,actions
549,"FocusL aims to to convey correct knowledge to
users rather than misleading hallucinations. We
hope to see a reliable and trustworthy dialogue sys-
tem impact from better guiding the model’s learn-
ing focus. However, even if the dialogue system
does not produce hallucinations, there is still a risk
of potential misuse. For example, the dialogue sys-
tems may be used to spread misinformation or to
mislead users. If possible, we would prefer that the model itself has the ability to identify undesirable
knowledge and block it.
",misuse,,misuse,2023,"statement, concerns"
550,"Here we briefly discuss some ethical concerns of
using such compressed models in the real world,
specifically the two techniques used in this work,
quantization and knowledge distillation. Hooker
et al. (2020) have found that compressed models
can amplify existing algorithmic bias and perform
very poorly on a subset of samples while the av-
erage out-of-sample accuracy is maintained close
to the uncompressed model. This general finding
for pruning and quantization may be also extrap-
olated to our work (including distillation), hence
it is important to recognize that our work, much
like the remaining literature on compression, may
have ethical concerns with regards to algorithmic
bias and how that effects downstream tasks. How-
ever, smaller models are more cost-efficient and
thus become more widely available to the general
public. To summarize, it is important to analyse any
aforementioned bias amplification for subsets of
samples for downstream tasks compressed models
are used for.
",bias,,bias,2023,concerns
551,"All data of MMSD2.0 come from the MMSD
dataset Cai et al. (2019), which is an open-source
dataset available for academic research. Our anno-
tation process was carried out by annotators who
were postgraduate students at Chinese universities
and they were paid properly.
",no ethical concerns,,no ethical concerns,2023,statement
552,"All proposed tasks aim at increasing the efficiency
of judges instead of helping the judges make de-
cisions. Extracted or classified information will
be further checked by judges and we only provide
techniques to serve as an auxiliary tool. All source
files of our datasets are from the official legal docu-
ment website and are properly anonymized. We do
not analyze the content of the case or the litigants
in any way other than provide tool for judges.
",no ethical concerns,,no ethical concerns,2023,statement
553,"The immersive experience brought by space audio
may make people indulge in the virtual world.
","virtual world indulgence
",,"virtual world indulgence
",2023,statement
554,"This paper proposes a method to exploit fine-
grained aspect/topic-relations between documents
and construct topic-diversified dialogues to en-
hance retrieval-free dialogue systems. The doc-
uments we used in this paper and the generated
dialogues have been carefully filtered to make sure
there is no offensive and toxic information.
",no ethical concerns,,no ethical concerns,2023,statement
555,"FluentSpeech improves the naturalness of edited
speech and promotes the automatic stutter removal
of stuttered speech, which may cause unemploy-
ment for people with related occupations. Besides,
the free manipulation of speeches may bring po-
tential social damage. Further efforts in automatic
speaker verification should be made to lower the
aforementioned risks.
","unemployment, social damage",,"unemployment, social damage",2023,"concerns, actions"
556,"RMSSinger provides a high-quality realistic-music-
score-based singing voice synthesis method, which
may cause unemployment for people with related
occupations. Furthermore, the possible misuse of
realistic music scores from the website may lead to
copyright issues. We will add some constraints to
guarantee people who use our code or pre-trained
model would not use the model in illegal cases.
","unemployment, research misuse",,"unemployment, research misuse",2023,"concerns, actions"
557,"CLAPSpeech improves the prosody of the synthe-
sized speech, which may cause unemployment for
people with related occupations. Besides, the pro-
duction of fake speeches may cause voice security
issues. Further efforts in automatic speaker verifi-
cation should be made to improve voice security.

","unemployment, security issues",,"unemployment, security issues",2023,"concerns, suggestions"
558,"The open-source benchmark dataset ESC ONV (Liu
et al., 2021) used in our experiments is well-
established and collected by employed crowd-
sourced workers, with user privacy protected and
no personal information involved. And for our hu-
man evaluation, all participants are volunteered and
transparently informed of our research intent, with
reasonable wages paid.
Moreover, our research only focuses on building
emotional support systems in daily conversations,
like the one to seek the emotional support from our
friends or families. It is worth to mention that we
do not claim to construct chatbots that can provide
professional psycho-counseling or professional di-agnosis. This requires particular caution and fur-
ther efforts to construct a safer emotional support
system, which is capable of detecting users who
have tendencies of self-harming or suicide.
",no ethical concerns,,no ethical concerns,2023,actions
559,"The datasets used in this study were those pro-
duced by previous researchers, and we followed
all relevant legal and ethical guidelines for their
acquisition and use. Furthermore, we recognize the
potential moral hazard of visual temporal-aligned
translation tasks, such as their use in surveillance
or listening. We are committed to conducting our
research ethically and ensuring that our research is
beneficial.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
560,"Prosody-TTS lowers the requirements for high-
quality and expressive text-to-speech synthesis,
which may cause unemployment for people with
related occupations, such as broadcasters and radio
hosts. In addition, there is the potential for harm
from non-consensual voice cloning or the genera-
tion of fake media, and the voices of the speakers in
the recordings might be overused than they expect.
","unemployment, misinformation, privacy, overuse",,"unemployment, misinformation, privacy",2023,concerns
561,"We adopt the widely-used datasets that were pro-
duced by previous researchers. We followed all
relevant legal and ethical guidelines for their acqui-
sition and use. Besides, we recognize the potential
influence of our technique, such as its application in
human-computer interaction and vision-language
grounding system. We are committed to conduct-
ing our research ethically and ensuring that our
research is beneficial. We hope our work can in-
spire more investigations for the domain adaptation
on multi-modal tasks and wish our framework can
serve as a solid baseline for further researches.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
562,"We adopt the widely-used datasets that were pro-
duced by previous researchers and followed all
relevant legal and ethical guidelines for their acqui-
sition and use. Besides, we recognize the potential
influence of our technique. When deployed our
approach will have to record, store and process
video and audio information related to human ac-
tivities, which will have privacy implications for
some application domains. We are committed to
conducting our research ethically and ensuring that
our research is beneficial. We hope our work can
inspire more investigations for transfer learning
on multi-modal tasks and wish our framework can
serve as a solid baseline for further research.
",privacy issues,,privacy issues,2023,"statement, actions"
563,"We adopt the widely-used datasets that were pro-
duced by previous researchers. We follow all rele-
vant legal and ethical guidelines for their acquisi-
tion and use. Besides, we recognize the potential
influence of our technique, such as its application in
human-computer interaction and vision-language
grounding systems. We are committed to conduct-
ing our research ethically and ensuring that our
research is beneficial. We hope our work can in-
spire more investigations for spoken video ground-
ing and wish our framework can serve as a solid
baseline for further research.",no ethical concerns,,no ethical concerns,2023,statement
564,"U.S.-Centric Perspectives The authors of this
work are based in the U.S., and our framing in this
work, e.g., references to minority identity groups,
reﬂects this context. This viewpoint is not univer-
sally applicable and may vary in different contexts
and cultures.
Misuse Potential In this paper, we showed that
hyperpartisan LMs are not simply achieved by pre-
training on more partisan data for more epochs.
However, this preliminary ﬁnding does not exclude
the possibility of future malicious attempts at cre-
ating hyperpartisan language models, and some
might even succeed. Training and employing hyper-
partisan LMs might contribute to many malicious
purposes, such as propagating partisan misinforma-
tion or adversarially attacking pretrained language
models ( Bagdasaryan and Shmatikov ,2022 ). We
will refrain from releasing the trained hyperparti-
san language model checkpoints and will establish
access permission for the collected partisan pre-
training corpora to ensure its research-only usage.
Interpreting Downstream Task Performance
While we showed that pretrained LMs with dif-
ferent political leanings could have different perfor-
mances and behaviors on downstream tasks, this
empirical evidence should not be taken as a judg-
ment of individuals and communities with certain
political leanings, rather than a mere reﬂection of
the empirical behavior of pretrained LMs.
Authors’ Political Leaning Although the au-
thors strive to conduct politically impartial analysis
throughout the paper, it is not impossible that our
inherent political leaning has impacted experiment
interpretation and analysis in unperceived ways.
We encourage the readers to also examine the mod-
els and results by themselves, or at least be aware
of this possibility.
","bias, misuse",,"bias, misuse",2023,"concerns, suggestions, actions"
565,"KALM is a knowledge-aware long document un-
derstanding approach that jointly leverages pre-
trained LMs and knowledge graphs on three levels
of contexts. Consequently, KALM might exhibit
many of the biases of the adopted language models
(Liang et al. ,2021 ;Nadeem et al. ,2021 ) and knowl-
edge graphs ( Fisher et al. ,2020 ,2019 ;Mehrabi
et al. ,2021 ;Du et al. ,2022 ;Keidar et al. ,2021 ).
As a result, KALM might leverage the biased and
unethical correlations in LMs and KGs to arrive
at conclusions. We encourage KALM users to au-
dit its output before using it beyond the standard
benchmarks. We leave it to future work on how to
leverage knowledge graphs in pretrained LMs with
a focus on fairness and equity.
",bias,,bias,2023,"statement, suggestions"
566,"Data Collection and Privacy Our data collection
is in compliance with Twitter’s terms of service and
matches previous publications. Although tweets
are public, when releasing data, we will share user
id or tweet id rather than raw data, to minimize the
privacy risk.
Political Leaning Since political identity is be-
coming increasingly important in American society,
the models could come up with some risks if a user
is mislabeled with an error affiliation, e.g., a user
may be socially ostracized for their supposed po-
litical beliefs (Alkiek et al., 2022). However, the
research subject in this paper is public political
actors, which have been studied in political sci-
ence for decades, rather than the common public.
Instead, understanding the bias and behaviors of
these characters can help our public avoid being
polarized by their certain strategies, mitigating the
potential risk.
","privacy, bias",,"privacy, bias",2023,"concerns, actions"
567,"Our dataset of tweets was obtained by scraping
Twitter. All tweets have been anonymized, and
metadata such as senders’ demographic identity is
never included in the data used to train our models.
We plan to release only the tweet ids and their
respective labels for the two tasks as part of our
dataset.
",no ethical concerns,,no ethical concerns,2023,statement
568,"Our dataset of tweets was obtained by scraping
Twitter. We also obtain a subset of data from ex-
isting aggressive detection datasets cited in this
paper, complying with the terms of use of each
of these datasets. All datasets were anonymized,
no tweet-ids or Twitter usernames or any of their
demographics are included in the data used to train
our models. We plan to release only the tweet ids
as part of our dataset, along with the labels, in the
final version.
",no ethical concerns,,no ethical concerns,2023,statement
569,"The dataset we use in this paper is from the public
ADReSS challenge, which contains the minimum
amount of personal information and restricts unau-
thorized access. Data usage and data sharing for
ADReSS data has been conducted in accordance
with the Ground Rules and Code of Ethics. Further-
more, it is important to note that the study does not
include all possible diagnoses of Alzheimer’s dis-
ease since it is based on transcript text data from an
English-speaking cultural context. As this model
was designed primarily for academic research, it
is unlikely to provide a valid diagnosis in every
situation and will be risky if applied to real-world
clinical diagnosis situations.
",no ethical concerns,,no ethical concerns,2023,statement
570,"Currently, large language models with retrieval aug-
mentation require a large amount of computation
in indexing a large-scale datastore, retrieving from
that large datastore and refreshing index during
training. Despite improving model performance,
the retrieval augmentation methods need too much
computation power. This not only limits the usabil-
ity of such models but also harms the fairness in
this community. Our work tries to balance the per-
formance of retrieval augmentation methods and
the training cost, in that our method does not need
to retrain a new retriever and rebuild an index when
facing a new task. This may help the community
in developing new low-cost methods.
During selecting the external datastore and tasks,
we follow previous studies and choose the well-
known Wikipedia dataset and common tasks. Bi-
ases from the data may be reflected in the results.
In addition, when using the model on a larger scale,
more consideration needs to be paid to deal with
biases in retrieved text.
","fairness, bias",,"fairness, bias",2023,"concerns, suggestions"
571,"The domain and data we work with don’t involve
any personal information and are all publicly avail-
able. However, as the work could be potentially
applied in the medical domain to resolve mentions
of disease, discretion is advised when any medical
decisions or diagnostics are made with the assis-
tance of the model.
",no ethical concerns,,no ethical concerns,2023,statement
572,"For a user’s question, our knowledge-augmentation
scheme can allow prompted LMs generate a fac-
tually correct answer, grounded by the provided
knowledge, for KGQA tasks. However, the per-
formance of our KAPING framework is still far
from perfect, due to potential failures in entity link-
ing, fact retrieval, and knowledge generation itself.
Thus, we should be aware whether LMs generate
correct answers, especially on high-risk domains.
",misinformation,,misinformation,2023,statement
573,"Our work aims to contribute and extend research
regarding hate speech detection in social media and
particular in Twitter. We believe that our efforts
to contribute on the ongoing concerns around the
status of hate speech on social medial.
We acknowledge the importance of the ACM
Code of Ethics, and are committed on following it’s
guidelines. Our current work, uses either publicly
available tweets under open licence and does not
infringe any of the rules of Twitter’s API. Moreover,
given that our task includes user generated content
we are committed to respect the privacy of the users,
by replacing each user mention in the texts with a
placeholder.
",no ethical concerns,,no ethical concerns,2023,statement
574,"For a user’s question, our knowledge-augmentation
scheme can allow prompted LMs generate a fac-
tually correct answer, grounded by the provided
knowledge, for KGQA tasks. However, the per-
formance of our KAPING framework is still far
from perfect, due to potential failures in entity link-
ing, fact retrieval, and knowledge generation itself.
Thus, we should be aware whether LMs generate
correct answers, especially on high-risk domains.
",misinformation,,misinformation,2023,"statements, concerns, suggestions"
575,"Our paper consider a theoretical aspect of influence
functions. It does not have any biases toward any
groups of people. Our findings do not cause any
harms to any groups of people.
",no ethical concerns,,no ethical concerns,2023,statement
576,"We use adversarial attack as a tool to evaluate the
robustness of visual dialog models. However, the
same techniques can also be used to maliciously
attack the system. Our experiments demonstrate
that most synonym-based attacks are successful in
remaining undetected by humans. However, our
results also show that the most effective attacks are
also the ones which are easiest for humans to detect.
Further work is thus needed to automatically detect
malicious attacks, e.g. using our proposed gram-
maticality and contextual multimodal methods.
",misuse,,misuse,2023,"concerns, suggestions"
577,"While our work primarily centers on mitigating bias
in Large Language Models (LLMs) using prompt
templates, it also acknowledges the inherent risk
shared by BERT, RoBERTa, and GPT models, as
they may potentially generate biased language.
",bias,,bias,2023,concerns
578,"This work focuses on current practice in fairness
evaluation and method comparison. Our proposed
“checklist” recommendations are specific to the
fairness literature and complement existing frame-
works, to encourage future research to think care-
fully about harms and what type of fairness is ap-
propriate.
Demographics are assumed to be available only
for evaluation purposes and are not used for model
training or inference. We only use attributes that
the user has self-identified in our experiments. All
data and models in this study are publicly available
and used under strict ethical guidelines.
",no ethical concerns,,no ethical concerns,2023,statement
579,"We utilized the pre-collected in-domain unlabeled
text corpora to explore the domain-adaptation pre-
training approaches with efficiency concerns in this
work. Although we carefully consider the data
distribution and the selection procedures, the pre-
collected background sets for each domain might
introduce the potential risk of sampling biases.
Moreover, (pre)training, as well as fine-tuning of
large-scale PTLMs, might pose a potential threat
to the environment (Strubell et al., 2019): in light
of the context, the task-agnostic domain adaptation
approaches we introduced are aimed at mitigating
towards the directions of reducing the carbon foot-
print of pretrained language models.
","bias, environmental_impact",,"bias, environmental_impact",2023,"concerns, actions"
580,"The additional data that we have used for the
pre-training process are from the MIMIC dataset,
which meets the ethical requirements of Patient
Health Information.
",no ethical concerns,,no ethical concerns,2023,statement
581,"In ethical considerations, our method risks being
used as a means of model stealing. Therefore, de-
fensive techniques against the proposed method
are required. However, it also has significant posi-
tive implications. On the one hand, it can serve as
a powerful tool for research on model extraction
attacks, thereby promoting the advancement of re-
lated studies. On the other hand, it has practical
applications in real-world scenarios. For instance,
a company may prefer to use a smaller model due
to cost considerations, and our method allows for
the easy distillation of smaller models without re-
quiring white-box access to larger models. Addi-
tionally, our method can be used to distill non-NN
models into NN models, reducing the number of
model types that need to be maintained and simpli-
fying operation and maintenance.
",misuse,,misuse,2023,"concerns, suggestions"
582,"Annotator Wellbeing As outlined in §3.2, we
followed guidelines by Davani et al. (2022) to pro-
tect the wellbeing of our annotators. Annotators
were clearly informed about the nature of the an-
notation task before commencing their work. They
completed their work in batches, on their own
schedules, and could decide to withdraw from the
work at any point. Compensation for annotators
was well above the living wage in their countries
of residence, at $16 per hour. We do not release
identifiable information about our annotators.
Data Privacy We used Reddit data made publicly
available via the Pushshift API (Baumgartner et al.,
2020) rather than scraping any new data ourselves.
Comment author usernames are anonymised by
replacing them with alphanumeric IDs.
environmental_impact We only trained a hand-
ful of models in our experiments, and did not per-
form any hyperparameter tuning. Relative to the
concerns raised around the environmental costs of
pre-training large language models (Strubell et al.,
2019; Henderson et al., 2020; Bender et al., 2021),
or even larger-scale fine-tuning with hyperparame-
ter tuning, we therefore consider the environmental
costs of our work to be relatively minor.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
583,"We are using neither large pretrained language mod-
els that have been found to be prone to bias issues
nor uncurated data scraped from the internet that
would open up myriads of problems. Still, there
could be some bias in the PhotoBook data that
should be investigated: players might have used
offensive or undesirable language in describing im-
ages. Therefore, deploying these speakers and lis-
teners directly is not advisable. Our research focus-
ing on the adaptation of a speaker to their audience
is done with the aim of improving communicative
success within scenarios with knowledge asymme-
try following human capabilities of self-monitoring
and Theory of Mind. It is possible that adaptation
to a specific listener could exacerbate possible bi-
ases if the training set of a given listener happens
to include more bias. However, the reverse is also
the case, where adaptation to underrepresented user
groups could be beneficial.
",bias,,bias,2023,"concerns, actions"
584,"As sociodemographic attributes are sensitive infor-
mation, we do not infer attributes, but build on a
self-reported, IRB-reviewed dataset (Kumar et al.,
2021). We also see potential for a discussion of
“privacy by design” in modelling human label vari-
ation based on our results: There can be circum-
stances in which knowing more about annotators is
not relevant, and indeed might lead to violations of
privacy.
As multi-annotator models attempt to capture
the preferences of individual annotators, there are
valid concerns around privacy and anonymity. As
discussed in Davani et al. (2022), increasing the
annotator count can be one option to reduce privacy
risks. We show it is feasible to learn a model for
a large number of individual annotators (5002 vs.
18 and 82 in their work). But a prerequisite for
improved privacy is to apply effective aggregation
on top of individual predictions, which we do not
study in the present work.
",privacy,,privacy,2023,concerns
585,"The authors foresee no ethical concerns about the
work presented in the paper.


",no ethical concerns,,no ethical concerns,2023,statement
586,"We release our corpus of tweets annotated with
stance, and our dataset of trending misinforma-
tion claims under Twitter’s Developer Agreement,8
which grants permissions for academic researchers
to share Tweet IDs and User IDs (less than
1,500,000 Tweet IDs within 30 days) for non-
commercial purposes, as of October 10th. 2022.
Our system is designed for research purposes
and may contain unknown biases towards demo-
graphic groups or individuals (Sap et al., 2019).
Further investigation into systematic biases should
be conducted before our models are deployed in a
production environment.
We believe this study helps shed light on how
NLP tools developed to help combat online misin-
formation might be used in a real content modera-
tion workflow. We hope this will encourage future
research on human-in-the-loop systems and help
shape the design of new tasks and datasets in this
area. We believe it is beneficial for some research
on combating misinformation to take place outside
of social media companies to provide an unbiased
view of the challenges involved in fighting online
misinformation.
",bias,,bias,2023,"statement, concerns"
587,"We took care of potential license issue of the data
underlying our dataset by exclusively selecting
open-access articles published under CC BY .
The main annotator was paid above the mini-
mum wage of our country in the context of a full-
time internship. The annotator was aware of the
goal of the study and consents to the public release
of the data. The remaining domain experts partici-
pated on a voluntary basis due to their interest in
the topic.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
588,"We propose a method that can significantly reduce
the financial and environmental cost of language
model learning. By reducing the need for data
collection and human labeling, our method can ef-
fectively protect user and data privacy by avoiding
leaking any information while building the training
corpora. We found that a medium-sized language
model can achieve similar performance as the state-
of-the-art large-scale language models, suggesting
that we can cost less financially and environmen-
tally during model training and evaluation for com-
parable performance. However, since we reduced
the need for human-labeling efforts, the deploy-
ment of the system might decrease the number of
data annotation jobs.
",unemployment,,unemployment,2023,"statement, concerns"
589,"We investigate the stereotypes and biases of pre-
trained language models and introduce the less bi-
ased textual entailment models that reduce bias on
gender, profession, religion, and race. We noticed
that the existing gender-related bias studies and cor-
pora mainly focus on the binary gender setting, and
we also follow this line of research because of data
limitations. While such data limitation might dis-
appoint a number of communities, we will extend
this work to non-binary settings in future work.
",bias,,bias,2023,statement
590,"We anticipate no ethical issues directly stemming
from our experiments. However, as with all dis-
tributional semantic models, our trained model is
likely to have picked up social biases present in the
training corpus. Any real-world application of a
trained model would need to mitigate risks due to
such biases.
",bias,,bias,2023,"statement, suggestions"
591,"We acknowledge and ensure that our study is
compatible with the provided Code of Ethics. Knowledge-grounded open-domain dialogue gen-
eration is crucial for building a knowledgeable dia-
logue system, which is beyond the wildest dreams
in natural language process field. All our experi-
ments are conducted on public available datasets
to avoid ethical concerns. All terms for using these
datasets are strictly followed in our study. There
are no direct ethical concerns in our research.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
592,"Private Information
We carefully remove all personal information
through the data cleaning process: First, we do
not include any account information during the
data collecting procedure, which means all the data
are anonymous. Second, we clean the potential
private information such as emails, ID numbers,
phone numbers, etc. in the data to further ensure
the privacy.
4https://openai.com/blog/chatgpt/Offensive Content
We have taken two steps to avoid offensive content
inNEWSDIALOGUES . First, we ask the annotators
not to speak offensive content during the conversa-
tions. Second, we manually check all conversations
after data collection and throw away the conversa-
tions including offensive content.

",no ethical concerns,,no ethical concerns,2023,"statement, actions"
593,"AutoConv is based on large language models
(LLM), while LLM has some potential risks, e.g.,
social bias (Liang et al., 2021), offensive content
(Ganguli et al., 2022) etc. Fortunately, we fine-
tune the LLM to capture the characteristics of the
information-seeking process, and the generated
conversations are mostly grounded on the provided
documents (take an example in Table 5). Therefore,
our method alleviates the potential risks of directly
using LLM. According to our manual check in error
analysis (Section 3.8), we do not find any harmful
content in the synthetic conversations. In addition,
we also encourage considering more safety meth-
ods (Xu et al., 2020; Sun et al., 2022) to guarantee
the quality of synthetic conversations.
",no ethical concerns,,no ethical concerns,2023,"statement,actions"
594,"In this work, our CoRe shows impressive rea-
soning capability, however, it also comes with
social risks. Here, we summarize three possible
ethical impacts: i) PLMs with bias, ii) gener-
ated data with social stereotypes and iii) prob-
lematic data environments. Considering utilizing
PLMs as backbones, several works present var-
ious potential risks in PLMs ( Lucy and Bam-
man,2021 ;Amin and Kabir ,2022 ). Fortunately,
our method supports the replacement of different
PLMs. Therefore, we encourage deploying some
risk-free PLMs, expecting to reduce the potential
ethical risks. Furthermore, once deploying harm-
ful PLMs, the self-thinking process might gener-
ate several undesired data and those data are fed
into language models, which deepens the bias and
causes unintended social impacts. For reducing the
aforementioned cases, we suggest recording gener-
ated sentences. In real-world applications, a good
choice is to monitor generated content and then
hand them over for human review. In addition to
the two risks posed by PLMs, the data in down-
stream tasks is of great concern. In particular, pri-
vate data might cause unpredictable inﬂuence be-
cause of their nature as a non-open source. There-
fore, we believe that a data cleaning workﬂow is
necessary to mitigate potential risks, such as Pri-
vateClean ( Krishnan et al. ,2016 ). Finally, we en-
courage open debating about its utilization for in-
creasing transparency and reducing the potential
for misuse.
",bias,,bias,2023,"concerns, suggestions"
595,"This work is centered on ensuring the best expe-
riences are served by a conversational AI through
learning and validation of customer initialed re-
ports. Therefore, we do not assess any particular
ethical risks associated with this work. However,
one penitential though unlikely risk area would be
human expert decisions for data collection to be
biased on certain use-cases or interactions. We did
not observe manifestation of such risk impacting
our experiments and after the production deploy-
ment. Regarding human data handling practices,
we ensured anonymity of data samples used in this
study and did not reveal any specifics that would
violate our internal policies or our customer privacy
policies.
",bias,,bias,2023,statement
596,"The presented work is focused on improving robust-
ness of off-policy bandit updates in conversational
systems by introducing robustness constraints on
the policy behavior. We do not believe there is
any additional risk associated with this work when
using the suggested platform on constraints that en-
courage controlled deviations from a current base-
line. Regarding human data handling practices, we
ensured anonymity of data samples used in this
study and did not reveal any specifics that would
violate our internal policies or our customer privacy
policies.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
597,"Our goal is to diagnose factual consistency of ﬁne-
tuning based summarization models and prevent
models from providing human with misleading in-
formation. By connecting model intrinsic features
and factual consistency, we aim to improve explain-
ability and faithfulness of summarization models.
One of major concerns is that there are still aspects
of the model which are not explored, such as scal-
ing law of factual consistency. Continuous studies
on model intrinsic features would result trustful
summarization models which we leave as future
work.
",no ethical concerns,,no ethical concerns,2023,statement
598,"Our method is used to compress VLMs. Therefore,
ethical considerations of VLMs generally apply to
our method. We encourage users to assess potential
biases before deploying EfficientVLM.
",bias,,bias,2023,"concerns, suggestions"
599,"We perform our experiments on existing hyperbole
and metaphor datasets by adding additional labels
to them. Some of the examples in these datasets
use slurs, abuses, and other derogatory terms to
bring out exaggeration or implicit comparison. Our
models may also propagate these unintended bi-
ases due to the nature of the datasets. We urge the
research community to use our models and these
datasets with caution and we are fully committed to
removing discrepancies in the existing hyperbole
and metaphor datasets in the future.
",bias,,bias,2023,"concerns, suggestions"
600,"We foresee no ethical issues arising from this re-
search. The reader may refer to Brown (1973)
for information on how the Adam and Eve child-
directed speech corpora were collected.
",no ethical concerns,,no ethical concerns,2023,statement
601,"Experiments presented in this work used datasets
from previously published research (Pradhan et al.,
2012; Marcus et al., 1993), in which the proce-
dures for data collection, validation, and cleaning
are outlined. These datasets were used to study
a large language model’s predictions about coref-
erence resolution and dependency parsing respec-
tively, which is consistent with their intended use.
As this work focuses on studying the factors un-
derlying the predictions of large language models,
its potential risks and negative impacts on society
seem to be minimal.
",no ethical concerns,,no ethical concerns,2023,statement
602,"We use publicly available data sets in our experi-
ments with permissive licenses for research experi-
ments. We do not release new data or annotations
as part of this work.
",no ethical concerns,,no ethical concerns,2023,statement
603,"We provide and emphasize some details of our
work to address potential ethical concerns. First,
all the data sources used in the data collection pro-
cess are publicly available. We did not make any
changes to the data sources and only extracted
dialogue examples from these data. We carried
out strict quality control during the extraction and
annotation process. We made sure that there are
no sensitive words even though the original data
sources have already conducted this kind of check-
ing. However, using our data to train or fine-tune
a pre-trained generation model may still generate
semantic errors or unpleasant similes or responses.
One reason is that simile is a difficult task that com-
pares two different things, mistakes could happen
even when humans use similes. The other reason
is that the knowledge stored in the original param-
eters of the pre-trained models may dominate the
generation. We protect the privacy rights of anno-
tators and paid 0.55 Chinese Yuan for annotating
each dialogue data. The income of each annotator
was above 100 Chinese Yuan per hour (On January
20, 2023, 100 yuan can be converted into 14.73
dollars).
",no ethical concerns,,no ethical concerns,2023,actions
604,"The dataset we used is from the DSTC 11 track-5.
We use the back-translation and synonym substi-
tution to construct a noise version of the data fortraining. The generation models trained with this
noise data may learn to generate semantically in-
correct or unfriendly responses.
","inaccuracies, unfriendly responses",,"inaccuracies, unfriendly responses",2023,concerns
605,"The ethical considerations for our work mostly re-
late to the limitations; there are a variety of un-
intended implications of equating a language and
a country, such as misrepresentation of communi-
ties, and disregarding minority and diaspora com-
munities. However, we believe it is the closest
approximation possible when comparing the sur-
veys used in this work and LMs. Further, the sur-
veys have been criticised; particularly Hofstede’s
cultural dimensions theory has been deemed too
simplistic (Jackson, 2020). This could lead also
to simplistic assumptions when probing an LM.
We address these problems by including the WVS,
another widely used survey, in our study. Due to
these limitations, we believe that further studies
and applications of our approach should be done
with these limitations in mind. Particularly the
simplification of cultural representation by both
our approach as well as the original surveys might
impact communities negatively. Such misrepre-
sentation can have a disproportionate impact and
exacerbate the marginalisation of minority commu-
nities or subcultures.
",bias,,bias,2023,"concerns, suggestions"
606,"We ask experts to read and conﬁrm a consent con-
cerning data privacy and informed consent before
signing up for our tool. In the form, we explicitly
state the aim of the study and the later use of col-
lected data. We provide detailed information to the
subjects about the personal data information we
require for participation and its temporary usage
throughout the study. Subjects can request data
deletion at any given step of the study. All subjects
who agree to sign-up also consent to participate in
the study.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
607,"We consider our work can make more researchers
in table-to-text pay attention to the computational
cost problem, which may benefit from saving the
cost of the online table-to-text model. We experi-
mented on the public datasets with no discrimina-
tory or insulting sentences.
",no ethical concerns,,no ethical concerns,2023,statement
608,"This work presents CATS, a free and open dataset
for the research community to study the answer-to-
sequence problem in the practical TableQA system.
And it helps enrich the D2T languages and alleviate
the datasets’ bias in English. To balance the data
quality and scale and bring it closer to the practi-
cal scenario, data in CATSare collected from two
sources, which are manually annotated as CATS-
DandCATS-S. In other words, CATSconsists of
CATS-DandCATS-S. The data in CATS-Dis col-
lected from DuSQL (Wang et al., 2020b) dataset, a
free and open dataset for the Chinese Text-to-SQL
problem. Meanwhile, to enlarge our dataset, we
adopt an automatic data construction pipeline to
collect a large number of high-quality SQL-table
pairs for annotation. To ensure the quality of our
dataset, we manually annotate the SQL-table pairs.
We hire 24 native annotators with undergraduate
degrees to annotate the data. Specifically, 20 anno-
tators are responsible for annotations, and another
4 workers are asked to review the annotated data.
We pay 2.1 yuan ($0.31 USD) for annotating each
SQL-table pair.
To avoid our dataset leakages personal privacy,
we replace the sensitive information in the collected
tables with predefined unique words. Furthermore,
we ask the annotators to filter out the examples that
leak personal privacy and contain social bias and
harmful content.",no ethical concerns,,no ethical concerns,2023,actions
609,"Controversial Generation Content. Our model
is less likely to generate controversial content(e.g.,
discrimination, criticism, and antagonism) since
the model is trained on a dataset from the BBC
News domain. Data in the news domain is often
scrutinized before being published, and thus the
model is not likely to generate controversial data.
Desensitization of User Data. We use the Ama-
zon Mechanical Turk crowdsourcing platform to
evaluate three artificial indicators (i.e., fluency, in-
formativeness, and conciseness). For investigators,
all sensitive user data is desensitized by the plat-
form. Therefore, we also do not have access to
sensitive user information.
",no ethical concerns,,no ethical concerns,2023,actions
610,"InkNN-MT works, the symbolic datastore helps
adaptation but also introduce privacy concerns.
Since kNN-MT explicitly saves all target language
tokens in the datastore, there is a risk of privacy
leakage. In the future, more efforts may be put into
addressing this issue.
",privacy,,privacy,2023,suggestions
611,"This paper proposes a method for Multilingual
Knowledge Graph Completion, and the experi-
ments are conducted on public available datasets.
As a result, there is no data privacy concern. Mean-
while, this paper does not involve human annota-
tions, and there are no related ethicalconcerns.
",no ethical concerns,,no ethical concerns,2023,statement
612,"Our method is capable of generating large number
of hierarchical scripts. Learning knowledge from
different sources of information, the model might
be misused into generating unsafe contents upon
asking inappropriate questions. For now this seems
unlikely since there is no offensive content in out
collected dataset.
",misuse,,misuse,2023,concerns
613,"The paper focuses on generating synthetic dia-
logues for training grounded dialogue systems. Our
framework is developed based on the commonly
used large pre-trained LM, T5 (Raffel et al., 2020).
It is trained on large scale web data that is known
to contain biased or discriminatory content. How-
ever, how to remove bias from large LMs is stilla hard research problem so far. The datasets we
use are publicly available and contain no personal
identifiable information.
",bias,,bias,2023,concerns
614,"Our method is essentially based on a generation
model, and thus the OCR results should be taken
as AI-generated contents. As the generated re-
sults should be aligned with the input, we have
not noticed deliberate harmful contents, e.g., hate
speech, bias, etc. However, the model maintains
such ability, which might be triggered. Although
after finetuning on the public datasets the risk of
such phenomena is extremely low, we still take it
into account. In the future research, besides fo-
cusing on improving downstream performance, we
will study how to increase the controllability on the
generation.
",bias,,bias,2023,"concerns, suggestions"
615,"We base our method on an existing multimodal pre-
trained model, which is capable of vision-language
understanding and generation. Thus, there exist po-
tential risks in AI-generated contents. Additionally,
as our method only finetunes only a small amount
of parameters of the pretrained models, we lack
control of the output model, which may generate
harmful contents. These results may possibly be
attributed to the noise in the pretraining data. In
the future research, it is essential to study how to
increase the controllability on the generation while
most parameters of the output model are originated
from the pretrained model.
",harmful content,,harmful content,2023,"concerns, suggestions"
616,"Since our method relies on pre-trained language
models, it may run the danger of inheriting and
propagating some of the models’ negative biases
from the data they have been pre-trained on (Ben-
der et al., 2021). Furthermore, we do not see any
other potential risks.
",bias,,bias,2023,concerns
617,"This work is concerned with protecting or defend-
ing against adversarial attacks on text classification
systems. For modeling, our method ATI NTER
uses another neural network based language model
T5 (Raffel et al., 2020). This means the ATI NTER
can itself be attacked by an adversary. We believe
that attacking a pipelined model such as ATI NTER
is not straightforward for the following two reasons.
First, performing an adversarial attack on a model
typically requires access to output scores from thatmodel. Since ATINTER is used in a pipeline with
a task classifier, the attacker can never get access to
ATINTER’s output scores. This adds an additional
layer of complexity for the adversary. Second, tar-
geted adversarial attacks on sequence-to-sequence
models (such as ATINTER) are much less promi-
nent and it is generally more difficult to make small
alterations in the input without forcing a more sig-
nificant change in the textual output (Cheng et al.,
2020; Tan et al., 2020). Nevertheless, we have not
explored this possibility and therefore recommend
practitioners interested in using this work to care-
fully check for this.
Additionally, the experiments were only per-
formed on four text classification datasets. Al-
though we expect our method to be effective for
other classification tasks like Toxicity detection,
Hate Speech identification, but considering the sen-
sitive nature of these applications, we urge the prac-
titioners to first comprehensively evaluate our work
on those tasks before deploying in a real world
scenario.
For all our experiments, we used pre-established
and published datasets, which do not pose any se-
rious ethical concerns. For transparency and re-
produciblity, we will make our code publicly avail-
able.
",misuse,,misuse,2023,statement
618,"There are several ethical considerations that must
be taken into account when developing a text style
transfer model. One important consideration is
the risk of the generated text being used to spread
hate speech or misinformation. It is also crucial
to ensure that the model does not exhibit bias to-
wards a particular demographic, which could result
in harmful outcomes. Another potential ethical
concern is the misuse of the model for malicious
purposes, such as generating negative comments or
fake news. These issues need to be addressed to
ensure that the development and use of the model
align with ethical principles and values.
","misuse, bias",,"misuse, bias",2023,concerns
619,"We paid U.S. standard market wage to our program-
mers (Appendix B). The rate was determined by
the workers. The lilGym environment and data as
is are intended to be used for research, including
algorithm development and evaluation, and not for
development of models to be deployed.
Commented for anonymous submission
",no ethical concerns,,no ethical concerns,2023,statement
620,"We do not anticipate any ethical issues particularly
to the topics of this work. Nevertheless, some work
presented here extensively uses large-scale pre-
trained models with self-attention, which may lead
to substantial financial and environmental costs.
","financial costs, environmental costs",,"financial costs, environmental costs",2023,"statement, concerns"
621,"One potential ethical concern for our model is
the risk of miscommunication. Due to the small
amount of resources used to train our system, it
tends to be less accurate than its supervised coun-
terpart, and its mistakes may cause confusion, mis-
understanding and other psychological harm to the
users of our systems. The other ethical concern is
that the data used to train the system is demographi-
cally homogeneous, as we have noticed from some
brief inspections that most of the signers in the
ASL datasets are white middle-aged adults. This
may lead the system to worse retrieval accuracy
for people underrepresented in the training corpus,
such as black people, children and elderly people.
","misinformation, bias",,"misinformation, bias",2023,concerns
622,"Large Language Models can be prone to gener-
ate toxic and unwanted content (Weidinger et al.,
2021). Since MURMURuses focused modules
to accomplish specific skills, we believe that this
might help limit inadvertent negative impacts. Fur-
thermore, the presence of specific modules should
provide users with more trust and control in real-
world scenarios, allowing one to verify, debug, and
improve the capabilities of these modules.
",no ethical concerns,,no ethical concerns,2023,statement
623,"This paper makes use of only open-source video
data from YouTube. During the transcribing we
only focus on the dog barkings, make no use of the
personal information of the users, so the released
dataset ShibaScript does not contain any personal
information, hence doesn’t breach the privacy of
any persons.
",no ethical concerns,,no ethical concerns,2023,statement
624,"All of the name lists we adopted in this paper
are borrowed from public websites and previous publications (Tzioumis,
2018; Khalifa et al., 2021). We considered only
binary genders and four different racial groups,
which are clearly incomplete for depicting all hu-
mans. Our work is mainly at drawing researchers’
attention to the unfairness caused by speaker names
in text generation tasks given dialogues. These de-
mographic features are selected to shed light on
this potential issue and our method is not restricted
to any speciﬁc demographic groups.
",misrepresentation,,gender/race misrepresentation,2023,statement
625,"Our work aims to develop and evaluate algorithms
that automatically detect and correct grammatical
errors in written English and Chinese text. We
use publicly available datasets for training and
evaluation purposes. These datasets consist of
anonymized and de-identified text samples, ensur-
ing the privacy and confidentiality of the original
authors. We are committed to conducting our re-
search in an ethical and responsible manner.
",no ethical concerns,,no ethical concerns,2023,statement
626,"During the pre-training process, the information
from the trained corpus may contain part of im-
proper expressions like violence or discrimination.
In addition, the compression of the generative pre-
trained language models may result in relatively
weak sentence generation.
Hence, the trained models are likely to be con-
fronted with some potential risks in the large lan-
guage models as mentioned in (Weidinger et al.,
2021). With the tools proposed in (Thoppilan et al.,
2022), the harmful training data can be removed
to make the trained model conform to the norms
of society. It is also noteworthy that the safety
check is necessary before we deploy the generative
language models.
","improper expressions, violent content, discrimination, harmfulness",,"improper expressions, violence, discrimination, harmfulness",2023,"concerns, suggestions"
627,"Wukong-Reader inherits the publically released
RoBERTa model [ 24], where the checkpoint may
contain some harmful information learned from the
pre-trained corpus. Meanwhile, Wukong-Reader is
further pre-trained on the IIT-CDIP Test Collection
dataset [ 18] or the collected large-scale Chinese
document corpus, where there can be also improper
expressions. Although we have developed rules to
manually filter out harmful expressions from the
OCR-recognized texts during pre-processing, it is
not guaranteed that all harmful information can be
removed.
",harmful information,,"harmful expressions, inaccuracies",2023,"concerns, actions"
628,"We build a dialogue-level dependency parsing cor-
pus by crowd annotations. The raw dialogue data is
obtained from an open source. Besides, we remove
information relating to user privacy. The annota-
tion platform is developed independently by us.
All annotators were properly paid by their efforts.
This dataset can be employed for dialogue-level
dependency parsing in both zero-shot and few-shot
setting as well as in any other data settings.
",no ethical concerns,,no ethical concerns,2023,statement
629,"This work constructs a new benchmark for syllo-
gistic reasoning. The main dataset is automatically
constructed using entities and their relations from
Wikidata and ConceptNet. The construction tem-
plate is predefined and manually reviewed, so the
ethical concerns are avoided. For the human rewrit-
ing process, we hire five annotators and require
them to avoid any social bias and privacy issues
in the rewritten material. The results are randomly
shuffled and sent back to them for an ethical review.
We pay them roughly $15 per hour for annotation.
",no ethical concerns,,no ethical concerns,2023,actions
630,"Biases. We note that there might be some biases
in the data used to train the LLMs, as well as in
factuality judgments. Both are beyond our control.
Intended Use and Misuse Potential. Our mod-
els can be of interest to the general public and
could also save a lot of time to human fact-checkers.
However, they could also be misused by malicious
actors. We ask researchers to exercise caution.
environmental_impact. The use of large lan-
guage models requires a significant amount of
energy for computation for training, which con-
tributes to global warming. Our work performs few-
shot in-context learning instead of training models
from scratch, so the energy footprint of our work is
less. The large language model (Codex) whose API
we use for inference consumes significant energy.
","bias, misuse, environmental_impact",,"bias. misuse, environmental_impact",2023,"concerns, suggestions, actions"
631,"AsYonhapNews is one of the most reliable media
outlets in South Korea, articles from YonhapNews
are published through a rigorous verification pro-
cess and will be deleted or revised if they contain
any form of bias. However, the period of data col-
lected is three years, and there may be past article
content that has not been modified by new facts, so
we cannot guarantee that all articles in YonhapNews
dataset are completely unbiased. Nevertheless, this
dataset has sufficient potential to develop into vari-
ous studies and thus is released for academic uses.
",bias,,bias,2023,concerns
632,"Throughout our human evaluation, we collected
demographic details such as name, age, gender,
and highest education level, after securing partici-
pants’ consent and assuring them that their informa-
tion would be exclusively utilized for research pur-
poses. The results from the human evaluation were
anonymized to protect participant confidentiality.
The authors meticulously examined all customerreviews employed in the assessment, verifying the
absence of any offensive or biased material. Partic-
ipants took part in the evaluation for an estimated
40 minutes. They were compensated with a 5,000
KRW (equivalent to 3.7 USD) gift card, which was
marginally above the Korean minimum wage dur-
ing that period.
",no ethical concerns,,no ethical concerns,2023,statement
633,"Annotators who participated in the annotation
and/or verification task are paid a competitive
monthly salary to help with the tasks. The salaries
were determined based on the qualification and
the prior experience working on similar tasks and
adhering to the norms of the government of our
country. All the annotators were native speakers
of the respective languages and from the Indian
subcontinent. The annotators were made aware
that the datasets will be publicly released. The
annotated datasets have no personally identifying
information. The annotated data and the crawled
corpus have been checked for any offensive data
and discarded if present.
The released code and models will have an MIT
License11. The dataset will be released under a
CC-0 License12.
",no ethical concerns,,no ethical concerns,2023,statement
634,"The datasets used in this paper are all public and
have been checked before use to not include any
information that names or uniquely identiﬁes indi-
vidual people or offensive content. However, since
the datasets come from the Internet, potential bias
may still be introduced. This paper does not con-
tain any data collection or release, so there are no
privacy issues. Our model is pre-trained on GPU,
which may cause an environmental_impact. This pa-
per does not involve human annotation or research
with human subjects.
","bias, environmental_impact",,"bias, environmental_impact",2023,concerns
635,"For the human annotations on the dataset, the lan-
guage experts were paid a competitive monthly
salary to help with the task. The salary was deter-
mined based on the skill set and experience of the
expert and adhered to the norms of the government
of our country. The dataset has no harmful con-
tent. The annotations are collected on a publicly
available dataset and will be released publicly for
future use. All the datasets created as part of this
work will be released under a CC-0 license7and all
the code and models will be release under an MIT
license8.
",no ethical concerns,,no ethical concerns,2023,statement
636,"The annotations are collected on a publicly avail-
able dataset and will be released publicly for future
use. Some of these datasets originate from we-
bcrawls and we do not make any explicit attempt to
7https://www.meity.gov.in/
8https://www.bhashini.gov.in/
9https://www.cdac.in/index.aspx?id=puneidentify any biases in these datasets and use them
as-is. All the datasets used have been cited. All
the datasets created as part of this work will be
released under a CC-0 license10and all the code
and models will be release under an MIT license.11
The annotations in the testset were mostly con-
tributed by volunteers interested in contributing to
building a benchmark NER dataset. The volun-
teers were not made any payment and worked pro
bono . Some annotators were paid for their services.
These language experts were paid a competitive
monthly salary to help with the task. The salary
was determined based on the skill set and experi-
ence of the expert and adhered to the norms of the
government of our country. The annotators were
made aware that the annotations would be made
publicly available. The annotations contains no
personal information.
",no ethical concerns,,no ethical concerns,2023,statement 
637,"Our experiments and proposed model framework
are intended to encourage exploration in the clinical
information extraction domain while avoiding the
risk of privacy leakage. The data we use in this
work is publicly available and fully de-identified.
Though recent research has found it to be difficult
to reconstruct protected personal information from
such data, there remains some small risk that future
models may be able to do so. We have not altered
the content of data in any that would increase the
likelihood of such an occurrence and are thus not
risking private information leakage.
",no ethical concerns,,no ethical concerns,2023,statement
638,"MR.CODis designed for retrieving evidence that
supports cross-document RE. However, the evi-
dence retrieved by MR.CODis not always factually
correct. This evidence can only be considered as
potential context describing facts between given
entities.
",no ethical concerns,,no ethical concerns,2023,statement
639,"All datasets in our benchmark and continual pre-
training are obtained according to each dataset’s
respective data usage policy.
",no ethical concerns,,no ethical concerns,2023,statement
640,"Datasets used for building partial KB inference do
not contain any patient privacy information.
",no ethical concerns,,no ethical concerns,2023,statement
641,"We do not foresee any considerable risks associ-
ated with our work. In principle, our framework
for hallucination mitigation could be intentionally
reversed to produce lower-quality translations. But
there are easier ways to produce a bad translation,
such as just sampling the output text randomly, so
we do not think that our work poses any additional
risks.
This work is based on the open source dataset
and model released by Guerreiro et al. (2022) and
thus inherits all their potential biases.
We will make our code publicly available to en-
sure reproducibility of our experiments.
","bias, misuse",,"bias, misuse",2023,"statement, concerns"
642,"We consider four datasets in our experiments, in-
cluding Topic, Situation, Emotion, and Complaint.
The first three are publicly accessible. The last
one will be released upon publication. In particu-
lar for this dataset, 1) it does not contain any Per-
sonal Identifiable Information (PII); 2) This dataset
is desensitized and encrypted; 3) Adequate data
protection was carried out during the experiment
to prevent the risk of data copy leakage, and the
dataset was destroyed after the experiment; 4) This
dataset is only used for academic research, and it
does not represent any real business situation.
",no ethical concerns,,no ethical concerns,2023,statement
643,"We provide a methodology for improving the per-
formance of resource and data constrained trans-
lation systems which rely on obtaining synthetic
targets from larger pre-trained systems. Given the
dependence on large pre-trained systems, we be-
lieve that their biases can negatively impact the
biases and fairness of the smaller consecutively
trained systems. This is a problem common with
any type of knowledge transfer where the biases of
the base model can also be transferred to the stu-
dent system and approaches on mitigating biases
in the larger models in the first place would be a
potential solution that can alleviate this problem.
","bias, fairness",,"bias, fairness",2023,statement
644,"The disclaimer of ChatGPT states that the model
may occasionally generate incorrect information
and may occasionally produce harmful instructions
or biased content. Models, code and datasets were
used in accordance with their respective licenses,
terms of use and intended use. We provide logs
and code that we created for this work.7Data that
we used and generated does not contain any infor-
mation that names or uniquely identifies individual
people or offensive content.
","bias, harmfulness, inaccuracies",,"bias, harmfulness, inaccuracies",2023,"statement, concerns"
645,"There are no serious ethical concerns with this
work. The human volunteers all performed the
human evaluation tasks willingly without any co-
ercion. The human evaluation took 2 hours per
person.
",no ethical concerns,,no ethical concerns,2023,statement
646,"The study used de-identified health data to develop
a system that overcomes biases in medical decision-
making. However, social biases in language models
need to be addressed to ensure fairness in model
training. Therefore, before deploying any pre-
trained language model, fairness audits are nec-
essary to ensure an ethical and trustworthy model
for all stakeholders. Note, doctors should not rely
on automated summarization systems for diagnoses
in the interest of patient care.
","bias, fairness",,bias,2023,"concerns, suggestions"
647,"AliBERT , a BERT-based biomedical language
model for the French language, has the potential
to improve healthcare and research in French lan-
guage. However, it is essential to address ethical
considerations such as biases, privacy, misinforma-
tion, access and control, as well as accountability
and transparency. We have implemented measures
to mitigate biases, protect privacy, prevent mali-
cious use and optimize efficiency as much as possi-
ble.
To responsibly develop and deploy AliBERT , col-
laboration between developers, researchers, poli-
cymakers, and healthcare professionals are crucial.
By working together, stakeholders can ensure that
AliBERT benefits a wide range of users and upholds
ethical standards, ultimately maximizing its poten-
tial to improve healthcare and research in French
biomedical domain.
",no ethical concerns,,no ethical concerns,2023,statement
648,"In this work, our approach crawls an unlabeled
privacy policy corpus from the web policy doc-
uments specifically from the Google play store
which we use as a retrieval database. Although
these documents are completely publicly available
and was used only for research purpose, they may
contain some nomenclature of certain persons, ob-
jects, products, users, developers, or production
houses (i.e., industries). We neither obfuscate nor
make any altercation/modification to them.
",privacy,,privacy,2023,"concern, statement"
649,"License The OPP-115 and APP-350 datasets
are made available for research, teaching, and
scholarship purposes only, with further parameters
in the spirit of a Creative Commons Attribution-
NonCommercial License (CC BY-NC). The Pol-
icyQA and PI-Extract datasets are derived from
OPP-115 datasets. The PrivacyQA and PolicyIE
datasets are released under an MIT license. The
pre-training corpus, MAPS Policies Dataset, is re-
leased under CC BY-NC. We strictly adhere to
these licenses and will release the PLUE bench-
mark resources under CC BY-NC-SA 4.0.
Carbon Footprint We only use RoBERTa large
models for continual training on the privacy pol-
icy domain to reduce the environmental_impactsof training large models. The PP-BERT, PP-
SpanBERT, PP-Electra, and PP-RoBERTa models
were trained for 100k steps on Tesla V100 GPUs
that took 1-2 days. Therefore, the training would
emit only 9kg of carbon into the environment.3
All fine-tuning experiments were very lightweight
due to the small size of the datasets, resulting in
approximately 12kg of carbon emission.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
650,"7.1 Data Collection and Usage
Congressional Report corpus is publicly available
and can be directly downloaded online. Posts from
Debatepolitics.com were collected in a manner
that strictly followed the terms of use of the orig-
inal sources and the privacy rights of the source
owner. Authors involved in the data collection pro-
cess have all finished their human subjects research
training.
The Debatepolitics data will be released upon
request. The personal information of forum posters
will be concealed from the public.
The annotators were recruited from Proflic.com
and compensated equitably, irrespective of demo-
graphic and geographic attributes. Remuneration
exceeded the platform’s recommended average
hourly rate. The study’s purpose was disclosed,
instructions were clear, and the task posed no harm
to participating annotators.
7.2 Benefit and Potential Misuse of BBBG
Intended Use .The goal of this project is to provide
a means to overcome the label bias and scarcity
problem that has not been fully addressed in the
ideology prediction literature. It also provides a
useful representation of ideology that can be further
explored for other learning purposes. It is particu-
larly useful to predict and evaluate the stance of the
non-extreme group who tends to politically inactive
(cf. sec 4.2.3).
Recent Kansas abortion vote5has demonstrated
the importance of predicting leanings of the silent
majority. Devoid of such tools, lawmakers are more
likely to incorrectly extrapolate views of the vocal
minority to the entire population. Furthermore,
poor extrapolation emanating from the vocal mi-
norities views can have a significant impact on
political disengagement6.
However, like any machine learning model, there
is a risk of over-generalization of its true capabili-
ties. The output of our model needs to be assessed
and evaluated with full consideration of the charac-
teristics of the input source. The potential domain
difference of authors of texts might be significant,
and any conclusion drawn from studying our group
of authors cannot be immediately generalized to
other groups.
Risk of Misuse and Potential Harm . Our
model should not cause harm unless its users in-
terpret the prediction results in an unintended way.
It is meant to provide insights on the ideology dis-
tribution of a group or a population, instead of
judgment of an individual. Its output is not with-
out error, albeit more accurate than most models
under realistic situations. And for slightly leaning
and moderate people, it is possible our model may
generate incorrect outputs relative to the ground
truth. Though, our model mitigates this relative to
the prior SOTA. The potential harm of our model
could be magnified if it is used in making decisions
on vulnerable populations.
The predictions and insights generated by our
model should not be treated as facts or golden rules.
We also suggest that results from any political re-
lated studies should be interpreted with skepticism
and encourage the users of our model to perform
careful evaluation in their corresponding applica-
tion domain, check more sources or consult politi-
cal scientists for expert opinions.","misuse, misinformation",,"misuse, harmful predictions, misinformation",2023,"statement, actions, concerns, suggestions"
651,"Training data and its risks We use the GitHub
public dataset available on Google BigQuery fil-
tered to keep only projects with open-source li-
censes. While we do not perform preprocessing
that would eliminate any personally identifiable
information or offensive content, we remove nat-
ural language comments that presumably reduce
toxic content. Nonetheless, using code language
models (LMs) comes with certain risks, e.g., gen-
erating biased, toxic, and vulnerable code. Chen
et al. (2021) discussed the broader impact and risks
of code LMs (Section 7). We should keep those
factors to ensure the responsible use of code LMs.
Carbon Footprint We avoided using large mod-
els, reducing their environmental_impacts. We train
PLBART-base model on summarization-generation
and backtranslation for a maximum of 10k steps on
8RTX 2080 Ti GPUs that took 1-2 days. There-
fore, the training would emit approximately 15kg
of carbon into the environment.17No model fine-
tuning is performed in this work.
","bias, toxic content, privacy",,"bias, toxic content, privacy",2023,concerns
652,"License TheLeetCode examples we crawled
from the GitHub repository are under an MIT li-
cense. On the other hand, Project Euler and
Code Jam examples collected from GitHub do
not have any license information. The AtCoder
andAIZU examples are collected from CodeNet
which is under Apache-2.0 license. We crawl exam-
ples from GeeksforGeeks andCodeforces
and release them under CC BY-NC-SA 4.0 license.
To use the AVATAR benchmark, we are required to
adhere to these licenses strictly.
Carbon Footprint We avoided fine-tuning large
models due to computational limitations, resulting
in a reduced impact on the environment. We fine-
tuned nine models on program and function transla-
tion tasks and due to the smaller size of the training
data, all jobs took a total of 1–2 days on RTX 2080
TiGPUs. A total of 100 hours of training in a sin-
gleRTX 2080 Ti GPU results in approximately
7.5kg of carbon emission into the environment.10
Sensitive Information AVATAR composed of
parallel programs and functions that do not have
any natural language (NL) comments or docstring.
We remove them to get rid of any personally identi-
fiable information or offensive content. However,
there could still be such content in the form of
string as we do not manually check each example.","privacy, offensive content, carbon footprint",,"data privacy, offensive content, carbon footprint",2023,"statement, actions"
653,"There are two issues worth mentioning in regards
to this project. First, it was not feasible for us
to thoroughly examine the content of the data for
all languages,thus we cannot confirm the absence
of discrimination based on factors such as race or
sexuality. The data was solely utilized as a textual
corpus, and the content should not be interpreted
as an endorsement by our team. If the model is sub-
sequently utilized for generation, it is possible that
the training data may be reflected in the generated
output. However,addressing potential biases within
the data is an area for future research. Second, it
is important to note that while the data sources
utilized in this study do not explicitly prohibit the
reuse of data for research purposes,some sources
do have copyright statements indicating that such
use is permissible while others do not. Additionally,
certain sources prohibit the redistribution of data.
As such, data from these sources is omitted from
the published version of Glot2000-c.
","bias, copyright",,"bias, copyright",2023,"concerns, suggestions"
654,"Although instruction-tuning using many datasets
enable better zero-shot transfer, TART does not al-
ways retrieve documents that perfectly align with
users’ expectations. Applying TART to safety-
critical domains requires extra attention. BERRI
includes approximately 40 tasks covering diverse
domains. Although the data has been automatically
filtered, and we have examined the data, there may
still be harmful or privacy-sensitive contents. We
will release all of the data and preprocessing scripts
for follow-up work to inspect those dataset issues
and the effects of those data.
","privacy, harmful content",,"privacy, harmful content",2023,"concerns, suggestions"
655,"While it has been shown that PLMs are powerful in
language understanding (Devlin et al., 2019; Lewis
et al., 2020; Raffel et al., 2020), there are studies
highlighting their drawbacks such as the presence
of social bias (Liang et al., 2021) and misinforma-
tion (Abid et al., 2021). In our work, we focus
on pretraining PLMs with information from the
inter-document structures, which could be a way to
mitigate bias and eliminate the contained misinfor-
mation.
",no ethical concerns,,no ethical concerns,2023,statement
656,"This paper establishes a benchmark for extremely
weakly supervised text classification frameworks.
We provide empirical results on various SEED and
PROMPT methods, test their robustness, and an-
alyze their connections. We give intuitions and
insights on what method one should use for XWS -
TCin different circumstances. We believe that we
are on the ethical side and do not find any ethical
concerns in this work.
",no ethical concerns,,no ethical concerns,2023,statement
657,"In this study, we use two datasets FB60K-NYT10
and UMLS-PubMed, which include the knowledge
graphs FB60K and UMLS as well as the text cor-
pora NYT10 and PubMed. The data is all publicly
available. Our task is knowledge graph completion,
which is performed by finding missing facts given
existing knowledge. This work is only relevant to
NLP research and will not be put to improper use
by ordinary people.9
",no ethical concerns,,no ethical concerns,2023,statement
658,"Although the goal of our study is for more reliable
evaluation, there is a risk of dual use of our tests:
We investigate stress tests to identify blind spots
in existing generation metrics, but a subset of the
approaches (e.g., copy-source or injection) could
be used for cheating in an evaluation. By an explicit
discussion of how these blind spots can be utilized,
we hope to increase awareness in the community
of scenarios in which the metrics are not perfect
and could be manipulated. Towards mitigating the
risks, we have discussed countermeasures that can
be adopted to cover or detect such blind spots.
",misuse,,misuse,2023,concerns
659,"We acknowledge controlled text generation is po-
tentially capable of generating harmful outputs
such as producing offensive languages or hate
speech. However, it is also shown in previous
work that controlled text generation techniques
can achieve text detoxification if used properly
(Dathathri et al., 2020; Krause et al., 2021). When
changing the control interface from a categorical
setting to natural language commands, we are giv-
ing the user a larger freedom of input. Thus, ex-
tra care should be taken when deploying natural-
language controlled text generation models to the
general public to avoid malicious user inputs.
",harmful content,,harmful content,2023,"concerns, advice"
660,"The main ethical consideration for a tool like GEO-
SEQ2SEQ isprivacy . We respect user privacy in
the creation of GEO-SEQ2SEQas well as in collect-
ing the data to build TWITTER -PUG by only using
immediately available data provided by users. As
discussed in Section 3, the training data is built
from user profile location strings paired with a
user’s most frequently tagged Twitter Place. Once
trained, GEO-SEQ2SEQonly needs the user profile
location to run inference.
Also, due to the structured nature of the out-
put string and easy integration with Carmen, re-
searchers can easily choose at which granularity
to aggregate their data, whether the city, admin
(state/province), or country level.
Further, the use case of our model is only meant
to support researchers studying location-specific
demographics. The content will be studied in ag-
gregate, as according to Twitter policy.
",no ethical concerns,,no ethical concerns,2023,statement
661,"In this paper, all experiments are conducted on QM-
Sum (Zhong et al., 2021b), which is open-source
and obeys MIT license. The meeting transcripts
data doesn’t contain any privacy information(such
as password, phone number and trade secrets) or
offensive content.",no ethical concerns,,no ethical concerns,2023,statement
662,"The research conducted in this paper has been car-
ried out in accordance with the ethical principles
of ACL. We have ensured that our experiments do
not harm any individuals or groups and have ob-
tained informed consent from all participants. As
mentioned in the paper, we also tried to base our
main experimentation on the more environmentally-
friendly option, OPT-175B.
",no ethical concerns,,no ethical concerns,2023,statement
663,"Considering that crowd workers are often under-
paid, experiments in this work all followed fair
working wage standards13when using MTurk for
recruitment purposes (details for each task are in
Table 3). In addition, we have not rejected the
work from any unqualified workers so far, though
we reserve the right to do so when conducting the
experiments.
In our experiments, personal data (any informa-
tion relating to an identifiable natural person) was
collected, processed, and stored based on certain
data protection regulations,14given relevant pri-
vacy concerns. Special category information (i.e.
12We encourage starting the design from the reference-
based task (which performs as the test of true annotation task)
and thinking about what specific training the annotators are
expected to have through the qualification and endurance task.
13https://livingwage.mit.edu/counties/27053
14https://gdpr.eu/article-4-definitions/personal data revealing racial or ethnic origin, etc.)
was not included in this work. More information
about the details of human evaluation experiments
in this work can be found in the Human Evaluation
Datasheet (HEDS) (Shimorina and Belz, 2022) in
the Appendix.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
664,"The datasets used in our research are collected
through open-source approaches. The whole pro-
cess is conducted legally, following ethical require-
ments. As for the Turing Test in our study, all
participants are well informed about the purpose
of experiments and the usage of test data, and we
would not leak out or invade their privacy.
We see opportunities for researchers to apply
the system to idea discovery, especially for inter-
disciplinary jobs. We encourage users to explore
different combinations of subjects with the help
of our system, making the most of its knowledge
storage and thus maximizing the exploration ability
of the system.
The main focus of the system is to provide a
possible direction for future research, but the effect
of human researchers will never be neglected.
The massive data from various disciplines be-
hind the system makes it capable of viewing the
knowledge of an area in a multi-dimensional per-
spective and thus helps promote the development of
novel interdisciplinary. However, considering the
risks of misinformation generated by NLP tools,
the verbalization only contains possible insights
into new ideas. Researchers must thoroughly con-
sider whether an idea is feasible or leads to adverse
societal effects.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
665,"Towards Inclusive NLP Systems Building in-
clusive NLP systems is important so that everyone
can benefit from their usage. Currently, these sys-tems exhibit many design biases that negatively
impact minoritized or underserved communities
in NLP (Joshi et al., 2020; Blodgett et al., 2020;
Bender et al., 2021). Our work is a step towards
reducing these disparities by understanding that
models and datasets have positionalities and by
identifying design biases. The authors take inspi-
ration from fields outside of NLP by studying po-
sitionality (Rowe, 2014) and acknowledge cross-
disciplinary research as crucial to building inclu-
sive AI systems.
Ethical Considerations We recognize that the
demographics we collected only represent a small
portion of a person’s positionality. There are many
aspects of positionality that we did not collect, such
as sexual orientation, socioeconomic status, ability,
and size. Further, we acknowledge the limitation
of assigning labels to people as being inherently
reductionist. As mentioned in §7, using a single
Likert scale for social acceptability and toxicity is
not sufficient in capturing the complexities in these
phenomena, such as situational context.
We note that quantifying positionality of existing
systems is not an endorsement of the system. In
addition to making sure that language technologies
work for all populations, researchers should also
continue to examine whether these systems should
exist in the first place (Denton and Gebru, 2020;
Keyes et al., 2019). Further, we note that under-
standing a dataset or model’s positionality does not
preclude researchers from the responsibilities of
adjusting it further.
This study was undertaken following approval
from the IRB at the University of Washington
(STUDY00014813). LabintheWild annotators
were not compensated financially. They were lay
people from a wide range of ages (including mi-
nors) and diverse backgrounds. Participants were
asked for informed consent to the study procedures
as well as the associated risks, such as being ex-
posed to toxic or mature content, prior to beginning
the study.
Research Team Positionality We discuss as-
pects of our positionality below that we believe
are most relevant to this research. The research
team is comprised of computer scientists who study
human-computer interaction and NLP and have a
bent for using quantitative methods. Thus, we ap-
proach the topic from a perspective that assumes
that positionality can be characterized, fixed, and
quantified.
The entire research team currently resides in
the United States. In alphabetical order, the team
members originate from Belgium and Switzerland,
France, Germany, India, and the United States; and
identify as East Asian, South Asian, and White.
These nationalities and ethnicities are overrepre-
sented in the development of NLP technologies.
Thus, we acknowledge that our knowledge of how
design biases in NLP datasets and models impact
people is largely through research, rather than per-
sonal experience.",bias,,bias,2023,"concerns, suggestions"
666,"Intended Use: This dataset will benefit journal-
ists, activists, the research community, and an
informed public analyzing environmental claims
made by listed companies at scale. Also, we see
this as a first step towards algorithmic greenwash-
ing detection using NLP methods. It might also be
useful to policy-makers and regulators in both the fi-
nancial sector and the legal domain. Next, we hope
companies are inspired by our work to produce
more carefully drafted environmental claims. To
conclude, we envision that the dataset and related
models bring a large positive impact by encourag-
ing truly environmentally friendly actions and less
verbose boasting about environmental credentials.
Misuse Potential: Although we believe the in-
tended use of this research is largely positive, there
exists the potential for misuse. For example, it
is possible that for-profit corporations will exploit
AI models trained on this dataset while drafting
10https://www.sec.gov/environmental claims.
bias: Although the performance of NLP
models usually achieves an F1 score of above 80%,
it is widely known that ML models suffer from
picking up spurious correlations from data. Fur-
thermore, it has been shown that large pre-trained
language models such as ClimateBERT suffer from
inherent biases present in the pre-training data lead-
ing to biased models – and we believe our models
presented in this work also suffer from these biases.
Data Privacy: The data used in this study are
mostly public textual data provided by companies
and public databases. There is no user-related data
or private data involved.
Annotator Salary: We paid standard research
assistant salaries of around $30 per hour, which is
common practice at the University of Zurich. We
were upfront in disclosing to annotators that their
annotations will lead to a dataset and models which
can automatically detect environmental claims. We
found that this goal motivated annotators. We spec-
ulate (and hope) annotators interpreted the dataset
creation process and the goal of releasing the result-
ing dataset and models as an AI4Good application.
The feedback was overwhelmingly positive, and
many annotators have asked whether it is possible
to participate in follow-up annotation work related
to greenwashing detection.
","misuse, bias",,"misuse, bias",2023,"statement, concerns, actions"
667,"Data Privacy and Bias : All datasets used in this
research are published in previous studies and pub-
licly available: datasets for TSA, SC, FSRL, CD,
and Numeracy-600K can be downloaded from the
internet, while datasets for NC, NAD, and Stock-
Sen require signing corresponding agreements and
requesting from the authors.
Licenses: TSA is under Apache License 2.0; SC
is under CC BY-NC-SA 3.0; CD and StockSen are
under CC BY 4.0; and Numercay-600K, NC, and
NAD are under CC BY-NC-SA 4.0. The license of
FSRL data is not explicitly specified, but the author
allows data usage with a proper citation in their
GitHub repository.
Most of the datasets are widely used in the Fi-
nancial NLP domain (e.g., shared tasks). We also
manually checked for offensive content in the data.
There is no bias against certain demographics
with respect to these datasets.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
668,"As for the ethical practice in this work, the data
involved are from existing MWP datasets with no
private user information, and available under the
MIT license. As for the ethical impact of the use
of this work, the study is about providing a metric
and analyzing existing models’ robustness, so there
is less concern over harmful usage. Rather, it is
more about putting checks on existing AI models
and helping humans understand them better before
use. Potential stakeholders that could benefit from
this research include NLP researchers working on
math models, practitioners working on various ap-
plications involving mathematical reasoning with
text, and e-learning design.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
669,"Membership inference attacks can be used by
malicious actors to compromise the privacy of
individuals whose data has been used to train
models. However, studying and expanding our
knowledge of such attacks is crucial in order to
build a better understanding for threat models and
to build better defense mechanisms that take into
account the tools available to malicious actors.
Due to the importance of this aspect, we have
extensively highlighted existing work studying
how to defend against MIAs in Section 6. As we
are aware of the potential risks that arise from
membership inference attacks, we will not freely
publicize our code, but instead give access for
research projects upon request.
With regards to the data we used, we do not
see any issues as all datasets are publicly available
and have been used for a long time in NLP research
or data science competitons.
",no ethical concerns,,no ethical concerns,2023,statement
670,"Although this work improves the reasoning capa-
bilities of smaller models, the models are still not
powerful enough to be used in sensitive settings
such as education. We plan to release our code and
model checkpoints, but the models must be used
carefully by users, as many generative models, in-
cluding ours, are prone to hallucination.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
671,"All the datasets that we use are publicly available.
We report only aggregated results in the main paper.
We have not or do not intend to share any Person-
ally Identifiable Data with this paper. We release
the code and data associated with this paper as well
-https://anonymous.4open.science/
r/data_efficient_hatedetect/132
",no ethical concerns,,no ethical concerns,2023,statement
672,"The annotation was performed with a data anno-
tation service which ensured that the annotators
were paid a fair compensation of 15 USD per hour.
The annotation process did not solicit any sensitive
information from the annotators.
Replicability We have released the model check-
points and data at: https://github.com/
kumar-shridhar/LongtoNotes .
",no ethical concerns,,no ethical concerns,2023,statement
673,"To our knowledge, our back-end GPT-3 model was
trained mainly on English web data, it may prefer
events happen in an English environment. Fur-
thermore, our test showed that it generated events
specifically fit in American setting, for example,
Miranda Rights for arrest, Democrats and Repub-
licans in United States for election. These facts
suggest GPT-3 may ignore the knowledge of non-
American cultures or minority groups. In addition,
currently, we only create schemas for scenarios that
are reported in mainstream news media, e.g. con-
flict, communication. It excludes the schemas from
other domains, such as biology, medicine.","cultural bias, domain limitations",,"cultural bias, domain limitations",2023,concerns
674,"Potential risks : As with all large language mod-
els, the models used in our research have the po-
tential to generate factually incorrect information.
This is a potential risk given that our intended ap-
plication is for education. As reported in our paper,
our best-performing models produce acceptable
quality flashcard questions only 70% of the time.
The remaining 30% is significant enough that man-
ual review by course is necessary before questions
are deployed to students.
Intended Use : Our models and methods shown
here are for research purposes only. They should
not be deployed in the real world as solutions with-
out further evaluation.
Potential applications : Bull2Sum could be uti-
lized in the field of education to convert course
slides into summaries, which can then be used to
generate pertinent and significant questions for the
course. This application could enhance and facili-
tate students’ exam preparation.
",misinformation,,misinformation,2023,"concerns, advice"
675,"If our platform works as intended and sees broader
use in higher-ed, course staff and students can ben-
efit.
For course staff, it can reduce the time taken in
writing and grading assignments. It should also
help to alleviate concerns around cheating, as new
questions can be generated each semester, poten-
tially even creating new questions for each indi-
vidual student. By reducing this logistical burden,
Learn will allow course staff to focus on the parts of
teaching which are higher-leverage, and to improve
their courses in ways that weren’t before possible.
For students, the introduction of Learn into a
classroom is an opportunity to use more effective
methods for studying (such as spaced repetition), to
see higher-quality study material, and to have many
more questions with which to study. The ability
to automatically generate questions provides an
opportunity for students to get much more practice
– an arbitrarily large quantities of practice question
could be created, tailored to the topics in which
any particular student is struggling. As the system
improves over time we hope that students will have
study materials of higher quality than they would
without automated tools.
There are also large disparities in computer
science, especially for certain underrepresented
groups. These groups face structural issues which
may lead to differences in achievement. For ex-
ample, students in underrepresented groups may
feel uncomfortable asking questions of or reaching
out to course staff, and therefore may receive less
attention and aid from instructors. We can help
tackle those disparities by improving accessibility
through more effective learning systems.
It is also important to note, however, the po-
tential risks to privacy when building and using
education applications. Given the sensitive nature
of the data, such as grades, it is important to take
proper privacy and security safeguards when de-
ploying such systems. In order to minimize such
risks, we went through thorough evaluation by the
privacy office for the Computer and Information
Science Department at the University of Pennsyl-
vania. We also filled out the HECV AT lite security
questionnaire, receiving an A rating.
As education is downstream of many important
societal factors, such as economic growth and pub-
lic welfare, we are optimistic about the positive
impacts of new AI applications for education, like
Learn.",privacy,,privacy,2023,"statement, concerns, actions, advice"
676,"By offering a highly scalable and low-cost tutor-
ing solution, ITS offer lower income and minority communities a critical resource in boosting edu-
cational outcomes that has historically only been
available to wealthy students in the form of expen-
sive individual private tutors. We hope that these
advancements will reduce key educational dispari-
ties. It is also important in that vein to ensure that
public schools with smaller budgets are given ac-
cess to ITS systems in pilot trials. Instructors and
students should become well-versed in using the
technology in order to ensure successful expansion
into such schools. Furthermore, advancements in
model distillation and the creation of smaller lan-
guage models will lead to lower costs for adoption
for the schools that are most in need. Intelligent
Tutoring Systems that run on generative AI models
bring many of the same dangers of bias that are
prevalent in models more generally. Gender and
racial stereotypes can be invoked when students are
presented with specific explanations. For example,
a model may explain a math question that involved
individuals choosing jobs through a hypothetical
example that invokes a gender or racial stereotype
based on the example given. However, recent ad-
vancements in alignment have made great strides
in reducing this issue.
As these models become more widely available
to students, there is an increased likelihood of stu-
dents using these models for cheating on assign-
ments that are supposed to be completed without
outside resources. Unlike traditional plagiarism
which can be checked by comparing document sim-
ilarity, the use of generative AI to answer questions
on exams and assignments is far more difficult to
detect.
Lastly, discrepancies in model outputs and in-
accurate answers given when some students use
the ITS but not others can lead to misunderstand-
ings and confusion amongst students. As a result,
instructors should supervise the outputs given by
the ITS to students. In the event that a student
was supplied incorrect information by an ITS, that
should be taken into account in grading that stu-
dent’s course material. Instructors should incor-
porate AI policies in their syllabi that outline ac-
ceptable uses of ITS systems, address the handling
of potential inaccuracies from those systems, and
ensure all students have access to the ITS systems.
By highlighting the limitations of large language
models as tutoring systems, we hope our work will
prevent the premature use of these technologies.
9 ","bias, misuse, inaccuracies, confusion, misunderstandings",,"bias, misuse, inaccuracies, confusion, misunderstandings",2023,"concerns, actions, advice"
677,"The datasets included in OPENRTall use licenses
that permit us to compile, modify, and publish the
original datasets. TARAT is developed based on
Turkle3, which is released under the BSD-2-Clause
license4. Both OPENRTandTARAT are also pub-
lically avaliable with the license BSD-2-Clause,
which allows users to modify and redistribute the
source code while retaining the original copyright.
",no ethical concerns,,no ethical concerns,2023,statement
678,"We used the LOGIC NLG (Chen et al., 2020a)
dataset for training and inference. LOGIC NLG is
publicly available under MIT license1and widely
used in NLP research and industry.
",no ethical concerns,,no ethical concerns,2023,statement
679,"All experiments can be conducted on a single
NVIDIA Tesla V100-32G GPU. The datasets
(Maas et al., 2011; Zhang et al., 2015) used to
compare SaFER with previous methods are pub-
licly available, and we did not modify any data or
labels in these datasets. The dataset used for indus-
trial biomedical literature mining tasks is protected
and we do not plan to make it public in this work.
But the source code and instructions for using our
framework will be released along with the paper.
",no ethical concerns,,no ethical concerns,2023,statement
680,"ROBUTwere constructed upon the development
set of WTQ (Pasupat and Liang, 2015), WIKISQL-
WEAK (Zhong et al., 2017), and SQA (Iyyer et al.,
2017) datasets, which are publicly available under
the licenses of CC BY-SA 4.05, BSD 3-Clause6,
and MIT7, respectively. These licenses all permit
us to compose, modify, publish, and distribute ad-
ditional annotations upon the original dataset. All
the experiments in this paper can be run on a single
NVIDIA Tesla V100-32G GPU. Our benchmark
and code will be released along with the paper.
For the ROBUTannotation, we hired 15 gradu-
ate students (9 females and 6 males) majoring in
STEM majors. The hourly rates are in the range of
$10 and $12 based on the different working speed
(above the local average wage of similar jobs). We
recommended that annotators spend at most 4 hours
per day for annotation in order to reduce pressure
and maintain a comfortable pace. The whole anno-
tation work lasted about 30 days.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
681,"In this paper, we construct a new EG alignment
evaluation dataset based on two publicly available
EGs, and manually annotated a set of equivalent
predicates with argument types person and loca-
tion. Annotators are offered a competitive pay of
¥60 per hour, which is more than double the lo-
cal minimum wage. This remuneration applies to
both the annotation stage and the discussion stage,
ensuring that annotators are compensated for their
time and effort. Annotators are required to famil-
iarize themselves with the ACM Code of Ethics
and Professional Conduct and promptly report any
instances that violate the code. Inappropriate cases
that breach the code are promptly eliminated from
the selected documents. The resulting annotations,
based on the consensus of three annotators, pro-
vide a respectable approximation of the gold la-
bels. Note that they may not represent the absolute
ground truth due to natural error rates. Users who
wish to utilize the dataset should be mindful of its
limitations. We are not responsible for problems en-
countered in subsequent model training processes
utilizing our data.",no ethical concerns,,no ethical concerns,2023,"statement, advice"
682,"The authors declare that we have no conflicts of
interest. This article does not contain any studies
involving business data and personal information.
",no ethical concerns,,no ethical concerns,2023,statement
683,"The authors declare that they have no conﬂicts of
interest. This article does not contain any studies
involving business data and personal information.
Our experimentation does not involve any ethical
concerns. However, similar to other models, when
deploying our link prediction model to real-world
applications such as online recommendation sys-
tems, the prediction might be biased or unfair to
some ethic/gender groups. We advise researchers
in the community to look into bias (Bourli and Pi-
toura, 2020) and fairness (Fu et al., 2020) in KGs.
","bias, fairness",,"bias, fairness",2023,"statement, advice"
684,"In this section, we will discuss the ethical consider-
ations of our work.
Licenses and terms. The Wikipedia corpus and
Wikidata types are obtained via the Wikimedia
dump2, under the CC BY-SA 3.0 license3. AIDA,
MSNBC, and WNED-WIKI are shared under the
CC BY-SA 3.0 license. These datasets have been
widely used in entity linking research, and we be-
lieve that they have been anonymized and desensi-
tized.
Human Annotation. We recruited 3 human an-
notators without a background of expertise in anno-
tation, and 1 expert annotator with adequate knowl-
edge in entity linking for checking. These anno-
tators are employed by commercial data annota-
tion companies. We have paid these recruited an-
notators with adequate rewards under the agreed
working time and price. The annotators are well
informed about how these annotated data will be
used and released, which has been recorded in the
contract.
Intended use. NEL is an entity linking dataset fo-
cusing on the NIL prediction problem. Researchers
are intended to use NEL for examining the ability
of NIL prediction of newly created entity linking
models. AIDA, MSNBC, and WNED-WIKI are
intended for entity linking research, which is com-
patible with our work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
685,"We do not foresee any significant harm directly as a
result of our work. Having said that, we must under-
stand that automatic tutoring is a high-stake setting
that can pose significant harm if appropriate care is
not taken before the deployment of these systems.
Issues of biases and lack of trust, and other ethical
issues such as privacy concerns must be considered.
Considering learners only as data points within a
neural dialog tutoring context may prevent us from
seeing the societal and socioeconomic barriers that
they may be up against, thereby running the risk of
not only failing to help relevant learner subgroups
but also sometimes giving additional privileges to
those who use these systems.
9
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
686,"The DWTS makes state-of-the-art machine learn-
ing (ML) models accessible to researchers that
could previously not benefit from these advances.
Our tool targets Digital Humanities researchers and
is intended to assist with qualitative discourse anal-
ysis. As with most digital tools, though, it could
be misused for other work. We strongly believe
that including and enabling more researchers to
benefit from modern ML technology outweighs the
potential for misuse.
When using ML models, it is important to un-
derstand their limitations and critically reflect on
their predictions. ML models often include cer-
tain biases that can manifest in various types and
forms and are not without error. We try to mitigate
this by visualizing confidence scores where appli-
cable and additionally provide traditional methods
as ML-free alternatives. In particular, we offer a
multi-modal semantic similarity search and high-
light the confidences, but also provide a standard
lexical search to cross-check the results. Still, er-
rors in preprocessing steps, like removing relevant
content during data cleanup or the entity recognizer
model missing a certain entity of interest, may lead
to biased search and analysis results.
Naturally, we also introduce a bias with our sys-
tem design and envisioned workflow. While we
tried our best to model the process of digital dis-
course analysis as closely as possible, we might
still restrict a user in their workflow by our design
decisions.
Regarding privacy and security, we identified
and mitigated two limitations. The DWTS requires
users to upload their potentially sensitive data to
the system. To alleviate this, we ensured the tool
is easily deployable and can be self-hosted even by
non-experts allowing users to stay in full control
of their data. In particular, it is even possible to
install DWTS locally on private devices. Further,
the DWTS includes a feature that automatically
logs user actions and populates a logbook in order
to improve the documentation and reflection of re-
search processes. By making this an opt-in feature,
we guarantee that users are in control of their usage
data.
We are aware of the limitations of our system
and the technology therein and are committed to
actively participating in discussions with domain
experts regarding ML, privacy, and bias to identify
and iron out further constraints.
","misuse, bias, privacy",,"misuse, bias, privacy",2023,"concerns, actions, advice"
687,"In our experiments, we compare different setups
in which large language models are exploited in
order to artificially create more data to train models
aimed at detecting offensive language. In this case,
synthetic data has two main potential advantages:
first, it limits the amount of data gathered from
online spaces without user consent, and second, it
reduces the amount of manual annotation required
to create labeled datasets for offensive language
detection, which can have a negative psychological
impact on annotators (Riedl et al., 2020).While using generative models to augment data
can in some cases be beneficial for classification
performance, the sequences generated by these
models can exhibit unpredictable characteristics
that exacerbate existing bias or produce new forms
of it. Given that the improvements provided by
generative DA are inconsistent, there are no clear
advantages to this method when considering its po-
tential risks. As a consequence, we advise against
deploying models trained on generated data in prac-
tice.
Since the main contribution of our work is not
a novel model or algorithm, but rather an evalu-
ation of different approaches for generative data
augmentation, we share as much information as
we can for the reproducibility of our experiments,
but we choose not to release the code openly to the
public, in order to limit potential misuse of mod-
els that can generate offensive language. We also
choose not to publicly release the generated data
for various reasons. As shown by our results, the
generated examples are not reliable for improving
existing systems, so their utility is limited. Further-
more, the generated texts are not curated, which
could result in including personal user information
or harmful statements targeting specific individuals
being generated. We will, however, share the data
upon request to other interested researchers.
","privacy, bias, misuse",,"privacy, bias, misuse",2023,"concerns, actions, advice"
688,"We do not foresee specific ethical risks related to
the current work. On the contrary, the analysis of
different types of disagreement is aimed also at
making the role of subjective annotations accepted
within the NLP research community, making sure
that the voices of minorities are included. Indeed,
this work contributes to providing methodologies
to distinguish subjective annotations from mistakes
and poor-quality judgments.
",no ethical concerns,,no ethical concerns,2023,statement
689,"ChatGPT has achieved massive public attention
and societal impact, as people use the tool for all
different kinds of tasks. This impact comes with a
huge responsibility and risks, such as discriminat-
ing biases or spreading misinformation.
However, the system fails to meet the require-
ments of open science, as data, training details, and
model characteristics are kept private. We, there-
fore, consider our work an important contributionto understanding ChatGPT’s capabilities and objec-
tively highlight its potential and limitations.
",societal impact,,societal impact,2023,statement
690,"In MIMIC-III dataset (Johnson et al., 2016), every
patient is deidentified, according to Health Insur-
ance Portability and Accountability Act (HIPAA)
standards. The fields of data which can identify thepatient, such as patient name and address, are com-
pletely removed based on the identifying data list
provided in HIPAA. In addition, the dates for ICU
stays are shifted for randomly selected patients,
preserving the intervals within data collected from
each patient. Therefore, the personal information
for the patients used in this study is strictly kept
private. More detailed information about deiden-
tification of MIMIC-III can be found in Johnson
et al. (2016).
",no ethical concerns,,no ethical concerns,2023,statement
691,"This study covers work that utilizes PLMs, which
have a wide variety of positive applications, such as
the application to summarization, or language un-
derstanding. At the same time, there are a numberof ethical concerns with PLMs in general, includ-
ing concerns regarding the generation of biased or
discriminative text (Bordia and Bowman, 2019),
the leakage of private information from training
data (Carlini et al., 2021), and the environmental
impact of training or tuning them (Strubell et al.,
2019).
Our framework attempts to train PLMs with min-
imal changes made to their pre-existing parameters
in FL scenarios. Our work is believed to bring
some insights into the two ethical dimensions: pri-
vacy and environment. First, with respect to private
information leakage, although our work has not
addressed address the privacy issue in the pre-train
process, our FL framework can mitigate the data
privacy issues in the fine-tuning stages. In addition,
with respect to environmental_impact, our work
may obviate the need for full fine-tuning, which
may also significantly reduce the cost in terms of
memory or deployed servers.
Acknowledgment
This work was supported by the Basic Research
Program through the National Research Founda-
tion of Korea (NRF) grant funded by the Korea
government (MSIT) (2021R1A2C3010430) and In-
stitute of Information & Communications Technol-
ogy Planning & Evaluation (IITP) grant funded by
the Korea government (MSIT) (No. 2019-0-00079,
Artificial Intelligence Graduate School Program
(Korea University)).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
692,"To protect the privacy, we only collected comments
rejecting all personally identifiable information, in-
cluding the user IDs. Subsequently, we removed
comments containing personal information, such
as phone numbers and emails. Our dataset contains
real-life examples of abusive language obtained
from actual web data. Therefore, we notified the
dangers of the postings in advance. To mitigate
the risks, we limited the number of maximum com-
ments workers worked per day, and they were given
sufficient time to work. We paid workers above
minimum wage. We are aware that our topics could
have side effects, such as KODOLI’s potential ma-
licious use such as generating bad words. Nev-
ertheless, we urge the practical use of KODOLI,
such as filtering offensive comments explicitly and
identifying potentially offensive content from mul-
tiple points of view. This can prevent the negative
influence of users intentionally leaving malicious
comments.
",toxic content,,toxic content generation,2023,"concerns, actions, advice"
693,"We conducted two separate meetings before the
first and second steps of data construction. At first,
we introduced the reason we built this dataset and
the goal of our study and clarified what the relationextraction task is and how the dataset will be used.
All annotators agreed that their annotated dataset
would be used to build an RE dataset and train neu-
ral networks. We explained each type of the named
entity and the relation with multiple examples and
shared user guidance. In the second meeting, we
guided the annotators in evaluating and modifying
the interim findings in an appropriate manner.
We adjusted the workload of each annotator to
be similar by assigning different text lengths during
the first and second steps. We compensated each
annotator an average of $1,700, which is greater
than the minimum wage in Korea. Among 15 anno-
tators, 14 were Korean, one was Chinese, 11 were
female, and four were male. 30% of annotators are
in a doctorate and 65% are in a master’s degree.
Regarding copyrights, since our corpus is a histor-
ical record, all copyrights belong to ITKC. ITKC
officially admit the usage of their corpus under CC
BY-NC-ND 4.0 license.
",no ethical concerns,,no ethical concerns,2023,statement
694,"Impact of our work. Thanks to the efforts of var-
ious researchers, pre-trained models have been pro-
posed that can solve NLP tasks with high accuracy.
However, labeled data is essential for fine-tuning
these models, and few languages have abundant
language resources like English. In addition, con-
struction of labeled data is not easy. Therefore, at-
tempts to train high-quality models with little effort,
as in our study, are very important for low-resource
languages. Although our method and study have
the limitations mentioned above, our experiments
provided useful insights into selecting annotation
candidates for few-shot cross-lingual transfer. In
addition, we will publish the code used in our ex-
periments, which will facilitate the reproduction of
our experiments and contribute to further research.
Potential risks for bias. In recent years, bias in
data has become an issue. Training a model on
such data can lead to unwarranted predictions or
generate negative sentences for a particular per-
son or group. When the training data is small andcontains biases, such problems may be more pro-
nounced because the model is optimized only for
the provided data. In this study, we did not take
into account this issue, and our proposed method is
not designed to select bias-less candidates. There-
fore, when using the proposed method, sufficient
attention should be paid to the problem of bias.
",bias,,bias,2023,"concerns, advice"
695,"All statistical methods are double-edged swords.
Used maliciously, these methods could be used to
misrepresent social values and opinions. Moreover,
while these methods would be more informative
with demographic information on the annotators,
this conflicts with the privacy of the annotators, a
group of workers who are often treated unfairly
(Gray and Suri, 2019).
",research misuse,,research misuse,2023,statement
696,"Our results for Qs 2–3 show that cluster-based ag-
gregation universally improves the performance
of distributional learning. This seems to confirm
that clustering is a powerful tool for combating la-
bel sparseness to predict population-level annotator
responses. However, results were mixed for single-
label learning. Also, among the clustering meth-
ods in both distributional and single-label learning,
there was relatively little variance in performance
aswvaries.
The latter is certainly a negative result with re-
spect to the technical AI question of whether or not
to use both data features and label distributions in
cases when we docluster. But it is positive in that,
combined with the overall superior performance of
clustering for population-level learning, it showsthateither label features or label distributions are
adequate for realizing the benefits of clustering as
a means of label distribution regularization. It also
suggests that annotator disagreements are, in fact,
meaningful and essential.
To gain a better sense of how these methods
can be used to address annotator inequality, we
extract examples from DJQ3(Table 5), DFB(Fig-
ure 5), and DSI(Figure 4). We select examples
from among the data items with the lowest KL-
divergence scores between their empirical label
distributions and their predictions according to the
CO-FMM-CNN- 0model. We report their predicted
distributions according to this model and two other
models at a data item level.
Here, we see that the predicted distributions
seem to differ from the empirical distributions and
each other in meaningful ways. This is because
Our analysis constitutes a secondary study of pub-
licly available datasets and thus is considered ex-
empt from a federal human subjects research per-
spective. However, as with any study that in-
volves data collected from humans, there is a risk
that it can be used to identify people (Hovy and
Spruit, 2016; Kralj Novak et al., 2022). We under-
stand these risks and train and test our models on
anonymized data to minimize them. In addition,
it is essential to note that any methods identifying
marginalized voices can also aid in selective censor-
ship. Our models in Stage 1 and Stage 2, generate
rich soft label distributions, this can be helpful for
ML models to learn from a representative label.
The distributions can also help with making deci-
sions taking into account the right to freedom of
expression and right to safety for human content
creators, consumers, and annotators.
","privacy, misuse",,"privacy, misuse",2023,"concerns, actions, advice"
697,"In our human evaluation of the explanation quality,
we took steps to ensure fair treatment of the crowd
workers. To do this, we internally conducted exper-
iments to determine the average completion time
for one Human Intelligence Task (HIT). Our goal
was to pay a fair wage of at least $20 per hour to
all participants involved in the study. As a result,
all crowd workers received fair compensation for
their work, and no HITs were rejected.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
698,"We note that low resource classifiers are prone to
overfitting. We encourage users to thoroughly anal-
yse the predicted labels before using our provided
models. No other ethical considerations need to be
made regarding the data and models.",no ethical concerns,,no ethical concerns,2023,"statement, advice"
699,"We note that low resource classifiers are prone to
overfitting. We encourage users to thoroughly anal-
yse the predicted labels before using our provided
models. No other ethical considerations need to be
made regarding the data and models.",no ethical concerns,,no ethical concerns,2023,"statement, advice"
700,"We make sure that MidMed is collected in a man-
ner that is consistent with the terms of use of any
sources and the intellectual property and privacy
rights of the original authors of the texts. And
crowd workers were treated fairly. This includes,
but is not limited to, compensating them fairly, en-
suring that they were able to give informed consent,
and ensuring that they were voluntary participants
who were aware of any risks of harm associated
with their participation.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
701,"While there is limited risk associated with our
work, similar to existing abstractive summariza-
tion systems, there is no guarantee that the gener-
ated summaries are factually consistent and free
from hallucination (Maynez et al., 2020; Kang and
Hashimoto, 2020). Therefore caution is imperative
when our system is applied to practical projects.
","bias, misuse",,"bias, misuse",2023,"concerns, advice"
702,"The authors declare that they have no conflicts of
interest. This paper does not contain any studies
involving business data and personal information.
",no ethical concerns,,no ethical concerns,2023,statement
703,"Our work uses existing datasets, so it inherits some
of the risks associated with them, namely gender
bias (Cho et al., 2019), or privacy leakage (Carlini
et al., 2021), and mitigation strategies such as Van-
massenhove et al. (2018) may be necessary. How-
ever, replacing bilingual translation systems with
multilingual systems should help reduce gender
bias caused by pivoting through English. Another
consideration is the energy consumption for model
training, which results in green-house emissions
(Strubell et al., 2019). Our proposed architectures
result in smaller (and faster to train) models, than
similarly-performing baselines, increasing the effi-
ciency of translation systems.
","bias, privacy, environmental_impact",,"bias, privacy, environmental_impact",2023,"concerns, suggestions?"
704,"We acknowledge that the use of large language
models pre-trained on the Web could lead to biased
outputs. We did find out that our model may some-
times generate the incorrect pronouns for neutral
names. For example, in Figure 1, Charlee is being
referred to as a male in the generated summary,
while Charlee is actually a female as shown in the
reference summary. Such an issue is often caused
by under-specified context (e.g. Charlee’s gender
is not mentioned in the input dialogue). Fortu-
nately, we found that such an error accounts for
< 1% of the total outputs from our framework and
the issue can be largely alleviated when enough
context is provided.
8
",bias,,bias,2023,"concerns, statement"
705,"We acknowledge the importance of the ACL Ethics
Policy and agree with it. Large language models
can appear confident while providing false infor-
mation. In our work we are fortunate that incorrect
SQL output is verifiable and take care to report
the true reliability of the systems. Additionally
we acknowledge that large language models, such
as those studied in this work, may generate toxic
language (Gehman et al., 2020). While we avoid
pretraining on data sources and content from web
domains with offensive language, we acknowledge
that even our data gathered from reputable publish-
ers introduces bias (Bolukbasi et al., 2016).
","toxicity, bias",,"toxicity, bias",2023,concerns
706,"This paper complies with the ACM Code of Ethics
and Professional Conduct. Firstly, our adopted
datasets do not contain sensitive private informa-
tion and will not harm society. Secondly, we es-
pecially cite relevant papers and sources of pre-
trained models and toolkits exploited by this work
as detailed as possible. Moreover, our code will be
released based on the licenses of any used artifacts.
At last, our proposed multimodal misinformation
detection approach will contribute to protecting hu-
man beings from the detrimental and unordered
online environment with more trustworthy interpre-
tations.
",no ethical concerns,,no ethical concerns,2023,statement
707,"In the dataset collection, annotators need to select
or amend the model-generated candidate responses,
where some candidates may contain potentially un-
safe content. We ask annotators to produce safe and
engaging responses. (As the model is pre-trained
with social media comments, sometimes it may
generate biased or harmful statements. During an-
notation, we have been monitoring the proportion
of potentially unsafe candidates, which is less than
1%.) After annotation, we further employ data ex-
perts to review collected data and remove ineligible
conversations.
Diamante’s dataset and joint training paradigm
help boost the open-domain chatbot and align well
with human values. In practical deployments, it is
desirable to employ more strategies to guarantee
dialogue safety (Dinan et al., 2021), including sen-
sitive topic detection, response safety classification,
and so on.
8 Reproducibility statement
We describe the collection of Diamante’s dataset in
Section 2 and Appendix A, including the annota-
tion interface, annotation procedures, quality con-
trol process, etc. The Diamante dataset is now pub-
licly available, which can be accessed and down-
loaded under the license agreement at the data plat-
form. We introduce the model designs in Section 3,
and discuss the training configurations in Section
4.1.1. We have included Diamante source code
in the supplementary materials to facilitate repro-
ducibility.","toxic content, bias",,"toxic content, bias",2023,"statement, concerns"
708,"We sample all data in FiNE from QQ Browser,
while the platform has already censored all samples.
Therefore, the sensitive or personal information is
neither present to the annotator nor included in the
released dataset. The annotation of FiNE cost over
25,000 CNY. We hired four experts in NLP data
annotation (through a third party), each of whom
has at least one year of working experience in NLP
data annotation. It took 672 work hours (8 hours
per day for 21 days) to complete the annotation pro-
cess. The hourly rate is about 37.20 CNY, higher
than the standard local rate of 25.30 CNY.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
709,"In this research, we consider the following ethical
issues:
•Privacy. Outdated adapted weights may contain
information about the data and tasks they were
trained on. Thus it is important to consider the
potential privacy implications when recycling
these weights. Efforts should be taken to en-
sure that personal or sensitive information is not
disclosed during weight recycling.
•Fairness. It is crucial to guarantee that the re-
cycling of adapted weights does not introduce
biases or unfairly advantage certain tasks or do-
mains. Thorough analysis and testing are needed
to make sure that recyclable tuning does not per-
petuate or amplify existing inequalities.
•Responsible AI. The responsible development
and deployment of AI systems require consid-
ering the potential impacts on the environment.
By improving the efficiency and sustainability of
PLM adaptation, recyclable tuning contributes
to the responsible development of AI systems.
•Transparency. To facilitate the responsible and
ethical use of recyclable tuning, it is vital to be
transparent about the methods and assumptions
underlying them. We encourage future worksto clearly document the conditions under which
recyclable tuning is effective, as well as the po-
tential limitations or risks.
","privacy, bias, environmental_impact, transparency",,"privacy, bias, environmental_impact, transparency",2023,"concerns, suggestions"
710,"In this research, we adhere to the highest ethical
standards and commit to making every effort to
minimize any potential harm. Specifically:
•When creating our dataset, we have ensured that
all data collected is obtained through legitimate
and legal means. In addition, we have obtained
the appropriate permissions and consent from
all necessary parties.
•We have also taken steps to protect the privacy of
individuals whose data is included in our dataset
through de-identification during annotation.
•We are committed to eliminating bias, discrim-
ination, or stereotypes during annotation by re-
moving any suspect examples.
•We take the responsibility of open-sourcing the
interface, dataset, codes, and trained models to
the public. However, there are cases that these re-
sources are maliciously used. For instance, our
models may be utilized to generate responses
without proper attribution of the information
source, causing severe consequences. We would
strive to ensure that they are used ethically and
not for any malicious or harm-causing intent.
",research misuse,,research misuse,2023,"statement, actions, advice"
711,"In the writing process of this paper, ChatGPT (Ope-
nAI, 2022) was utilized for revision and refinement.
However, the authors can guarantee that each sen-
tence in this paper has been thoroughly reviewed
and checked to accurately convey the authors’ in-
tended meaning.
",no ethical concerns,,no ethical concerns,2023,statement
712,"The paper proposes a dialogue evaluation method,
which is intended to evaluate open-ended dialogue
on topics such as books and movies. A new dataset
is developed using some existing dialogue systems,
such as DialoGPT, which are trained on large-scale
web data that is known to contain biased or discrim-
inatory content. The datasets that we trained on
may also include subjective knowledge (comments
on movies) that may express the bias of the writers.
",bias,,bias,2023,concerns
713,"We use publicly-available datasets, meaning any
bias or offensive content in those datasets risks
being reflected in our results. By its nature, the
Gun Violence Corpus contains violent content that
may be troubling for some.
","bias, violent content, offensiveness",,"bias, violent content, offensiveness",2023,concerns
714,"Data Usage Because of the publicly-available,
internet-sourced nature of our training data, we
cannot deﬁnitively state that the current version
of AxomiyaBERTa is free of bias, both in terms
of outputs nor, as mentioned in the limitations
section, if there are dialect-level biases toward or
against certain varieties of Assamese that may be
trained into the model. Such investigations are the
topic of future research.
Resource Usage and environmental_impact
At 66M parameters, AxomiyaBERTa is a smaller
language model that is relatively quick to train
and run. Training was conducted on single GPU
devices. Pretraining AxomiyaBERTa took ap-
proximately 3 days, and task-level ﬁne-tuning
took roughly 30 minutes for non-phonological Ax-
omiyaBERTa and 1-2 hours for phonological Ax-
omiyaBERTa (depending on the task). Training
the pairwise scorer for CDCR took 12-19 min-
utes. Training and ﬁne-tuning took place on
the same hardware. For comparison, ﬁne-tuning
IndicBERT and MBERT on the AsNER dataset
for evaluation took roughly 20-30 minutes each.
These ﬁgures indicate that relative to work on
other Transformer models, training and evaluating
AxomiyaBERTa (including running other base-lines for comparison) comes with a comparatively
lower resource usage and concomitant environ-
mental impact. This lower resource usage also has
implications for the “democratization” of NLP, in
that we have demonstrated ways to train a perfor-
mant model with fewer local resources, meaning
less reliance on large infrastructures available to
only the biggest corporations and universities.
Human Subjects This research did not involve
human subjects.
",bias,,bias,2023,"concerns, suggestions, actions"
715,"We use publicly-available datasets, meaning any
bias or offensive content in those datasets risks
being reflected in our results. By its nature, the
Gun Violence Corpus contains violent content that
may be troubling for some.
We make extensive use of GPUs for training the
discriminator models as part of our pipeline. While
this has implications for resource consumption and
access implications for those without similar hard-
ware, the linear time complexity of our solution
presents a way forward that relies less overall on
GPU hardware than previous approaches, increas-
ing the ability to perform event coreference resolu-
tion in low-compute settings.
","bias, offensive content, violent content, resources consumption",,"bias, trigger warning offensive content, violent content, resources consumption",2023,"concerns, actions"
716,"We do not foresee any immediate negative ethical
consequences of our research.
9 Broader Impact statement
Knowing what we do not know, i.e., a well-
calibrated uncertainty estimation, is fundamental
for an AI-assisted application in the real world. In
the area of word sense disambiguation, the ambigu-
ity and vagueness inherent in lexical semantics re-
quire a model to represent and measure uncertainty
effectively. Our work explores the combination of
these two areas and hopes that it will provide an
approach to understanding the characteristics of
languages.",no ethical concerns,,no ethical concerns,2023,statement
717,"Our proposed AdMul aims to detect metaphors in
English, and the method can also be applied to
other languages or multi-lingual cases. Though our
manual observations did not show that there were
biased metaphor detection cases for AdMul, there
may still exist biases from the pre-trained language
model.
We use DeBERTa basein all experiments, which
is pre-trained on a variety of datasets, includ-
ing Wikipedia, BookCorpus3, and CommonCrawl,
etc(He et al., 2021). The total pre-training data size
is about 78GB. Since AdMul needs to fine-tune
DeBERTa base, AdMul may inherit poisonous lan-
guages from the pre-trained language model, like
hate speech, gender bias, stereotypes, etc.
",bias,,bias,2023,statement
718,"In the construction of the dataset, we forbid the an-
notators to compose any sentence that is offensive,
harmful, or contains personal information. The an-
notated data is manually checked to ensure safety.
We pay our annotators a competitive salary relative
to market rates. The annotated dataset is helpful
to encourage models “think” before they provide a
response, thus being safer in practical deployment.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
719,"Our proposed CMOT can help build a strong end-
to-end ST system. It has prospects for applica-
tion in many scenarios requiring speech translation,
which helps people understand speech in a foreign
language. Nevertheless, the results generated by
end-to-end ST systems are not necessarily perfect,
so people may not fully rely on the results for the
time being.
",model inaccuracies,,model inaccuracies,2023,"statement, advice"
720,"In this paper, we present an effective method
CRESS for speech translation. While our model
achieves superior performance on the widely used
ST benchmark MuST-C, applying it directly to real
scenarios is still risky. This is due to the fact that
our training corpus only contains hundreds of hours
of audio recordings from TED talks, which is far
from covering all domains of the real world. Be-
sides, the datasets we used in this paper (MuST-C,
WMT, and OPUS-100) are all publicly available.
We also release the code implemented with a well-
known framework fairseq. These guarantee the
reproducibility of our work.
",risky application,,risky application,2023,"statement, advice"
721,"Our model is developed and evaluated with pub-
licly available datasets: MuST-C and WMT. The
pre-trained models we use, like HuBERT and d-
vector models, are open and permitted for research
purposes. Our use of the above artifacts is consis-
tent with their intended use since they are widely
used in the speech research community. Although
our method could help the speech translation of
unwritten languages like some dialects, the perfor-
mance of the ST model still heavily relies on the
amount of ST training data. Therefore, the output
of the model is not always reliable and it would be
better to be assisted by professional human transla-
tors in real applications.
",model inaccuracies,,model inaccuracies,2023,"statement, advice"
722,"As stated in the introduction, all data and code used
in this work is freely available. The text included
in the dataset is in the public domain. Additionally,
we explicitly confirmed approval from the author
of the abridged books to use them in our research.
For the data validation task, the validators were
employed within our institution and thus were com-
pensated as part of their normal job role. Given
that the dataset is derived directly from published
books, it is possible that readers may be offended
by some content in these books. The validators did
not report any subjective experience of this.
With regard to our modeling approaches, large
pretrained models like the ones we use here for gen-
erating abridgements have a well-known risk of pro-
ducing harmful content (e.g. Gehman et al., 2020).
For the generation models fine-tuned on ABLIT,we did not subjectively observe any such text in
the sample output we assessed. We judge that our
controlled selection of training data reduces this
risk, but does not eliminate it. Accordingly, future
applications of abridgement can similarly consider
careful data curation for mitigating this risk.
",offensiveness,,offensiveness,2023,"Concerns, actions, suggestions"
723,"There are several perspectives from which we need
to consider the ethical considerations of this work.
Privacy: Lifelogs are personal data and should
only be used and shared given user authorization.
The lifelogs presented here are fictitious and do not
reveal the personal information of any individual.
No personal data is used to create this benchmark.
This work is intended to unlock development in the
creation, maintenance, querying and usage of lifel-
ogs, and additional work will certainly be needed to
ensure that they are secure and being meaningfully
and responsibly used.
Comprehensiveness and diversity: We recognize
that the lifelogs generated in this work are far from
representing the full range of human experiences.
While we strived to make the lifelogs complex
enough to benchmark and compare current state-
of-the-art, these lifelogs would not be considered
diverse in the sense that a social scientist would
note, and are likely biased by the life experiences
of its creators. We encourage future work in cre-
ating lifelogs that are more inclusive and faithful
to all walks of life. This includes further work in
making lifelogs that are more diverse in terms of
life experiences, personas, time scales, and queries
as well as more granular and complex in detail.
The strength of the benchmark is in identifying
patterns of questions on lifelogs rather than the
specific events described in them.
Inferring episodes: TimelineQA is a collection of
time-and-space boxed episodes, and not the raw
data itself from which the episodes are inferred
(e.g., a wedding photo, or video snippet from smart
glasses). Naturally, more research would need to be
devoted to understanding how to extract important
information in natural language and infer episodic
events from this raw data before performing ques-
tion answering. As mentioned previously, this also
involves sometimes grappling with the linguistic
variation amongst the language used in the episode
description and the query itself.
Intended use: We clarify that the benchmark
should not be used to train models for making key
decisions that will impact people’s lives (e.g., job
matching, insurance approvals or building personal
assistants). The intended use of TimelineQA is
as a benchmark to reveal potential limitations of
QA systems over lifelog data. Even if the bench-
mark is determined to be sufficiently comprehen-sive, a detailed study should be conducted to un-
derstand the potential representational harms of
using TimelineQA before using it for training mod-
els. Conceivably, TimelineQA can also facilitate
research in evaluating the biases of QA systems
by creating counterfactual pairs in the dataset: two
timelines which are exactly the same, but differ
by the demographic group or a specific life event
(e.g., having dropped out of college or committed a
crime). The QA system can then be systematically
probed for differences in performance between the
two timelines.
","misuse, bias",,"misuse, bias",2023,"statement, concerns, suggestions"
724,"We point out the limitations of large language mod-
els (costly to train, deploy, maintain, hallucinate,
opaque). The vision of POSTTEXT shows promise
of less costly training, maintenance, and more ex-
plainability. However, no actual system is built yet
to validate these claims and it is also not clear that a
system with POSTTEXT architecture will be easier
to deploy since it has more components.
",no ethical concerns,,no ethical concerns,2023,statement
725,"When deploying a system such as ours on real text,
e.g., news, one should carefully consider the impli-
cations of labeling real entities with certain proto-
role properties. For example, answering the ques-
tion of whether or not an actor instigated some
action could have serious ramifications in the real
world. Care should be taken so that such cases
might be, for example, flagged for human review.
",model inaccuracies,,model inaccuracies,2023,"concerns, advice"
726,"We consider the proposed technology to impose
little risk to readers, as it only summarizes what
has already been presented in the paper. However,
when the generated caption contains some inac-
curate information, it could mislead readers. Fur-
thermore, the proposed technology has the nature
of neglecting visual content, which might have an
impact on the accessibility of figure captions.
",model inaccuracies,,model inaccuracies,2023,"concerns, advice"
727,"Intervention in high-risk settings such as mental
health necessitates ethical considerations related
to safety, privacy and bias. There is a possibil-
ity that, in attempting to assist, AI may have the
opposite effect on people struggling with mental
health challenges. Here, in active collaboration and
consultation with mental health professionals and
clinical psychologists, we took several measures to
minimize these risks.
IRB Approval. We obtained approval from
the University of Washington’s Institutional Re-
view Board for both our data collection (IRB ID
STUDY00015882) as well as the randomized field
study (IRB ID STUDY00016783). Our organi-
zation requires all research personnel who con-
duct human subjects research to complete human
subjects protection training using the online CITI
course. The graduate students conducting these
studies were certified by our IRB.
Informed Consent from Participants. We ob-
tained informed consent from all participants in our
randomized field study (Appendix H). All partici-
pants were 18 years of age and older. Participants
were informed that they will be interacting with
an AI-based model that automatically generates re-
framed thoughts and is not monitored by a human.
Also, they were informed about the possibility that
some of the generated content may be upsetting or
disturbing.
Crisis Resources. We made it very explicit that the
model should not be used as a “cry for help” outlet
and should not be used in cases of suicidal ideation
and self-harm. Also, we provided two crisis re-
sources – Crisis Text Line (crisistextline.org) and
988 Suicide and Crisis Lifeline (988lifeline.org) –
to our participants at the start of the study.
Safety Measures. To minimize harmful LM-
generated reframings, we filtered out any response
that contained suicidal ideation or self-harm-related
words or phrases. For this, we created a list of
50 regular expressions (e.g., to identify phrases
like “ feeling suicidal ”, “wish to die ”, “harm my-
self”) using suicidal risk assessment lexicons such
as Gaur et al. (2019). An LM-generated response
that matched any of the regular expressions was
filtered out and not shown to the participants. Also,
participants were given an option to flag inappro-
priate reframing suggestions through a “Flag inap-
propriate” button (Appendix C).
Privacy. We did not collect any privately identifi-
able information in our randomized field study and
removed any user identifiers before conducting our
data analysis. All research data was stored within
a separate secure computing environment and only
trained research personnel were provided access to
data. The situations and thoughts collected in §4.1
went through an anonymization process, where we
manually removed any user identifiers and replaced
any specific identifiable information including loca-
tions, names, etc. with their more general version,
following Matthews et al. (2017).
11 ",no ethical concerns,,no ethical concerns,2023,"statement, actions"
728,"In this survey, we present and discuss various risk
analyses and intervention strategies to prevent soci-
etal harms from LMs. We also comment on com-
mon themes across approaches for detecting and
resolving population-centric harms (such as toxic-
ity and discrimination) and misinformation-related
harms, and we recommend future work combining
them. First, many datasets and resources we dis-
cuss may contain biases, and using them in down-
stream applications can lead to risks as we have
outlined. Second, many techniques we discuss have
limitations or are known to exacerbate other kinds
of harms (Xia et al., 2020), and thus, applying them
to newer problems may lead to unseen issues. Fi-
nally, the interventions we identify to raise general
awareness have the potential for misuse: a mali-
cious user can further imbalance the data to train
even more harmful models, use the models and
decoding algorithms to generate fake news, and
target marginalized populations. This, however,
should not discourage the development of mitiga-
tion strategies; rather, more work should be done
to detect and ban malicious users. This requires
not only technological solutions in NLP, but also in
social science, social network analysis, and public
policy.
","bias, toxic content, misinformation, misuse",,"bias, toxic content, misinformation, misuse",2023,"concerns, advice, suggestions"
729,"As with any effective method for controlled text
generation, we acknowledge that PREADDcould
be misused to increase toxicity, gender bias, or
any other harmful attribute (McGuffie and New-
house, 2020). Nonetheless, controlled text genera-
tion methods such as ours are also powerful tools
for content moderation and mitigating harmful text
generation, problems which are of major impor-
tance in practice due to large language models’
propensity to generate toxic or biased text (Sheng
et al., 2019, Gehman et al., 2020, Garbacea and
Mei, 2022). We are hopeful that future research on
controlled text generation will continue to improve
our ability to detect and mitigate such harms.
Additionally, as discussed in greater detail in the
","bias, toxic content, misuse",,"bias, toxic content, misuse",2023,"concerns, advice, suggestions"
730,"Strong automated systems for natural language gen-
eration have the potential for harm, for instance by
generating toxic or untruthful text. In this work,
we focus on creative stories, limiting the poten-
tial for abuse. Although we have not explicitly
attempted to decrease the likelihood of harmful
text in this work, DOC is built to be modular with
respect to the base language models we depend on,
so advancements in those systems can in principle
be transferred to DOC as well. Additionally, con-
trolled generation schemes can be used to reduce
output toxicity, similar to how we used FUDGE in
this work to control for outline relevance.
DOC is currently designed only for English;
transferring to other languages would require adapt-
ing our prompts. Performance might suffer in
lower-resource languages, as we depend heavily
on large pretrained language models which may
perform worse on such languages.
","toxic content, bias, misinformation",,"toxic content, bias, misinformation",2023,"concerns, suggestions"
731,"The MIMIC-CXR and MIMIC-III datasets are de-
identified to satisfy the US Health Insurance Porta-
bility and Accountability Act of 1996 (HIPAA)
Safe Harbor requirements. Protected health in-
formation (PHI) has been removed. Therefore,
the ethical approval statement and the need for
informed consent were waived for the studies on
this database, which was approved by the Mas-
sachusetts Institute of Technology (Cambridge,
MA) and Beth Israel Deaconess Medical Center
(Boston, MA). This research was conducted in ac-
cordance with the Declaration of Helsinki, describ-
ing the ethical principles of medical research in-
volving human subjects.
",no ethical concerns,,no ethical concerns,2023,statement
732,"•Data Annotation: Since the calls are an-
notated by real-world contact center man-
agers/supervisors, we did not require any
additional compensation for this annotation.
Rather, we develop a system where the man-
agers/supervisors put their scores for different
call conversations in their contact centers. To
map the questions to different question types,
Labelbox8was used for data annotation and
the annotators were provided with adequate
compensation (above minimum wages).
•Privacy: There is a data retention policy avail-
able so that the call transcripts will not be used
if the user does not give permission to use
their call conversations for model training and
evaluation. To protect user privacy, sensitive
data such as personally identifiable informa-
tion (e.g., credit card number, phone number)
were removed while collecting the data.
•Intended Use by Customers: Note that our
model is intended to help contact center su-
pervisors to be more effective in coaching
their employees by improving over the ran-
dom sampling of calls. The model does not au-
tomate the scoring of employee performance
or replace human review.
•Prevention of Potential Misuses: Below, we
discuss some of the potential misuses of the
system and our suggestions to mitigate them:
(i) Automatic Performance Reviews of
Agents by Considering all Recommended
Calls as Bad Calls: One potential misuse of
8https://labelbox.com/
the system could be the evaluation of agent
performance by considering all recommended
calls as bad calls without any manual review
of the call. To mitigate this, we can do the
following:
–Contact center supervisors that use this
system must be properly instructed that
this system does not determine whether
an agent performs badly in a certain call.
Rather, the intention of the proposed sys-
tem is to only suggest a set of calls to the
managers (instead of randomly selecting
calls) that they need to manually review
to determine whether the agent requires
coaching or not.
(ii) Considering Agents with More Recom-
mended Calls as an Indicator to Poorer
Agent Performance: Another potential mis-
use of the system is if contact center managers
start considering that if more calls are recom-
mended by our system for a particular agent,
then the agent is more likely to perform poorly.
To prevent this, we can do the following:
–We may suggest some positive calls as
well as negative calls to the managers.
Here, positive calls are the ones that our
system rates with a very high score and
categorizes as not requiring any coach-
ing. Whereas negative calls are the
ones that our system rates with quite
lower scores and classifies as coaching
required. To avoid any misuse of the
suggested calls, the proposed AI Coach
Assist system should never reveal to the
managers whether a call requires coach-
ing or not. Rather it should only al-
low the managers to make the final de-
cision on whether the call is a positive
call or a negative call. Once the sug-
gested calls are manually reviewed by
the managers and categorized as positive
by them, these calls can then be used to
train other agents that require improve-
ment in certain areas, whereas a call cat-
egorized as negative can be used to train
a particular agent who did not perform
well (i.e., requires coaching) in that spe-
cific call.
–In addition, to avoid suggesting too many
calls for the same agent, the system maysuggest only a certain number of calls
(not above a pre-defined limit) per agent
to the managers.
(iii) Using Bad Questions For Model Devel-
opment: In some contact centers, there may
be questions that are used for evaluating agent
performance which may contain any potential
biases toward a specific race or gender. We
can prevent this in the following way:
–The system should only respond to a pre-
selected set of questions that were used
during the training phase of the model.
Any questions that may pose any ethical
concerns or potential biases should not
be used while training the model such
that these questions can also be automati-
cally ignored during the inference phase.
•License: We maintained the licensing require-
ments accordingly while using different tools
(e.g., HuggingFace).",no ethical concerns,,no ethical concerns,2023,"statement, actions"
733,"The models and datasets utilized in the project pri-
marily reflect the culture of the English-speaking
populace. Gender, age, race, and other socio-
economic biases may exist in the dataset, and mod-
els trained on these datasets may propagate these
biases. Text generation tasks such as simplifica-
tion have previously been shown to contain these
biases.
In our collaboration with Wikipedia Editors to
produce the annotations for SW IPE, we ensured
to remunerate the participants fairly ($25/hour),
including for fully or partially completing the on-
boarding task. Participants could communicate
with us to voice concerns, could work at their own
pace, and choose to stop working on the project
at any time. Finally, we ensured to anonymize the
annotations by not including personally identifi-
able information in any version of the dataset (an-
notator identity is instead marked as annotator1 ,
annotator2 , etc.).We note that the models we use are imperfect
and can make errors. When interpreting our mod-
els’ outputs, results should be interpreted not in
terms of certainty but probability. For example, if
one of the simplification models generates edits
that introduce non-trivial information, it is possible
for this information to be hallucinated and not fac-
tually correct. Model outputs should therefore be
checked, or a warning that content was machine-
generated should be given to the reading audience.
To build the SW IPEdataset, we relied on several
datasets as well as pre-trained language models.
We explicitly verified that all datasets and models
are publicly released for research purposes and that
we have proper permission to reuse and modify the
models.
","bias, model inaccuracies",,"bias, model inaccuracies",2023,"concerns, actions, advice"
734,"The ability of models to answer complex logical
queries is achievable to reason about knowledge
graphs. Due to model’s uncertainty, one potential
negative impact of this task is the out-of-control in
automatic reasoning over open large-scale knowl-
edge graphs, where there are diverse source of in-
formation. Some of which though can be missed
from KGs due to incompleteness or private pur-
poses, but they can be possibly reasoned using the
query embedding methods.
",model inaccuracies,,model inaccuracies,2023,concerns
735,"Training data We use WikiText-103 and source
code in Python from permissively licensed GitHub
repositories to train GPT2 and CodeGen, respec-
tively. We do not perform any preprocessing that
would get rid of any personally identifiable infor-
mation or offensive content. However, the use of
code LMs comes with certain risks, e.g., generating
biased, toxic, and insecure code. We refer readersto Chen et al. (2021) (Section 7) for a detailed dis-
cussion on the broader impact of code LMs.
Compute We use an in-house cluster of 128
A100s for all jobs in this paper. Each run takes
a couple of hours to one day to finish, depending
on the configuration and the model size. We per-
formed one round of training for each setting as
it is very expensive to repeat them multiple times.
However, we perform the code completion and re-
ranking evaluation with three seeds. STS and code
search evaluation do not need multiple runs of in-
ference (as the predictions are deterministic).
Author Contributions
Dejiao and Wasi proposed the initial framework for
CONTRA CLM and completed the paper writing.
Nihal and Dejiao setup the pretraining code. Ni-
hal processed the pretraining data for the program-
ming language experiments. Dejiao designed and
completed all natural language related training and
evaluations. Nihal and Wasi completed the associ-
ated counterparts for programming language data.
Zijian was in-charge of the pretraining data collec-
tion and multinode distributed training of CONTR -
ACLM models on the programming language data.
Feng and Xiaopeng helped with our preliminary ex-
plorations on natural language data evaluation. All
the other co-authors provided thought-provoking
discussions and suggestions for this project, and
helped shape and proofread the paper draft.
","bias, toxic content",,"bias, toxic content",2023,"concerns, actions"
736,"No protected health information were used in the
creation of this dataset. Annotators were paid a
fair hourly wage consistent with the practice of the
state of hire.
",no ethical concerns,,no ethical concerns,2023,statement
737,"Human Evaluation. In terms of fair-pay, the pay-
ment to the expert annotators was above the mini-
mum wage in the US. Consent was given from the
annotators to use their annotations and release them
as part of this research. The annotators were not
aware which solution generated each completion
and the completions were presented in a random or-
der as to avoid bias based on the order. To achieve
high quality results, the annotators had a calibra-
tion session as to better understand the guidelines
and requirements described in Appendix D.
Environmental. Compared to the massively
large language models such as T5-3B, our models
are lightweight and can be run on smaller more
energy efficient hardware such as a CPU. In addi-
tion, this becomes more apparent when consider-
ing the superior inference-latency of our solutions.
Since energy is composed of both time and power-
consumption, our lightweight models should waste
significantly less energy during inference.
Biases and Exclusion. The proposed models de-
pend on a fixed and pre-determined vocabulary of
potential multi-token completions, and the choice
of this set in itself may result in omissions, ex-
clusions under-representation of some groups or
concepts, and over-representation of others. Care
should be taken to select a set that alleviate such
biases to the extent possible. Also after the selec-
tion of the set, the algorithm does not guarantee
balanced, fair or unbiased selections of candidate
completions. Users should be aware of this when
designing algorithms whose predictions may influ-
ence certain groups.","bias, lack of inclusivity",,"bias, lack of inclusivity",2023,"statement, concerns"
738,"No protected health information will be released
with the created annotations. Annotators were paid
a fair hourly wage consistent with the practice of
the state of hire.
",no ethical concerns,,no ethical concerns,2023,statement
739,"The training and evaluation of our experiments
rely on many compute resources, which may not
be environment-friendly. For example, each pars-
ing model requires training using NVIDIA A100-
SXM4-40GB GPUs for many hours, which can
inevitably cause more CO2 emission.
",environmental_impact,,environmental_impact,2023,statement
740,"To consider ethical concerns, we describe the fol-
lowing: (1) We conduct all experiments on existing
datasets derived from public scientific research. (2)
Our work does not involve any sensitive tasks or
data. (3) We describe the datasets’ statistics and our
method’s hyper-parameter settings. Our analysis
is consistent with the experimental results. (4) We
will release our code on GitHub for reproducibility.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
741,"The shared encoder architecture proposed in this
paper significantly reduces compute infrastructure
cost of large-scale SLU systems in practice. A
large absolute compute infrastructure cost reduc-
tion implies a positive environmental_impact due
to less power consumption.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
742,"In this paper, we design a multi-objective white-
box attack against DG models on four benchmark
datasets. We aim to study the robustness of state-
of-the-art transformers in DG systems from sub-
stantial experimental results and gain some insights
about explainable AI. Moreover, we explore the po-
tential risk of deploying deep learning techniques
in real-world DG scenarios, facilitating more re-
search on system security and model robustness.
One potential risk of our work is that the method-
ology may be used to launch an adversarial attack
against online chat services or computer networks.
We believe the contribution of revealing the vul-
nerability and robustness of conversational models
is more important than such risks, as the research
community could pay more attention to different
attacks and improves the system security to defend
them. Therefore, it is important to first study and
understands adversarial attacks.
",misuse,,misuse,2023,"statement, concerns"
743,"Our study makes use of three datasets. First is the
set of prompts collected in Boghrati et al. (2018),
which involved anonymous participants creating
fictional statements, so there is no personal infor-
mation involved. Second is the publicly available
r/ChangeMyView dataset collected by Tan et al.
(2016), which consists of statements made by users
behind typically anonymous aliases. Lastly is the
publicly available WikiQA corpus (Yang et al.,
2015), which does not contain identifying infor-
mation.
In our r/ChangeMyView application study-
ing the relationship between syntactic similar-
ity and persuasion, we make the assumption
that r/ChangeMyView is a community represen-
tative of online arguments. However, partially
due to its anonymity, it is unknown whether
r/ChangeMyView is a representative sample with
diversity in location, educational background, so-
cioeconomic status, ethnicity, and many other im-
portant factors. An ideal study should be able to
control for proxies for individual traits in order to
isolate the impact of syntax itself.
Generally, while most algorithms are not inher-
ently unethical, there is often potential for abuse in
their applications. The individual computations in
the FastKASSIM algorithm do not have any nega-
tive implications, but it is possible to use syntactic
similarity for unethical downstream tasks. For in-
stance, because syntax is an important aspect of
writing style, it is possible that users may try to
adversarially uncover an anonymous author’s iden-
tity. We do not condone the use of FastKASSIM
for any unlawful or morally unjust activities. We
do not propose any new tasks that would introduce
unethical activity.","misrepresentation, abuse",,"misrepresentation, abuse",2023,"statement, concerns"
744,"Chatbot Identities. All study participants were
informed that they were speaking to a chatbot, in
accordance with law in certain localities (e.g. Cali-
fornia’s Bot Disclosure Law).
Dangers of Fully Automated Dialogue Systems.
We do not encourage the deployment of fully auto-
matic dialogue systems for tasks such as emotional
support in production settings. Bot Disclosure
Laws exist because knowledge of chatbot identities
affect human perception (Shi et al., 2020), and thus
in sensitive situations such as therapy or emotional
support, patients may not receive adequate support.
Moreover, there is the possibility of emotional sup-
port dialogue systems without proper guardrails
introducing harmful or otherwise unethical content,
e.g. by mentioning references which could be con-
sidered “triggering.” Instead, we advise the use
of mixed-initiative dialogue systems in a support-
ive manner, e.g., to assist trained counselors who
have the emotional intelligence to recognize what
content may be hurtful.
Reproducibility. In this study we used GPT-3,
which is not an open-access language model. How-ever, we have clearly described all of the prompts
used in our paper.
biases Every dataset, including P4G and
ESC, has its own biases. LLMs such as Instruct-
GPT have been trained on large amounts of data
but may still not capture language usage of a suffi-
ciently diverse population. While in Appendix C
we see InstructGPT’s ability to handle diversity in
language, this is something that warrants further
interactive study with more extreme cases.
Crowdsourcing. All crowdworkers were paid at
a rate of $15 per hour. We did not collect any
personal or demographic information about any
workers. Our study and data collection process has
received IRB approval.
","bias, misuse",,"bias, misuse",2023,"concerns, advice"
745,"We propose a slowdown attack against dynamic
transformers on GLUE benchmark datasets in this
work. We aim to study the efficiency robustness of
dynamic transformers and provide insight to inspire
future works on robust dynamic transformers.
Our proposed framework may be used to attack
online NLP services deployed with dynamic mod-
els. However, we believe that exploring this new
type of vulnerability and robustness of efficiency is
more important than the above risks. Research
studying effective adversarial attacks will moti-
vate improvements to the system security to defend
against the attacks.
",security concerns,,security concerns,2023,"statement, concerns"
746,"Even though some of the investigated systems may
achieve a high level of factuality on the CNNDM
dataset, this does not guarantee that they can be
used as off-the-shelf factual consistent summariza-
tion models. Thorough evaluation should be con-
ducted before using these models in high-stakes
settings to ensure their reliability.
",model inaccuracies,,model inaccuracies,2023,"statement, advice"
747,"The MCoNaLa dataset is built to serve as a testbed
for evaluating code generation systems from nat-
ural languages extending beyond English, given
that an English-centric setting can harm universal
accessibility to language technologies.
We hire annotators who are proficient in target
languages and assist them with clearly documented
instructions, flexible annotation interfaces (e.g.,
Google Sheets), and automated methods (e.g., us-
ing a neural classifier to filter out possibly invalid
cases) to optimize the annotation efficiency. We
carefully check in line with our instructions and
standards, to ensure the quality of both the ques-
tion posts given and the annotation results back
from our annotators. We emphasize the differences
between samples in different languages, because
they are natural reflections of the questions that pro-
grammers asked in each specific language, similar
to many works in fields such as multilingual ques-
tion answering (Clark et al., 2020) and named entity
recognition (Nothman et al., 2013). We reckon that
it is of paramount importance to evaluate on data
that was originally produced in the target language,
and results may be less reliable otherwise.
Nevertheless, with the advances in models capa-
ble of generating code from natural language in-
puts, we should be aware of the potentially harmful
usage such as concealing malicious code (Wallace
et al., 2020), or generating code with security vul-
nerabilities (Verdi et al., 2020; Pearce et al., 2021).
","misuse, security",,misuse,2023,"actions, concerns"
748,"We address the ethical considerations and conse-
quences of the SexTok dataset and the accompany-
ing experiments here.
•The study’s focus is on the technical aspects
of the problem. It does not address the broader
societal and ethical implications of censorship
and of regulating sexually suggestive content
on social media platforms. The work only
aims to detect sexually suggestive content and
sex education content against other video top-
ics but makes no stand on censorship or con-
tent regulation of sexually suggestive videos.
•Sexual suggestiveness, as well as perceived
gender expression, is a subjective matter and
is hence susceptible to annotators’ bias.
•Gender expression, specifically visual cues
only, was annotated and offered only to evalu-
ate bias based on visual cues since such biases
are known to exist within large-scale visual
datasets (Meister et al., 2022). The authors do
not condone the practice of assigning gender
identity based on a person’s external appear-
ance since gender is an internal sense of iden-
tity (Association, 2015). This dataset is not
intended to be used for any such practices.
•Due to the nature of the problem, and potential
licensing issues, the publicly-collected data is
not anonymized.","bias, sexually suggestive content, privacy",,"bias, sexually suggestive content, privacy",2023,statement
749,"In this work, we propose an interactive framework
for spreadsheet formula prediction via hierarchical
Formulet expansion. The datasets we use are all
collected from public spreadsheet datasets, Enron,
FUSE, and Euses, and we follow SpreadsheetCoder
andFORTAPto use Enron for formula prediction
training and testing. We additionally use FUSE for
training and testing to check the model’s general-
izability. The data we used are all in English and
has no privacy issue. It does not contain any infor-
mation that names or uniquely identifies individual
people or offensive content. We do not collect ad-
ditional spreadsheet data in this work. The TUTA
model we used in this research is open source un-
der the MIT License. Our code will be released to
the public under the MIT License. We think that
the interactive formula generation can reduce the
affection on user experience when faced with chal-
lenging cases and incorrect prediction occurs. Our
framework has been deployed at GridBook (Srini-
vasa Ragavan et al., 2022) for internal testing, and
we hope it can stimulate new experiences in real
products such as Excel and Google Sheets to ben-
efit a large number of real spreadsheet users. Al-
though our model predicts expansions with high
accuracy in each step, it is still possible that our
model cannot meet the users’ specific needs, requir-
ing the users to write the expansions or formulas
by themselves.
",no ethical concerns,,no ethical concerns,2023,statement
750,"The significant improvements in classifying minor-
ity emotion categories brought by our method can
make MultiEMO a powerful tool in psychopatho-
logical fields such as depression detection, whereminority emotions sadness ,fear andanger are im-
portant early indicators of depression (O’Connor
et al., 2002).
",no ethical concerns,,no ethical concerns,2023,statement
751,"We perform English and Chinese experiments on
public datasets and corpora. Specifically, English
datasets come from ROCstories and Project Guten-
berg. Moreover, Chinese datasets include the LOT
dataset and public corpora of JY and LX. Auto-
matic and manual evaluation demonstrate that our
model outperforms strong baselines on both Chi-
nese and English datasets. In addition, our model
can be easily applied to different languages by sub-
stituting specific pre-trained language models.
As for manual evaluation, we hired three native
Chinese speakers as annotators to evaluate gener-
ated texts and did not ask about personal privacy or
collect the personal information of annotators. We
pay 1.8 yuan (RMB) per sample in compliance with
Chinese wage standards. Considering it would cost
an average of 1 minute for an annotator to score a
sample, the payment is reasonable.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
752,"We collect two original medical databases from
the real world. However, cell-values in medical
databases are commonly sensitive, since the in-
formation of patients and doctors are involved in
these values. Thus we only retain the database
schema and generate sufficient cell-values with cer-
tain rules. We ensure that generated values are rea-
sonable and that privacy of medical system users
can get protected.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
753,"Those who wish to use this protocol should be wary
of some possible ethical implications around the
usability and validation that the protocol gives to a
debiasing method.
First, the protocol was designed to be used as
an evaluation tool for consistency and is far from
representing all tests and considerations that must
be taken before deploying a debiasing method into
public or private use. The use of the protocol is
encouraged to gain insight into possible shortcom-
ings of a methodology, but there are risks to this
as there may be considerations and inefficiencies
that the protocol does not account for. Thus, the
protocol is meant for research purposes only and
is not meant to be a foolproof ethical check for
software deployment.
Second, the protocol was built using, and only
considering, English with North American defini-
tions of specifications. This means that any results
found using the protocol, and even the protocol
itself, may not work or be as effective if used with
different languages or different social and cultural
definitions of specifications. However, we sincerely
hope that our work helps open the doors for future
work into testing the effectiveness of the protocol
on different languages and within different cultures
and values and adapting it accordingly.
","lack of inclusivity, model inaccuracies",,"lack of inclusivity, model inaccuracies",2023,"statement, concerns, suggestions"
754,"Below we present some considerations related to
the ethical and broader impact of our work.
First, all datasets used in our study are from pub-
lished work and are publicly available, including
theVIZWIZdata (Gurari et al., 2018) which has
been curated from visually impaired users and re-
leased publicly after proper ﬁltering to preserve the
privacy of the users.
Second, for human evaluation of our models, we
collected human data via the Amazon Mechani-
cal Turk platform. We detail the data collection
process and measures taken to control the qual-
ity of collected data in App. D. As for the ethical
considerations related to collecting data from hu-
man subjects, our data collection campaign was
approved by an ethics review board in our institu-
tion. Human subjects were paid at the rate of 0.15
USD per HIT (Human Intelligence Task) resulting
in an hourly payment well above minimum wage.
Third, by testing models on a data distribution
different from the training one, the OOD evalua-
tion setting studied in our work has the following
broader impacts: it highlights (1) the challenges of
generalizing to real-world VQA datasets such as
VIZWIZ, and (2) the kind of biases learned (and
also potentially ampliﬁed) by the models.Lastly, we discuss both potentially beneﬁcial and
harmful applications of the task of Visual Question
Answering studied in our work. VQA has many
potential applications beneﬁcial for society:
•Aiding visually impaired users in understand-
ing their surroundings (Human: What is on
the shelf above the microwave? AI:
Canned containers. )
•Teaching children through interactive demos
(Kid: What animal is that? AI:That is
Dall Sheep. You can find those in
Alaska. )
•Aiding analysts in processing large quanti-
ties of visual surveillance data (Analyst: What
kind of car did the man in red shirt
leave in? AI:Blue Toyota Prius. )
•Interacting with in-home physical robots (Hu-
man: Is my laptop in my bedroom
upstairs? AI:Yes. Human: Is the
charger plugged in? )
•Making visual social media content more
accessible (AI: Your friend Bob just
uploaded a picture from his Hawaii
trip. Human: Great, is he at the
beach? AI:No, on a mountain. )
But like most other technology, VQA could also be
used for potentially harmful applications such as:
•Invasion of individual’s privacy by using VQA
to query streams of video data being recorded
by CCTV cameras at public places.
•Visually impaired users often need assistance
with parsing data containing personal infor-
mation (Ahmed et al., 2015), such as credit
cards, personal mails, etc. Such VQA systems
could be conﬁgured to leak/retain personally
identiﬁable information.",no ethical concerns,,no ethical concerns,2023,"concerns, actions"
755,"AI-generated teacher utterances may contain bias,
which may become apparent particularly in exer-
cises or chit-chat.
In this project, we took steps to avoid profanity
in the AI-generated responses, but similar protec-
tion against bias should be put into place. Addi-
tionally, human evaluators should be used to assess
the quality of the AI-generated responses and to
identify any potential biases.
We recognize that language models like GPT-
3.5-turbo are trained on large datasets that re-
flect the biases and prejudices present in society.
As there is always a risk of perpetuating these bi-
ases when using generative AI for dialogue sys-
tems it is important to evaluate the AI-generated
responses for potential biases and to take steps to
correct them.
",bias,,bias,2023,"statement, concerns"
756,"We construct both kNN-MT and subset kNN-MT
datastores from open datasets; therefore, if their
datasets have toxic text, kNN-MT and our sub-
setkNN-MT may have the risk of generating toxic
contents.
",toxic content,,toxic content generation,2023,statement
757,"All the data used in this paper are publicly avail-
able and are used under the following licenses: the
Creative Commons BY-NC-ND 4.0 License andCreative Commons Attribution 4.0 International
License, the TED Terms of Use, the YouTube’s
Terms of Service, and the BBC’s Terms of Use.
The data is collected from TED and BBC and con-
tain thousands of speakers from a wide range of
races. To protect the anonymity, only mouth area
of speaker is visualized wherever used in the paper.
",no ethical concerns,,no ethical concerns,2023,statement
758,"All the data used in this paper are publicly available
and are used under the following five licenses: the
Creative Commons BY-NC-ND 4.0 License and
Creative Commons Attribution 4.0 International
License, the TED Terms of Use, the YouTube’s
Terms of Service, and the BBC’s Terms of Use. The
data is collected from TED and BBC and containthousands of speakers from a wide range of races.
To protect the anonymity, only the mouth area of a
speaker is visualized wherever used in the paper.
",no ethical concerns,,no ethical concerns,2023,statement
759,"In this paper, we created a probing dataset TEMP-
REASON for temporal reasoning evaluation. The
dataset is constructed based on the matching of
Wikidata KB and Wikipedia articles. This approach
is commonly used for distantly supervised data con-
struction. The Wikidata KB is under the public
domain4and the Wikipedia articles are licensed un-
der the Creative Commons AttributionShareAlike
3.0 Unported License5. Therefore, we are able
to adapt these data to construct our dataset. We
will also release our data under the same license
as Wikidata. The scope of our dataset is purely for
scientific research of language models’ temporal
reasoning capability. However, the contexts from
the Wikipedia articles may contain improper con-
tent. The adoption of such content is not a decision
of the authors, and all content in the dataset does
not reflect the views or stances of the authors of
this paper.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
760,"Following the ACM Code of Ethics and Profes-
sional Conduct we evaluate the ethical impact of
the work presented in this paper. Our work aims
at broadening the accessibility of communication
technology. Spontaneous spoken language is the
least limiting and exclusive mode of interacting
with an information system. This mode does not
require any digital competencies or expensive re-
sources. The ability to correctly process sponta-
neous human conversations opens access to tech-
nology to stakeholders who might have been previ-
ously excluded. We strive to diminish discrimina-
tion resulting from biased training datasets, which
may cause specific individuals to be disproportion-
ally mistranscribed due to their accent, dialect, or
speech impediments. As digital voice applications
become increasingly integrated into society’s in-
frastructure, we feel the need to improve the quality
of statistical models processing spoken communi-
cations continuously.
The ability to better process and understand spo-
ken human conversations carries the significant
ethical risk associated with clandestine eavesdrop-
ping by adversarial agents. Correct recognition of
spoken names of people, places, organizations, or
events, can be malevolently used by authoritarian
government agencies trying to suppress free speech.
Recognition of names of products or services may
be utilized by marketers for non-consensual profil-
ing. Thus, it is in the best interest to foster public
awareness and understanding of computing, the au-
tomatic processing of spontaneous speech, and its
consequences.
","bias, misuse",,"bias, misuse",2023,"concerns, actions, advice, suggestions"
761,"In Section 3.1, we have discussed our procedures to
identify and remove potential harmful content and
user privacy information. However, it is important
to also consider the broader ethical implications
of using AI in collaborative storytelling. These
include issues such as ensuring fair and unbiased
representation, protecting data privacy, and prevent-
ing the use of AI-generated content for harmful
purposes. For example, AI-generated stories orcharacters may perpetuate stereotypes or reinforce
societal biases if they are trained on biased data.
Therefore, it is crucial to consider and address these
ethical issues in order to create inclusive and re-
sponsible AI-generated stories that do not harm
individuals or groups.
",bias,,bias,2023,"concerns, actions, advice"
762,"In the present paper, we have argued against adding
languages if practical implementation costs are a
relevant constraint. We acknowledge that this rec-
ommendation may push NLP researchers and en-
gineers towards constructing models specifically
for high-resource languages, which would further
the coverage gap between low- and high-resource
languages.
Nonetheless, it must be stressed that our experi-
ments say nothing of linguistic diversity, as we have
ensured that even our smallest dataset ( opus-03 )
would contain maximally different languages. Also
relevant to the discussion at hand is that one sce-
nario where practical implementation costs are a
known constraint is that of developing low-resource
languages systems and NLP tools. We believe that
providing evidence as to which approach is most
effective can prove valuable in such scenarios as
well, so as to ensure that efforts can be focused
on the most viable path towards endowing lower-
resource languages with more efficient and suitable
tools.247
",no ethical concerns,,no ethical concerns,2023,statement
763,"The authors have used only existing datasets and do
not identify any elements for ethical considerations.
",no ethical concerns,,no ethical concerns,2023,statement
764,"In this study, we honor the ethical code in the ACL
Code of Ethics.
",no ethical concerns,,no ethical concerns,2023,statement
765,"LiPoR shares similar concerns with other contem-
porary approaches for performing commonsense
reasoning. Specifically, because LiPoR exploits
the knowledge already present in pretrained lan-
guage models, it can potentially reinforce existing
harmful biases in such models.
",bias,,bias,2023,"statement, concerns"
766,"All of the training data used in our experiments are
official releases of publicly available benchmarks.
In addition, the toxic word lists used to measure tox-
icity are obtained from the public FLORES repos-
itory which requires a password to access, thus
reducing the risk of hacking by a malicious user
or adversarial bot. In addition, as for the issue
of hallucinated toxicity discussed previously, we
note that our work also has the potential to address
other problematic translation behaviors, such as
hallucinated bias.
9
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
767,"In this paper, we propose a method for automat-
ically identifying groups of data that are under-
performing due to a lack of training examples.
It is important to note that these underperform-
ing groups may be related to marginalized demo-
graphic groups, which may be underrepresented in
the data. By identifying these groups, our work is
able to reveal potential discriminatory behaviors
in NLP models and facilitate bias mitigation by
augmenting these underrepresented groups. How-
ever, there is also the risk that malicious actors may
exploit this information and create adversarial ex-
amples that further bias the model. To address this
concern, we suggest involving the user audience or
implementing fairness regulations in the interactive
procedure to prevent such behaviors. Finally, it’s
worth noting that our model relies heavily on large
language models to improve the performance of
challenging groups as a result if some groups are
not represented in LLMs our method is unable to
increase their performance.
","bias, misuse",,"bias, misuse",2023,"concerns, advice"
768,"The reliability of language models is crucial to
the stable deployment of real-world NLP applica-
tions. For example, the computer-aided resume
recommendation system and neural conversational
AI should provide trustworthy predictions because
they are intimately related to the issue of trust in
new technologies. In this paper, we propose a sim-
ple but effective method called POE for OOD detec-
tion tasks. We introduce a novel OOD construction
pipeline without any external OOD samples to train
a rejection network. We hope our work to provide
researchers with a new methodological perspective.
",no ethical concerns,,no ethical concerns,2023,statement
769,"The reliability of deep-learning models is crucial
to the stable deployment of real-world NLP appli-
cations. For example, the computer-aided resume
recommendation system and neural conversational
AI system should produce trustworthy predictions,
because they are intimately related to the issue of
trust in new technologies. In this paper, through
extensive empirical analysis, we address diverse
calibration techniques and provide a detailed exper-
imental guideline. We hope our work will provide
researchers with a new methodological perspective.
",no ethical concerns,,no ethical concerns,2023,statement
770,"Our Life Event Dialogs dataset is an extension of
an existing public dataset DailyDialog, with all
speakers being anonymized in the original release.
In other words, our dataset does not contain any
personally identifiable information that would in-
fringe on someone’s privacy. In this work, we will
only release the life event annotations for research
purposes. The dialogues in DailyDialog will not
be included in LED, but one can access the full
DailyDialog dataset from the author’s website.2
Our dataset is constructed upon a considerable
amount of human annotation. We recruited three
annotators and paid them a local hourly wage for
the time they spent. The annotation period spanned
1.5 months and resulted in 1,003 annotated conver-
sations (including conversations without events).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
771,"We ensure that our work complies with the ACL
Ethics Policy.
",no ethical concerns,,no ethical concerns,2023,statement
772,"We considered some ethical aspects while scraping
the data. We requested data at a reasonable rate
without any intention of a DDoS attack. Moreover,
for each website, we read the instructions listed in
robots.txt to check whether we can crawl the in-
tended content. We tried to minimize offensive
texts in the data by explicitly crawling the sites
where such contents are minimal. Further, we re-
moved the Personal Identifying Information (PII)
such as name, phone number, email address etc
from the corpus.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
773,"In our data scraping process, we took into account
ethical considerations. We obtained data at an ap-
propriate pace, avoiding any potential DDoS at-
tacks. Additionally, we eliminated any Personal
Identifying Information, such as names, telephone
numbers, and email addresses, from the data set.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
774,"PlotQA charts contain only factual information
which is openly available in public domain and
is not (i) specific to any individual or (ii) offensive.
The solution provided in the paper is agnostic to the
domain of the data. Like any QA task, to avoid the
risks involved in critical domains such as finance,
healthcare or medicine, we would have to calibrate
the model or need human intervention, such that
the errors are not propagated to the downstream
tasks.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
775,"Controllable dialogue generation(CDG) is an essen-
tial task in Natural Language Processing (NLP) and
has been widely studied for decades, which aims
to guide dialogue generation toward the desired at-
tributes such as emotions, acts, and personas. In the
open-domain dialogue scenario, CDG can gener-
ate emotional and diverse responses to enhance the
user’s sense of participation. In the task-oriented di-
alogue scenario, CDG can generate responses that
meet the user’s needs according to the user’s intent.
However, most previous works focus on single-
attribute generation where there is only one at-
tribute label like happiness in emotion and pay less
attention to the multi-attribute generation, which
is a more practical setting. Different from single-
attribute, the control signal of the multi-attribute
generation is a combination of multiple values from
different attributes, which faces the challenge of
lacking sufficient annotated attribute-specific data.
Therefore, we explore the compositional gener-
alization for multi-attribute controllable dialogue
generation where a model could learn from seen attribute values and generalize to unseen combina-
tions. We also design a novel and general reference-
free evaluation framework to unify the evaluation
of different granularity attributes. The experimen-
tal results prove the effectiveness of our model and
evaluation framework. Besides, there is no huge
biased content in the datasets and the models. If
the knowledge base is further used, the biased con-
tent will be brought into the generated responses,
just like biased content posted by content creators
on the Web which is promoted by a search engine.
To prevent the technology from being abused for
disinformation, we look forward to more research
effort being paid to fake/biased/offensive content
detection and encourage developers to carefully
choose the proper dataset and content to build the
knowledge base.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
776,"IMAGE CODE(Krojer et al., 2022) is an open data
set used for scientific research. For ablation studies
in the test set, we hired masters and undergraduate
students from the research group to re-annotate the
label of the test set. We have informed the cre-
ators of the data set and only conducted scientific
research.
Acknowledge
We thank the anonymous reviewers for their con-
structive comments, and gratefully acknowledge
the support of Natural Science Foundation of China
(No.62006061, 61872107) and the Stable Sup-
port Program for Higher Education Institutions
of Shenzhen (No.GXWD20201230155427003-
20200824155011001).
",no ethical concerns,,no ethical concerns,2023,statement
777,"Dataset & Annotation SAFECONV is proposed
to help reduce unsafe behavior in a conversation.
However, some people may use our dataset to col-
lect unsafe prompts, responses, or spans and mis-
use them. This is a common issue for all public
datasets regarding toxicity or safety. We believe
that our dataset creates more value than risks. Be-
sides, there is no leakage of personal information
because our data sources, LCCC-base (Wang et al.,
2020) and PchatbotW (Qian et al., 2021) have al-
ready been preprocessed to remove personal in-
formation by researchers of previous work (see
their papers for details). Also, though our dataset
contains more instances compared to previously
proposed datasets, the dialogues are mostly from
social media and may not cover types of conversa-
tional unsafe behavior found in other media. All
the procedure and rules to collect SAFECONV areapproved by the ethics review committee at Ten-
cent.
Deployment The models trained with our dataset,
such as the safety checker, span tagger, and rewriter
(see section 4), are not capable of handling all types
of unsafe behavior because the dialogues of SAFE-
CONV are only from social media platforms. In
addition, though SAFECONV is designed to build a
more civil conversational environment, there may
exist wrong usages of the dataset, such as training
a rewriter that converts safe responses to unsafe
ones and using the trained safety checker or span
tagger to gather unsafe expression for misconduct.
SAFECONV is available to the public under a usage
agreement for research and related purposes only
and we urge people interested to use it ethically.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
778,"This research was conducted in accordance with the
ACM Code of Ethics. The ethical considerations
during the dataset collection process are discussed
in Section 2.
",no ethical concerns,,no ethical concerns,2023,statement
779,"This research was conducted in accordance with
the ACM Code of Ethics.",no ethical concerns,,no ethical concerns,2023,statement
780,"The AltCLIP approach presents an innovative way
of building robust multilingual multimodal repre-
sentation models while minimizing the need for
energy-intensive GPU training, promoting a more
sustainable approach. Additionally, it allows for
greater accessibility as it does not require extensive
computational resources to implement. Further-
more, our model was trained using open-sourced
data and our model is open-sourced to promote
transparency and reproducibility. However, we
have not carefully investigated the training data
we used, such as LAION (Schuhmann et al., 2022).
The data may contain unsafe or biased text and/or
images. It is important to note that models pre-
trained on it have the potential to reproduce sensi-
tive training data. It is crucial to use this method
responsibly and ethically to ensure it contributes to
safe applications.
","bias, misuse",,"bias, misuse",2023,"statement, actions, concerns, advice"
781,"Intended Use. VisKoP is designed for users to
edit their knowledge base queries with graphical
elements.
Potential Misuse. As we count, there are
339,531 human female entities and 1,458,903
male entities in total. It can lead to gender bi-
ased answers on the grounds that a number of fe-
males do not exist in the KB. This problem stems
from the imbalanced data (Wikidata), and can be
solved when Wikidata includes more female en-
tities. Therefore, it’s important to allow users to
debug the knowledge base in future work.
Data. The VisKoP is built on a high-quality sub-
set of Wikidata, which attributes to the intelligence
of the crowd.
User Study. The participants in the user study
part are volunteers recruited from graduate stu-
dents majoring in engineering. Before the user
study experiments, all participants are provided
with detailed guidance in both written and oral
form. The only recorded user-related information
is usernames, which are anonymized and used as
identifiers to mark different participants.
","bias, misuse",,"bias, misuse",2023,"statement, concerns, advice"
782,"Our proposed dataset, KORC, is constructed with
the knowledge guidance from Wikidata. As a
crowd-sourced knowledge base, it is possible that
Wikidata contains bias knowledge and even poi-
sonous information. For example, Wikidata con-
tains more information in the English. It is possible
that KORCalso inherit the bias from Wikidata.
Another ethical concern raises from the payment
of our annotators. All the annotators are payed
equally according to the number of documents and
questions they annotated. We hope that KORCcan
be properly used to guide the development of deep
text understanding models after we release it.
",bias,,bias,2023,"concerns, actions"
783,"The SelfAware dataset, meticulously curated to
evaluate LLMs’ ability to discern unanswerable
questions, is composed of unanswerable questions
extracted from sources such as Quora and How-
StuffWorks, alongside answerable questions pro-
cured from three distinct open datasets. Every ques-
tion was thoroughly examined for relevance and
harmlessness. To ensure content validity, three an-
notation analysts, compensated at local wage stan-
dards, dedicated regular working hours to content
review.
Throughout our research process, we under-
scored the significance of privacy, data security,
and strict compliance with dataset licenses. In
order to protect data integrity, we implemented
anonymization and content filtration mechanisms.
Our adherence to OpenAI’s stipulations remained
unyielding for the usage of GPT-3 and InstructGPT
models, and likewise for Meta’s terms pertaining
to LLaMA models. We rigorously vetted the li-
censes of the three publicly available datasets for
compliance, ensuring that all our research method-
ologies were in alignment with ethical standards at
the institutional, national, and global levels.Adhering to the CC-BY-SA-4.0 protocol, the
dataset, once publicly released, will be reserved
exclusively for research purposes. We pledge to
promptly and effectively address any concerns relat-
ing to the dataset, while concurrently anticipating
researchers to maintain high ethical standards in
their utilization of this data.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice"
784,"The proposed method has no obvious potential
risks. All the scientific artifacts used/created are
properly cited/licensed, and the usage is consistent
with their intended use.
",no ethical concerns,,no ethical concerns,2023,statement
785,"Pretraining from scratch and further pretraining
such as DAPT need large-scale unlabeled corpus
to learn general knowledge, which results in cor-
responding greenhouse emissions due to energy
consumption (Strubell et al., 2019). However, as
shown in Section 5, our new efficient algorithms
greatly increase the data efficiency of PTMs, re-
ducing these harms as well as the various harms
associated with labor for data collection. Our
work introduces a new subset selection algorithm
but leverages pre-existing datasets and models.
Overall, this work inherits some of the risks of the
original work upon which it is implemented, (such
as bias (Bender et al., 2021) or privacy leakage
(Carlini et al., 2021).
","bias, privacy",,"bias, privacy",2023,statement
786,"The human evaluation is conducted by the em-
ployed workers, who does not involve privacy is-
sues. We use public datasets to conduct our experi-
ments. Existing packages involved in this work are
displayed in the appendix.
Acknowledgment
This work was supported by UniDT’s Cognitive
Computing and Few Shot Learning Project.
",no ethical concerns,,no ethical concerns,2023,statement
787,"Natural Language Processing (NLP) models that
perform poorly on a minority group have raised
a lot of concerns within the research community
and broader society in recent years. In this work,
the proposed Q-Diversity is a versatile method that
could be employed to train a robust model across
groups even when the group information is not
available. This is a rather practical scenario as
the group information is almost missing during the
data collection. We believe that our work is a step
towards a suite of algorithms capable of solving
a broader class of group DRO problems at scale.
Moreover, such an algorithm will empower NLP
researchers and engineers to create more reliable
and ethical systems.
",no ethical concerns,,no ethical concerns,2023,statement
788,"Our research aims to benefit the efforts in deliv-
ering domain-adaption technology to data-limited
settings. As a key task in NLP, named entity recog-
nition has broad applications, e.g., machine transla-
tion, question answering, and potentially protecting
endangered languages. Compared with many pre-
vious studies, we stress the importance of diversity
in the sense that our experiments cover seven do-
mains, including five lower-resource domains from
the CrossNER dataset. Hoping that our work can
contribute to extending modern NLP techniques
to the lower-resource named entity recognition set-
ting. The three datasets we use are both publicly
available. To our best knowledge, the data do not
contain any sensitive information and have no fore-
seeable risk.
",no ethical concerns,,no ethical concerns,2023,statement
789,"Since pre-trained speech models usually show bet-
ter performance on native (L1) speech automatic
speech recognition (ASR) due to the nature of the
pre-training data used, this work have contributed
to improve the ASR performance for non-native
(L2) English speakers and mitigating the perfor-
mance gap between them. This has the potential
to construct a fair ASR machine well-operating
not only on L1 English speakers but L2 speakers,
which is an important feature to have for its de-
ployment in real-life. Additionally, since we utilize
the pre-trained model, it is possible to have ethical
issue depending on the pre-trained model.
",no ethical concerns,,no ethical concerns,2023,statement
790,"In this paper, we have focused on exemplification
modeling using data (SemCor, Oxford Dictionary,
BookCorpus) and models (BART) that may contain
biases related to attributes like gender, race, or
disability. It is possible that these biases surface in
sentences generated using the techniques proposed
in this paper (Nadeem et al., 2021; Liang et al.,
2021), because we do not actively focus our efforts
on the removal of such biases. For this reason,
sentences generated by our proposed models or the
baseline should be used with caution in order to
prevent perpetuation of harmful stereotypes.
",bias,,bias,2023,"concerns, advice"
791,"In this work, we focus on the problem of synthetic
text generation with formal privacy guarantees. Ourgoal is to generate synthetic text that preserves the
statistical properties of the original text while also
protecting the privacy of individuals. We take the
issue of privacy very seriously and have designed
our method to ensure that it meets the highest ethi-
cal standards. In particular, we have incorporated
differential privacy, which is the gold-standard pri-
vacy mitigation technique employed in industry
and by the US census bureau, to ensure that the
synthetic generations do not compromise the pri-
vacy of individuals present in the original data. We
also recognize that synthetic text generated by our
model has the potential to be misused, and we en-
courage responsible and ethical use of our model.
We encourage researchers and practitioners to con-
sider the ethical implications of the method and to
follow best practices in data privacy.
",no ethical concerns,,no ethical concerns,2023,"actions, advice"
792,"RISE is built upon pre-trained language models.
Any biases within these models may possibly inﬂu-
ence the scoring of summarization models, in that
it is possible biases may cause the models to rate
one summary better than another.
",bias,,bias,2023,statement
793,"In this work, we leverage a synthetic dataset that
is generated using GPT-3 and verified by human
annotator. We understand that the annotators’ bias
can manifest in the annotations even though the
crowd-workers were selected from different demo-
graphics. Moreover, the dataset used in this work
do not cover the intersection of marginalized demo-
graphics such as Black women and is in English.
Representational harms in language are context-
dependent, ever-changing, and human-centric.
Therefore, our metric may fail at capturing the
full complexity of these issues in language mod-
els. Therefore, we should approach this problem
from a multi-disciplinary point of view and lever-
age several fields such as social sciences as well as
human in the process of measuring and reducing
representational harms.
Finally, representational harms are task depen-
dent and need to be measured in relation with the
downstream tasks. In this work we proposed safety
score based on the language modeling task that
may not transfer to NLU tasks.
","bias, lack of diversity, lack of inclusivity, annotator bias",,"bias, lack of diversity, lack of inclusivity, annotator bias",2023,concerns
794,"All datasets used in this work have been previously
published. Multimodal datasets frequently include
social biases (Meister et al., 2022), and we expect
the models trained on them to reflect the biases
in these datasets. Datasets also include images of
people, and there is no mechanism for people to
remove themselves from these datasets.
Multimodal models have many downstream uses.
Some examples of beneficial applications include:
more advanced image and video retrieval, visual
description systems to aid the visually impaired,
and interfaces which allow users to more seam-
lessly interact with smart home devices. Harmful
applications might include surveillance, especially
when imagery of people is being used without their
consent, or fine-tuning a model to retrieve harmful
content, such as pornographic material.
In this work, we aim to understand how models
perform on fine-grained tasks which highlights cur-
rent failure modes of our models. We hope insights
from our work can inspire (i) novel models which
perform well on a broad set of fine-grained tasks, as
well as (ii) more high quality data to stress test our
models. We hope our work also helps those who
might use multimodal models in downstream ap-
plications better anticipate how well these models
might perform on their tasks.
","bias, privacy",,"bias, privacy",2023,concerns
795,"Since the Jigsaw dataset is unclean, the model
trained on this corpus may also output some toxic
and offensive expressions. Besides, our model may
also be utilized to produce toxic and harmful con-
tent by simply setting the attribute label as toxic,
which would take the risk of producing and propa-
gating harmful information. Also, topic/sentiment-
controlled generated text may contain some so-
cially biased, offensive, or politically sensitive ex-
pressions. Besides, since our model significantly
improves the controllability of generated text, it
is likely to produce more plausible texts like fake
news and movie reviews, which could possibly be
utilized to produce and propagate disinformation.
However, these generated texts can also be used
as pseudo data in data augmentation for fake news
detection and thus have the potential to increase
the current fact-checking and fake news detection
model.
","toxic content, bias, misinformation",,"toxic content, bias, misinformation",2023,concerns
796,"The texts we have used for continued pre-training
come from Wikipedia dumps and the Newsela Cor-
pus. Using Wikipedia dumps requires following the
CC-BY-SA license and GFDL. Using Newsela Cor-
pus requires authorization, and we have received
it.
This paper contains a human evaluation. We
hire three experienced workers to perform it. In
the recruiting process, we follow a first-come, first-
served order. We pay much more than the local
minimum hourly rate.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
797,"Our work aims at capturing various social biases in
Hindi social media posts and demonstrates the an-
notation quality on biases in one of existing dataset.
We briefly discuss the annotation guidelines given
to the annotators for the task. Also, studies of social
biases come with ethical concerns of risks in de-
ployment (Ullmann and Tomalin, 2020). As these
biased news articles or social media posts can cre-
ate potentially harm to any user or community, it is
required to conduct this kind of research to detect
them. If done with precautions, such research can
be quite helpful in automatic flagging of users and
news firms creating such contents.
Researchers working the problem of social bias
detection on any form of text would benefit from
the dataset we have collated and from the infer-
ences we got from multiple training strategies.
",no ethical concerns,,no ethical concerns,2023,statement
798,"We choose original sentences from the PFR corpus,
which has been released to the public. There are no
intellectual property disputes for our data source.
All simplifications are collected from workers we
employ, and adhere to the relevant code of ethics.
We pay annotators a fair salary that matches their
workload.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
799,"In this section, we discuss the intended use and
energy saving considered in our paper.
Intended Use. In this paper, we consider beyond
the textual attack-and-defense arms race and high-
light the role of adversarial attacks in robustness
evaluation. We design a systematic robustness eval-
uation paradigm to employ adversarial attacks for
robustness evaluation. We first summarize deficien-
cies in current works that limit the further use of
adversarial attacks in practical scenarios. Then we
propose a standardized paradigm to evaluate the
robustness of models using adversarial attacks. We
also develop an extensible toolkit to instantiate our
paradigm.
Energy Saving. We describe our experimental
details to prevent other researchers from unneces-
sary hyper-parameter adjustments and to help them
quickly reproduce our results. We will also release
all models we use in our experiments.
",no ethical concerns,,no ethical concerns,2023,statement
800,"All the datasets included in our study are publicly
available (SST2, MR, CR, MIT, ATIS), and all
the models are publicly available. We would like
to state that the contents in the dataset do NOT
represent our views or opinions.
",no ethical concerns,,no ethical concerns,2023,statement
801,"In this section, we discuss the potential broader
impact and ethical considerations of our paper.
Intended Use. In this paper, we design a general
framework to adapt existing gradient-based meth-
ods in CV to NLP, and further, propose a decision-
based textual attack method with impressive per-
formance. Our motivations are twofold. First, we
attempt to introduce adversarial attack methods of
CV to NLP, since image attack methods have been
well-explored and proved to be effective, therefore
helping these two ﬁelds better share research re-
sources hence accelerating the research process on
both sides. Second, we hope to ﬁnd insights into
the interpretability and robustness of current black-
box DNNs from our study.
Potential Risk. There is a possibility that our at-
tack methods may be used maliciously to launch
adversarial attacks against off-the-shelf commer-
cial systems. However, studies on adversarial at-
tacks are still necessary since it is important for
the research community to understand these pow-
erful attack models before defending against these
attacks.
Energy Saving. We will public the settings of
hyper-parameters of our method, to prevent people
from conducting unnecessary tuning and help re-
searchers quickly reproduce our results. We will
also release the checkpoints including all victim
models to avoid repeated energy costs.
",misuse,,misuse,2023,"concerns, actions"
802,"We note three points on ethics. First, we recognize
that appropriateness is a value judgment, and there-
fore our data is limited here by the viewpoints of
the annotators. Multiple works on offensive lan-
guage have shown that the values and identities of
annotators can bias the judgments and potentially
further marginalize communities of practice whose
views and norms are not present (Sap et al., 2019;
Garg et al., 2022). We have attempted to mitigate
this risk by adding diversity to our annotator pool
with respect to gender, age, and culture, yet our lim-
ited pool size necessitates that not all viewpoints
will be present. Given that we show relationships
do matter in judging appropriateness, we hope that
future work will add diversity through new addi-
tions and data to study relationships. We will also
release demographic information on annotators as
a part of our dataset to help make potential biases
more explicit and more easily addressed.
The annotators themselves were authors of the
study and were compensated as a part of their nor-
mal work with a living wage. Due to the nature of
our ﬁltering, the vast majority of our content was
not explicitly toxic. Nonetheless, some comments
did contain objectionable messages, and annotators
were provided guidance on how to seek self-care if
the messages created distress.
With any new tool to identify offensive or abu-
sive language comes a dual use by an adversarial
actor to exploit that tool to ﬁnd new ways to harass
or abuse others while still “abiding by the rules.”
Our work has shown that relationships are effective
context (and features) for identifying previously-
unrecognized inappropriateness. This new capabil-
ity has the beneﬁt of potentially recognizing more
inappropriate messages before they reach their des-
tination. However, some adversaries could still use
our data and model to screen their own messages
to ﬁnd those that still are classiﬁed as appropriate
(while being inappropriate in practice) to evade de-
tection. Nevertheless, given the new ability to iden-
tify context-sensitive offensive messages—which
we show can represent a substantial percentage of
conversation (Section 6)—we view the beneﬁts as
outweighing the risk.","annotation bias, misuse",,"annotation bias, misuse",2023,"concerns, actions"
803,"Since event argument extraction only requires pre-
dicting arguments from the given text, the risk of
generating toxic languages is relatively low as long
as the given test is not toxic. This is because the
prediction can be grounded in the input sentence,
eliminating potential toxic tokens that did not ap-
pear in the original sentence. However, discrim-
ination and bias are possible, as observed in the
foundational LLMs we used (Brown et al., 2020;
Chen et al., 2021; Ouyang et al., 2022), which we
refer to Brown et al. (2020) for detailed discussion.
",bias,,bias,2023,concerns
804,"All the experiment data is from PhysioNet10. To
get full access to the training and test data in this
9https://chat.openai.com/
10https://physionet.org/content/
bionlp-workshop-2023-task-1a/1.0.0/
task, it is required to get the training for CITI Data
or Specimens Only Research11.",no ethical concerns,,no ethical concerns,2023,statement
805,"We carefully select the dialogue corpora used in
this paper to control for potential biases, hate-
speech and inappropriate language by using hu-
man annotated corpora and professionally curated
resources. Further, we consider the privacy of dia-
logue partners in the selected datasets by replacing
names with generic user tokens.
Since we are investigating the nature of the dis-
course structures captured in large PLMs, our work
can be seen as making these models more transpar-
ent. This will hopefully contribute to avoid unin-
tended negative effects, when the growing number
of NLP applications relying on PLMs are deployed
in practical settings.
In terms of environmental cost, the experiments
described in the paper make use of RTX 2080 Ti
GPUs for tree extraction and A100 GPUs for BART
fine-tuning. We used up to 4GPUs for the parallel
computation. The experiments on corpus STAC
took up to 1.2hours for one language model, and
we tested a dozen models. We note that while
our work is based on exhaustive research on all the
attention heads in PLMs to obtain valuable insights,
future work will able to focus more on discourse-
rich heads, which can help to avoid the quadratic
growth of computation time for longer documents.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
806,"The work presented here is part of a project which
was reviewed and approved by our institution’s
ethical committee. This committee provided useful
guidance regarding this specific work concerning
the selection of news sources, and the annotation
process. The recommendations were integrated in
the dataset creation process.
One potential issue is that the dataset includes
negative sentiments expressed about public figures.
This is also applicable to any TSC dataset that fo-
cus on politics. Sentiment expressions were col-
lected from publicly available news sources which
have a right to freedom of expression in the Eu-
ropean Union (EHCR Article 10). News articles
were collected from diversified newspapers, which
lean toward different parts of the political spectrum,
and this reduces the risk of mischaracterizing any
of the mentioned entities. Sentiment classification
datasets are needed in order to understand how
sentiment is expressed in the media, and thus con-
tribute to the characterization of societal debates.
",bias,,bias,2023,"concerns, actions"
807,"In this work, we have proposed a method for in-
corporating ER into DocRE. Our approach directly
supervises the weights of attention modules within
a Transformer-based PLM encoder. Inside the
research community, we hope our approach can
provide a new viewpoint on the explainability of
document-level relation extraction systems. Fur-
thermore, a better DocRE system will benefit the
research on other tasks, such as question answering
and reading comprehension. In the real world, a
DocRE system with good performance can help
extract useful information from unstructured text,
reducing human efforts and expenses. Further-
more, as our method is memory-efficient, it is also
friendly to the environment.
We also have demonstrated a use case of our
method in ER self-training, utilizing massive data
obtained from relation distant-supervision. Al-
though in this work, we directly adopt the data
provided by Yao et al. (2019), it is possible to ex-
tend the scale of data by utilizing numerous un-
structured texts. Utilizing a wide range of unstruc-
tured texts may expose our system to the risk of
vulnerable data, potentially biasing our system in
the wrong direction. To mitigate the problem, we
encourage performing data pre-processing to detect
and remove harmful contents before training.
",bias,,bias,2023,"statement, concerns, actions, advice"
808,"There are no ethical problems in this paper. All of
the datasets are publicly available.
",no ethical concerns,,no ethical concerns,2023,statement
809,"Our tool can benefit researchers from social sci-
ences that want to study biases in word embed-
dings or language models. It can also be used by
small companies that cannot train their own lan-
guage models and that want to study the biases
present in different pre trained language models
when deciding which to use in their products.
The metrics we use to measure bias are known
to have limitations (Badilla et al., 2021) and the
benchmarks existing in the area (Blodgett et al.,
2021). A potential risk of our tool is that users
assume that our tool can be used to show that a
model is not biased in a particular dimension with-
out considering the limitations of the metrics and
the benchmarks.
Finally, this work discusses how to involve dis-
crimination experts in the exploration of biases in
NLP and argues that this is important. This might
discourage researchers in NLP working on bias
analysis and mitigation to keep working in this
area because they do not have access to interdisci-
plinary experts. In this way, we could discourage
work in an area we believe is important. We think
different approaches are valuable in this area and
studying in more detail the metrics of the area is
very important and needs deeper technical exper-
tise. This might not require discrimination experts
if reliable benchmarks are available in the area.
Participation in our workshops involved an-
swering a pre-survey, a post-survey, and a 3-hour
hands-on in-person workshop. Participants were
volunteers and did not receive compensation.
EDIA does not censor the models, so words
that might be censored by other tools can be ex-
plored. In one of our workshops the participants
explored words associated to feminine sexuality
vs words associated with masculine sexuality and
found that feminine words were associated with
disease while sexual masculine words were asso-
ciated with health in the language model (Cañete
et al., 2020).
",misuse,,"misuse, disencouragement",2023,"concerns, actions"
810,"The scope of this work is to provide a principal
procedure for measuring the implicit valence and
stereotypical biases in image generations. The ex-
periments conducted involve generating images
that pertain to demographic groups, and all im-
ages were generated in compliance with the terms
of service and guidelines provided by the stable
diffusion’s license. The AI-generated images are
used solely for research purposes and no identities
are explicitly attributed to individuals depicted in
the images. People’s names are used to generate
images. We justify that these are common Ameri-
can names publicly accessible, and do not contain
any information that can uniquely identify an indi-
vidual.
",no ethical concerns,,no ethical concerns,2023,statement
811,"Similarly to Kennedy et al. (2022), we recognize
that our analysis involved the examination of data
containing a significant amount of hateful speech,
which can be emotionally taxing and distressing
for annotators. To address this concern, we pro-
vided our annotators with comprehensive informa-
tion about the task’s nature and the language and
content they would encounter.
Furthermore, we took measures to ensure that the
data we utilized for our analysis was gathered and
utilized ethically and responsibly. We de-identifiedthe data by eliminating tweet ids, user ids, and lo-
cation data, utilizing only the raw text to guarantee
that no personal data was accumulated or employed
in any manner.
",toxicity,,toxicity,2023,"concerns, actions"
812,"This paper analyses scientific literature at an ag-
gregate level. The ACL Anthology freely provides
information about NLP papers, such as their title,
authors, and year of publication. We do not make
use of or redistribute any copyrighted information.
All of the analyses in this work are at aggregate-
level, and not about individual papers or authors.
In fact, we desist from showing any breakdown
of results involving 30 or fewer papers to avoid
singling out a small group of papers.

",no ethical concerns,,no ethical concerns,2023,"concerns, actions"
813,"The goal of the proposed method is to understand
the temporal relation between events based on the
descriptions in the given text. What the method
can achieve in the most optimistic scenario is no
more than giving the same text to a human reader
and letting him or her explain the event relations.Therefore, the ethical concerns only come from
the data collection. In this paper, we only use pub-
licly available datasets which have already been
widely used in the research field. As for potential
application, as long as the user collects the training
data legally, the proposed method does not have
the potential to have a direct harmful impact.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
814,"This work is based on publicly available medi-
cal datasets (Devaraj et al., 2021a; Shardlow and
Alva-Manchego, 2022). As stated by the authors
of datasets, no personal identification informa-
tion were released. Current language technologies
generally—and automated simplification models
such as the one proposed in this work—still in-
troduce “hallucinations” and factual inaccuracies
into outputs; at present we would therefore recom-
mend against deploying fully automated generative
models for medical texts.
Acknowledgment
This work was supported in part by the UK Engi-
neering and Physical Sciences Research Council
(EP/T017112/1, EP/V048597/1, EP/X019063/1),
and the National Science Foundation (NSF) grant
1750978. YH is supported by a Turing AI Fel-
lowship funded by the UK Research and Innova-
tion (EP/V020579/1). This work was conducted
on the UKRI/EPSRC HPC platform, Avon, hosted
in the University of Warwick’s Scientific Comput-
ing Group. BCW was supported in this work by
the National Institutes of Health (NIH), grant R01-
LM012086. GP, JL, and JL, were supported by
the National AI Strategy award (Warwick/ATI):
‘METU: An Inclusive AI-Powered Framework Mak-
ing Text Easier to Understand ’.
",model inaccuracies,,model inaccuracies,2023,"statement, advice"
815,"In this work, we study and propose solutions to
resolve existing ambiguities in prompts given to
text-to-image generative models. In addition to
resolving ambiguities in prompts, this work not
only frames and analyzes fairness from a new and
different perspective, but it also results in more
faithful image generations aligned with end user
intention. These aspects can contribute to numer-
ous positive impacts to the research community.Not only one can generate more diverse images
through disambiguating fairness type ambiguities,
but our framework can also improve user satisfac-
tion by generating aligned images to end user’s
intention despite existing ambiguities in the pro-
vided prompts. Resolving ambiguities can also
avoid spread of misinformation and development
of fallacies. Despite the aforementioned positive
impacts, we also acknowledge the limitations as-
sociated with this work. We acknowledge that our
benchmark dataset is just a very small sample of
different types of ambiguous prompts that can be
provided to a system. In addition, for the fairness
type ambiguities, we only consider gender (male vs
female), skin color (dark vs light), and age (young
vs old). We acknowledge that these are only a
limited number of characteristics that can represent
identity of an individual and that we do not cover all
the cases possible. We agree that we do not cover
all the cases possible; however, our intent is to
showcase a few examples through our benchmark
(TAB) and highlight existing flaws associated with
these systems encountering ambiguous prompts.
In our experiments, we also utilize human anno-
tators. We ensure to provide appropriate guidelines
with a proper compensation to our workers (around
12$ per hour). We also utilize master workers based
in the United States with proper expertise (comple-
tion of more than 1000 HITs with an acceptance
rate above 85%). In addition, we provide the work-
ers the opportunity to raise any concerns about our
task. Based on the feedback, we believe that the
task and the pay was satisfactory to the workers.
We hope that our study can provide valuable in-
sights to the research community with the positive
implications out-weighting its limitations. We also
open-source our benchmark dataset for the com-
munity to benefit from our work. As future work,
researchers can investigate and propose better al-
ternatives than our proposed framework for resolv-
ing ambiguities in text-to-image generative models
along with extension of our work to semantic ambi-
guities in addition to the ones studied in this paper.
Our benchmark dataset can also serve as a valuable
resource for research in commonsense reasoning
studies in text-to-image generative models which
is less explored in our current work. We provide
information in our benchmark dataset (whether an
interpretation is commonsensical or not) which can
be accessible to interested researchers in this area.
",no ethical concerns,,no ethical concerns,2023,"statement, concerns, actions, advice, suggestions"
816,"We leverage prompt-tuning to control the ex-
tractability of memorized data from LLMs in an
open language generation task and explore two
settings; an attack and a defense. We acknowl-
edge that our attack methodology could be misused
by an adversary with white-box access to extract
memorized private information from a target large
language model. Our goal is to raise awareness
in the community to the possibility and severity
of this nature of attack. We hope that developers,
armed with this knowledge, can use relevant de-
fense mechanisms to avoid such potential misuse.
",misuse,,misuse,2023,"concerns, advice"
817,"•We have read and abide by the ACL Code of
Ethics8.
•Data Privacy : We follow the data privacy
measures in place at our company which in-
clude scrubbing personal identifiable informa-
tion (PII) from customer data and restricting
our use of customer data to improvements to
the services we provide them. We did not rely
on any external annotations.
•Intended Use by Customers : In the product
we highlight both high and low CSAT calls
for review by a supervisor to ensure employ-
ees receive a mix of positive and constructive
feedback. Since supervisors review calls, they
can adjust incorrect classifications produced
by the model.
8https://www.aclweb.org/portal/content/acl-code-ethics
•Potential bias: We sample subpopulations
of users and their customers and evaluate in-
ternally to ensure the model outputs are not
biased against specific groups.
•Carbon Footprint: We minimized the carbon
footprint of our experiments while meeting
the need for variability required by statistical
analysis. We achieved this by running 24 ex-
periments, each with different experimental
conditions, rather than running multiple ex-
periments with different random seeds within
each of the 24 conditions. In total the experi-
ments described in this paper represented less
than 500 hours of computation on a single
V100 GPU.
7 ",carbon footprint,,carbon footprint,2023,"statement, actions"
818,"We use the four XMC benchmark datasets which
are publicly available and widely used in research2.
The datasets with social tags (e.g., WIKI10-31K
andWIKI-500 K) may contain inappropriate vul-
garisms if they are not filtered out from the original
data processing.
",toxic content,,toxic content,2023,"statement, advice"
819,"The authors foresee no ethical concerns with the
research presented in this work.
",no ethical concerns,,no ethical concerns,2023,statement
820,"To the best of our knowledge, DEPLOT is of low
risk to the society since it is an information ex-
traction model that converts graphics information
from image to textual information in the form of
table. That said, when combined with LLMs, DE-
PLOT,LLM can demonstrate potential risk such as
generating toxic content similar to when LLMs are
used standalone. As a result, we should proceed
with caution when deploying DEPLOT,LLM in the
real-world and take necessary precautions such as
having a ﬁltering stage after the generation.
In terms of data used, all training and evaluation
data are either synthetically created using rules or
publicly available data on the web with appropriate
permissive licenses.
",toxic content,,toxic content,2023,"statement, concerns"
821,"To the best of our knowledge, MATCHAhas not
been trained on sensitive private information and
should be of low risk to generate harmful contents.
All pretraining and ﬁnetuning data are either syn-
thetically created using rules or publicly available
data on the web with appropriate permissive li-
censes.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
822,"We do not foresee any harmful or malicious misuse
of the technology developed in this work. The data
used to train models is about seeking information
about domains like restaurants, hotels and tourist
attractions, does not contain any offensive content,
and is not unfair or biased against any demographic.
This work does focus on widely-spoken languages,
but we think the cross-lingual approach we pro-
posed can improve future dialogue language tech-
nologies for a wider range of languages.
We fine-tune multiple medium-sized (several
hundred million parameters) neural networks for
our experiments. We took several measures to
avoid wasted computation, like performing one
run instead of averaging multiple runs (since the
numerical difference between different models is
large enough to draw meaningful conclusions), and
improving batching and representation that im-
proved training speed, and reduced needed GPU
time. Please refer to Appendix A.1 for more de-
tails about the amount of computation used in this
paper.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
823,"We use publicly available CHILDES data to build
our corpora (MAO-CHILDES). Please read more
about their terms before using the data.3We use
the dataset extracted from the CHILDES database
only for research purposes and not for commercial
reasons. We will release the dataset upon publica-
tion under the same license as CHILDES and this is
compatible with the license of CHILDES database
(Macwhinney, 2000). The results of this study are
reported on a single run as part of measures taken
to avoid computation wastage. We do not foresee
any harmful uses of this work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
824,"We do not foresee any harmful or malicious mis-
uses of the technology developed in this work. The
data used to train models is about seeking infor-
mation about domains like restaurants, hotels and
tourist attractions, does not contain any offensive
content, and is not unfair or biased against any
demographic. This work does focus on two widely-
spoken languages, English and Chinese, but we
think the cross-lingual approach we proposed can
improve future dialogue language technologies for
a wider range of languages.
We ﬁne-tune multiple medium-sized (several
hundred million parameters) neural networks for
our experiments. We took several measures to
avoid wasted computation, like performing one
run instead of averaging multiple runs (since the
numerical difference between different models is
large enough), and improving batching and repre-
sentation that improved training speed, and reduced
needed GPU time. Please refer to Appendix 5.2 for
more details about the amount of computation used
in this paper.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, suggestions"
825,"Our translation method replaces the manual work
needed to create multilingual dialogue datasets usu-
ally done via crowdsourcing. Instead, it requires
some computation time which can be an environ-
mental concern. However, in practice, such addi-
tional computing is small and much cheaper than
the cost of human annotation for the same amount
of data. The translation of the data set takes about
half an hour on an Nvidia TITAN V GPU. Training
takes about 6 hours on an Nvidia V100 GPU. We
did not use crowdworkers for this paper. The error
analysis was done by the authors.
",environmental_impact,,environmental_impact,2023,statement
826,"The paper investigates the missing skills problem
with the help of a graph-based framework by in-
corporating word-based embeddings that can be
insightful for other researchers in academia and in-
dustry. Any biases present in the dataset or embed-
ding model can creep into the proposed approach.
",bias,,bias,2023,statement
827,"In this work, we adapt a pragmatic-driven familiar-
ity heuristic to study factual and contextual errors
for familiar and unfamiliar entities. Our work re-
veals that description generation disproportionately
rely on context for unfamiliar entities leading to
incorrect predictions. Our heuristic for tagging fa-
miliarity is aimed at contrasting model errors. We
do not propose the use of this heuristic to filter
unfamiliar entities in end services.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
828,"We utilize various datasets in our experimental
analysis, including the UN v1.0 corpora (Ziemski
et al., 2016), the Chinese News, and the WMT
dataset (Bojar et al., 2017) in the classification
experiments, as well as the cLang8 (Rothe et al.,
2021), CoNLL14 (Ng et al., 2014), BEA19 datasets
(Bryant et al., 2019), NLPCC18 (Zhao et al., 2018),
Falko-MERLIN (Boyd et al., 2014) and RULEC-
GEC (Rozovskaya and Roth, 2019) in the GEC ex-
periments. All of these datasets are publicly avail-
able resources and acquired for research purposes.
We affirm our commitment to the responsible and
ethical use of data throughout this research paper.
The utilization of data in this study strictly adhered
to relevant legal and ethical guidelines.
",no ethical concerns,,no ethical concerns,2023,statement
829,"There are no ethics-related issues in this paper. The
data and other related resources in this work are
open-source and commonly used by many existing
studies.
",no ethical concerns,,no ethical concerns,2023,statement
830,"In this paper, our experiments adopt the widely
used EMPATHETIC DIALOGUES benchmark, an
open-source dataset collected from Amazon Me-
chanical Turk (MTurk) that does not contain per-
sonal information. We also ensure the anonymiza-
tion of the human evaluation. We believe that this
work honors the ethical code of ACL.
",no ethical concerns,,no ethical concerns,2023,statement
831,"In this paper, the ESConv dataset used in our ex-
periments is a publicly-available benchmark for
emotional support conversation, which does not
contain sensitive and personal information as well
as unethical language. Our work builds on this
dataset to study positive emotion elicitation to im-
prove the user’s mental state. Therefore, we focus
on constructing a dialogue system to provide emo-
tional support from families and friends in the daily
scenarios limited by this dataset rather than profes-
sional psychological counseling or psychological
treatment. For risky non-daily scenarios such as
self-harm or suicide-related conversations, we do
not claim that the dialogue system we built has
a treatment or improvement effect on them. Ad-
ditionally, we also ensure the anonymity of our
interactive human evaluation. We believe our work
meets ACL’s Code of Ethics.",no ethical concerns,,no ethical concerns,2023,statement
832,"In this work, we leveraged two publicly available
datasets. First, we used the Persona-Chat dataset,
which is collected by assigning a set of fixed pre-
defined persona sentences to workers. Therefore,
by participating in this dataset, workers were re-
quired not to disclose any personal information
(Zhang et al., 2018), which prevents issues regard-
ing the leakage of their privacy. Similarly, during
the collection of the ESConv dataset, participants
were asked to create imaginary situations and play
the role of a support seeker who is in that situation.
In addition, they were instructed not to provide per-
sonal information during their conversations with
the trained supporters (Liu et al., 2021). Regard-
ing the persona extractor, this module is trained to
infer and extract persona information solely from
what the user has mentioned in the conversation
rather than making assumptions about the user’s
background and character, further highlighting the
importance of user privacy in our research.
Regarding our experiments, we ensured that all
workers agreed to participate in the annotation
tasks. Moreover, as the workers were recruited
from the US, we ensured that they were paid above
the minimum wage in this country for successfully
completing our tasks. We acknowledge that using
trained dialogue models to provide support is a sen-
sitive subject and research on this topic should be
conducted with sufficient precautions and super-
vision. We also acknowledge that in their current
stage, such models cannot replace human support-
ers for this task (Sabour et al., 2022a). Thus, they
should not be employed to replace professional
counselors and intervention and interact with users
that suffer from mental distress, such as depression
or suicidal thoughts.
",no ethical concerns,,no ethical concerns,2023,"statement,actions, advice"
833,"The EmpatheticDialogues (Rashkin et al., 2019)
dataset for dialogue post collection, the GPT-J
model (Wang and Komatsuzaki, 2021), and the
BlenderBot model (Roller et al., 2021) are all
widely used in academic research, can be accessed
from HuggingFace Hub or official websites, and
are all in the English language as well as AUGESC .
Using the above public resources, the construction
ofAUGESC does not involve human participants
and thus does not collect any personal identifying
information.
We raise attention that AUGESC may possibly
contain toxic or biased contents, which cannot be
fully assessed in either automatic or human evalu-
ation ( §5). Future access to AUGESC should be
only for research usage and should NOT be used
for real-deployed systems, commercial purposes,
or any other usage than academic research. Anyone
using AUGESC in the research should be aware of
its limitations and should acknowledge and/or try
to mitigate them to the extent possible.
Our work strictly follows the task definition and
evaluation protocols ( §5 and 6) of the original
ESC paper (Liu et al., 2021), where the support
is provided through social interactions (e.g., be-
tween peers or friends) rather than professional
counseling. As mentioned in (Liu et al., 2021),
further efforts are still needed to probe the ethical
extent to which dialogue models can or should pro-
vide support. These protocols should also not be
used directly in fields other than the ESC task (i.e.,peer emotional support in daily life) that require
the guidance of professional researchers, such as
psychological counseling.
We also ethically conducted the human evalu-
ation. We transparently communicated with the
participants of our study intent and explicitly in-
formed them of the disclaimers before they partici-
pated. We paid the participants at the hourly wage
above $10/hour, going well beyond the local labor
compensation standard. We acknowledge that the
results of human evaluation could be affected by
the participants’ demographic and geographic char-
acteristics. This work has obtained study approval
from the Institutional Review Board (IRB).
","bias, toxic content",,"bias, toxic content",2023,"statement, actions, concerns, advice"
834,"An ethical concern associated with neural metrics
is the presence of unpredictable bias in the evalua-
tion process. Unlike traditional text-based metrics,
neural metrics pose challenges in mitigating eval-
uation bias due to their black-box nature, which
also introduces potential issues like gender bias in-
herent in pre-trained language models. While our
current study does not investigate the bias problem,
reducing uncertainty in the evaluation process may
help contribute to mitigating the potential risks as-
sociated with generating biased results.
Acknowledgment
This work was supported in part by the Sci-
ence and Technology Development Fund,
Macau SAR (Grant Nos. FDCT/060/2022/AFJ,
FDCT/0070/2022/AMJ), the National Natu-
ral Science Foundation of China (Grant No.
62206076), the Research Program of Guang-
dong Province (Grant No. 2220004002576),
Shenzhen College Stability Support Plan
(Grant Nos. GXWD20220811173340003,
GXWD20220817123150002), Shenzhen Sci-
ence and Technology Program (Grant No.
RCBS20221008093121053) and the Multi-year
Research Grant from the University of Macau
(Grant No. MYRG2020-00054-FST). This work
was performed in part at SICC which is supported
by SKL-IOTSC, and HPCC supported by ICTO of
the University of Macau. We would like to thank
the anonymous reviewers and meta-reviewer for
their insightful suggestions.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
835,"A target-oriented dialogue system may have the risk
of misusing it to guide users to malicious topics
actively. Since the proposed system tries to ensure
relevance with the dialogue context in the applica-
tion deployment, the possibility of the above mis-
use is small on the premise of checking the training
corpus.
All models in this paper are trained on the public
corpus. The used datasets do not contain personal
information or unethical language. We also ensure
the anonymization of the human evaluation.
",misuse,,misuse,2023,"statement, concerns, actions"
836,"Our multi-turn target-oriented dialogue model can
facilitate and quickly conduct dialogues with users.
It can be used in many applications, such as movie
recommendation, product recommendation, psy-
chological consultation, educational dialogue on a
particular topic, etc. All models in this paper are
trained on the public corpus. The used datasets
do not contain personal information or unethical
language. We also ensure the anonymization of the
human evaluation.
7
",no ethical concerns,,no ethical concerns,2023,statement
837,"From a general moral point of view, the generation
of personalized dialogue in a broad sense may in-
deed cause problems such as identity forgery and
the spread of false information. However, in this
study, personalized corpus and responses are lim-
ited to the scope of experiments, which are not
enough to threaten the real conversation.
Furthermore, all models in this paper are trained
on public corpus. The used datasets do not contain
unethical language. We also ensure the anonymiza-
tion of the human evaluation.
",no ethical concerns,,no ethical concerns,2023,statement
838,"This research uses publicly available data and
benchmarks for evaluation. We believe that this
research was conducted in an ethical manner and in
compliance with all relevant laws and regulations.
",no ethical concerns,,no ethical concerns,2023,statement
839,"We follow the ACL Code of Ethics. In our work,
there are no human subjects and informed consent
is not applicable.
",no ethical concerns,,no ethical concerns,2023,statement
840,"As an important domain of natural language pro-
cessing, information extraction is a common tech-
nology in our society. It is necessary to discuss the
ethical influence when using the extraction mod-
els (Leidner and Plachouras, 2017). In this work,
We develop a new universal IE framework, which
enhances the generalization ability in various sce-
narios. As discussed (Schramowski et al., 2019,
2022; Blodgett et al., 2020), pre-trained LMs might
contain human-made biases, which might be em-
bedded in both the parameters and outputs of the
open-source models. In addition, we note the poten-
tial abuse of universal IE models, as these models
achieve excellent performance in various domainsand settings after adapting to pre-training on large-
scale IE datasets, which allows the models to be
integrated into applications often without justifica-
tion. We encourage open debating on its utilization,
such as the task selection and the deployment, hop-
ing to reduce the chance of any misconduct.
","misuse, bias",,"misuse, bias",2023,"concerns, advice"
841,"Anticipation future action based on videos is an im-
portant for many applications such as assistive tech-
nologies, augmented reality etc. Our work demon-
strates that knowledge derived from text sources
can be used to further improve the performance
of video based action anticipation model. Even
though our proposed work is able to improve the
current state-of-art numbers on the standard bench-
mark datasets, the absolute performance is still low,
especially in the case where the action space is very
large. As such we would recommend to carefully
analyze the cost of erroneous prediction before de-
ploying the system for real world application.
Since the proposed method involves distilling the
knowledge gained by pre-trained language model
from text sources into a vision based model for
action anticipation, this can also transfer the bi-
ases that these languages models can learn from
the training text. As such data on which these
text-based teacher models are trained should be
analyzed for potential biases before deploying the
proposed system for actual application. Analysis
of bias propagation during knowledge distillation
and devising bias reduction techniques are some
potential extension of this work that we are highly
interested in.
9
","erroneous predictions, bias",,"erroneous predictions, bias",2023,"concerns, advice, suggestions"
842,"Our work adheres to the ACL Ethics Policy. Us-
ing our proposed models, we can scale corefer-
ence resolution to long documents while leveraging
transformer-based mention-pair scorers and with-
out substantially increasing memory consumption.
",no ethical concerns,,no ethical concerns,2023,statement
843,"As with other applications of NLP in the medical
domain, results of MSLR systems must be verified
by domain experts before they should be consid-
ered for use in clinical guidance. We do not intend
the system outputs included in our dataset and anal-
ysis to be used for such end applications, as this
would be clearly premature given the low quality
of generated summaries and our lack of ability to
assess the prevalence of factuality errors in these
summary texts. Nonetheless, we believe that medi-
cal MDS holds eventual promise, and it is of vital
importance that we study its challenges and how to
measure and detect quality issues in generated text.
",factuality errors,,factuality errors,2023,"statement, advice, suggestions"
844,"All experiments involving human evaluation in this
paper were exempt under institutional IRB review.
We fairly compensated each Upwork freelancer in-
volved in this study, at a rate of 15-20$ per hour
(respecting their suggested Upwork hourly wage).
For each round of annotation, we estimated the
average amount of time the task would take (by
running pilots among ourselves), and provided an-notators with the estimated time requirement. Most
freelancers finished the task within the time win-
dow, but sometimes exceeded it by 0.5-1 hr. We
compensated freelancers based on the actual time
they took and their hourly wage, rather than a fixed
amount per annotation.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
845,"This work has obtained clearance from author’s
institutional review board. The annotators for POI
and MIDV2020 are all paid full-time interns and
researchers hired by our institute, whose compen-
sation are determined based on the the salary guide-
lines of our institute. Among the datasets and anno-
tations released, POI only contains specimens with
dummy values, while MIDV is a synthetic dataset.
External data are accessed and used in compliance
with fair use clauses. We conduct experiments on
the private dataset PRV in a secure data zone with
strict access control, using auto-labeling scripts for
annotations.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
846,"In this section, we discuss the main ethical consid-
erations of Tell2Design (T2D): (1) Intellectual prop-
erty protection. The floor plans of the T2D dataset
are from the RPLAN (Wu et al., 2019) dataset. Our
dataset should be only used for research purposes.
(2) Privacy. The floor plan data sources are pub-
licly available datasets, where private data from
users and floor plans have been removed. Lan-
guage instructions are either generated artificially
or collected from Amazon Mechanical Turk, a legit-
imate crowd-sourcing service, and do not contain
any personal information. (3) Compensation. Dur-
ing the language instruction collection, the salary
for annotating each floor plan is determined by the
instruction quality and Mturk labor compensation
standard.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
847,"We honour and support the EACL Code of Ethics.
The datasets used in this work are well-known and
widely used, and the dataset pre-processing does
not make use of any external textual resource. In
our view, there is no known ethical issue. End-to-
end pre-trained dialogue generators are also used,
which are subjected to generating offensive context.
But the above-mentioned issues are widely known
to commonly exist for these models. Any content
generated do not reflect the view of the authors.",toxicity,,toxicity,2023,statement
848,"The CQA generation system proposed in this work
can augment the performance of CQA models, but
also introduce the following risks. Firstly, the ques-
tions are generated according to grounding docu-
ments. As a result, they might deliver social biases
and misinformation contained in the documents.
Secondly, the method increases the size of CQA
corpora and the computational cost of model train-
ing. Lastly, since the system can automatically
annotate unlabeled documents, it might reduce the
number of jobs in manual data annotation.
","bias, misinformation, unemployment, computational cost",,"bias, misinformation, unemployment, computational costs",2023,concerns
849,"All the experiments are conducted on publicly avail-
able datasets, which don’t include any private in-
formation. Our work doesn’t involve identity char-
acteristics or any gender and racial discrimination.
",no ethical concerns,,no ethical concerns,2023,statement
850,"All the experiments are conducted on publicly avail-
able datasets, which don’t include any private in-
formation. Our work doesn’t involve identity char-
acteristics or any gender and racial discrimination.
",no ethical concerns,,no ethical concerns,2023,statement
851,"Our work will not cause ethical issues and the
datasets used in this paper are publicly available.
",no ethical concerns,,no ethical concerns,2023,statement
852,"The construction of dataset. All videos in our
newly-introduced dataset are available on the Chi-
nese video sharing websites and are public to the
download. To avoid the potential ethical issues, we
carefully checked all videos in multiple aspects,as said in Section 3.1. We try to guarantee that
all videos do not involve any offensive, gender-
biased, political content and any other ethical is-
sues. During the annotation, we instruct annotators
to anonymize or remove the sensitive or private
information.
We recruit eight annotators that passed our ex-
aminations from the crowdsourcing platform, and
two quality inspectors from our research team. To
fairly paid the annotations, we first take an in-house
annotation to evaluate the speed and difficulty of
the large-scale annotations. Finally, we pay each
annotator $25-$30 per hour. Typically, it would
take around 2 hours to annotate a one-hour meet-
ing. So, the workers are compensated $50-$60 per
sample.
Applications. We will release the code and
dataset along with friendly instructions to support
its correct use. However, we still need to emphasize
that abstractive summarization is a kind of genera-
tion task, which is not as controllable as we think.
It still would generate some novel or unexpected
words occasionally. Therefore, further research on
the summarization faithfulness is warmly needed.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice, suggestions"
853,"The usage of the generated summary results from
legal opinions remains important. Abstractive sum-
marization models have been found to contain hal-
lucinated artifacts that do not come from the source
texts (Kryscinski et al., 2019; Zhao et al., 2020;
Kryscinski et al., 2020). While our model incor-
porated the argument structure of the source arti-
cle, the generation results may still carry certain
levels of non-factual information and need to be
utilized with extra care. Similarly, as mentioned
in the prior line of works using CanLII (Elaraby
and Litman, 2022; Zhong and Litman, 2022), Can-
LII has taken measures to limit the disclosure of
defendants’ identities (such as blocking search in-
dexing). Abstractive approaches may cause user
information leakage. Thus using the dataset needs
to be cautious to avoid impacting those efforts.
","misinformation, privacy",,"misinformation, privacy",2023,"concerns, actions, advice"
854,"The Association of Internet Researchers (AoIR)
acknowledges internet research is complex, dy-
namic and often involves many gray areas– specif-
ically related to what constitutes human subjects,
private versus public spaces and data versus per-
sons (Markham and Buchanan, 2012). For this rea-
son, the AoIR guidance recommends an inductive,
ongoing and context-specific approach to ethics
throughout the research process. At all stages, this
involves being mindful of the vulnerability of the
community under study and taking efforts to pro-
tect them where appropriate, while balancing their
rights with social benefits and the researcher’s right
to conduct research.
Following this guidance, we subscribe to a util-
itarian philosophy where we focus on doing the
greatest good for the greatest number of people.
In the case of black-pilled incels, we believe the
necessity to better understand this potentially dan-
gerous group outweighs the possible damage to
forum members. For this reason, in addition to the
AoIR guidance outlined above, we have followed
some commonly accepted standards to protect par-
ticipants and refrain from amplifying misogynist
voices.
Data was collected only from publicly available
online message boards and no private or identifiable
information has been included in this manuscript.9
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
855,"There are significant ethical issues to consider in
developing text classifiers for ideologies. Since this
research has clear social implications, we wish to
be explicit about values and author positionality be-
yond a sense of “objectivity” in selecting research
questions (Schlesinger et al., 2017; D’Ignazio and
Klein, 2020; Waseem et al., 2021). The authors
come from European- and American-dominated
university contexts and consider working against
racism and white supremacy a priority. Most iden-
tify as white and some identify as people of color.
This research proceeded with values of racial jus-
tice and places those values at the center of as-
sessing knowledge claims (Collins, 1990; Daniels,
2009). Our choice of focusing on white supremacy
among other ideologies stems from those values.
White supremacist extremism, as well as structural
white supremacism, is responsible for substantial
harms against those with marginalized identities.
This research responds to a need from practitioners
for more nuanced classifiers than for broad cate-
gories of hate speech or abusive language. We thus
choose to pursue this research, though caution that
developing classifiers for other ideologies should
be done with careful consideration and a clear state-
ment of motivating values.
There are significant risks which we consider,
and attempt to mitigate, in such a dataset and clas-
sifier. First, there is the risk of misuse of a large
corpus of white supremacist data, as has been seen
in building and releasing a hate speech “troll bot”
from 4chan data7. For this reason we build a dis-
7https://www.vice.com/en/article/7k8zwx/ai-tcriminative, not generative, classifier, and only plan
on releasing our dataset through a vetting process
instead of publicly.
There are also privacy risks in how such a clas-
sifier could be used. Our classifier only identifies
language that is likely similar to white supremacist
content. The intended use of this classifier is to
measure the prevalence of such an ideology on par-
ticular platforms or within networks for research
purposes, not to label individuals as holding or not
holding white supremacist ideologies. Using the
classifier for this purpose poses significant risks
of misclassification and could increase harmful
surveillance tactics. We strongly discourage such
a use. Our hope is that our proposed classifier and
dataset can increase knowledge about the nature
and extent of white supremacist extremist move-
ment online and can inform structural interventions,
such as platform policies, not interventions against
individuals.
Hate speech classifiers, developed by researchers
with similar equity-based values, have been found
to contain biases against marginalized groups (Sap
et al., 2019; Davidson et al., 2019). We measure
and mitigate this bias from the start by incorporat-
ing anti-racist data, though caution that this risk
still exists.
","misuse, bias, privacy",,"misuse, bias, privacy",2023,"concerns, actions, advice"
856,"We evaluate the proposed method on established
and publicly available datasets. There is also no
human evaluation involved. This paper is not con-
cerned with the above ethical risks. When the pro-
posed framework is deployed into domain speciﬁc
production, the domain adapted language models
might express ethical-related outputs, but just as
any other language models do (Weidinger et al.,
2021), and should be treated with according tech-
niques to eliminate ethical risks such as bias, stereo-
types.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
857,"We do not find serious risks or ethical concerns with
this work. We do note this work advances a spe-
cific position, which we clearly identify. It should
not be assumed there is consensus in the commu-
nity (or beyond) on any account for evaluation, let
alone the account on power that we espouse. In
this regard, we actively solicit response and inter-
rogation of the positions presented in this work,
especially given myriad relevant analyses of eval-
uation/measurement/benchmarking exist in other
parts of AI, computer science, linguistics, and other
disciplines.
",no ethical concerns,,no ethical concerns,2023,statement
858,"We are sure that PDP has been collected in a man-
ner that is consistent with the terms of use of any
sources and the intellectual property and privacy
rights of the original authors of the texts. Mean-
while, our project is approved by an IRB. Finally,
we also provide details on the characteristics of
PDP and steps taken to ensure the potential prob-
lems with the quality of the dataset do not create
additional risks.
8
",no ethical concerns,,no ethical concerns,2023,statement
859,"In this work, we have identified the potential vul-
nerability of code pre-trained models to backdoor
attacks, which could target a wide range of code-
related downstream tasks. Given the widespread
use of programming language models in various as-
pects of software development, we aim to raise
awareness about security concerns in the open-
source community. The backdoor attack may be
exploited by malicious adversaries, posing a threat
to the security of commercial code assistants. Forexample, attackers may implant backdoors in pro-
gramming assistance models (e.g., Copilot), lead-
ing to code with vulnerabilities. Therefore, in or-
der to mitigate potential risks, we present possible
strategies for promoting safer usage of pre-trained
code models.
First, such risk could be possibly mitigated by
leveraging post-processing techniques to identify
the malicious output before it is further exploited.
Detailed discussion about these techniques can be
found in Appendix E. We suggest developers down-
load pre-trained code models from a trustworthy
platform and perform thorough post-processing be-
fore directly adopting the model’s output. This
can not only improve the code quality but also
minimize the risks of backdoor attacks. Second,
we suggest the open-source platform adopt strict
regulations, strengthen public authentication mech-
anisms, and provide model weights along with dig-
ital signatures for models, as outlined by Zhang
et al.. Once the malicious model has been found, it
should be discarded by the platform and the victim
users should be informed immediately. This is cru-
cial for preventing the distribution of backdoored
models and improving community awareness.
While the techniques discussed above may help
mitigate current backdoor attacks, it’s important
to note that there is currently no perfect defense
against code backdoor attacks. Our work aims to
demonstrate the risks posed by such attacks and
raise awareness in the community. To prevent back-
doors from being further designed and exploited
and causing damage, we hope that our work will
draw attention to this issue and inspire future re-
searchers to design more effective defense tech-
niques based on our work.
",security,,security,2023,"concerns, advice, suggestions"
860,"Our work focuses on the task of sign language
translation. Such systems aims to use technology
to facilitate the day-to-day life of the deaf and hard-
of-hearing community. Though we improve on the
baseline, the proposed model still does not equip
with the ability to serve as an interpreter in real-
life scenarios. We use extracted keypoints as the
input of the model, there are little to no concerns
about personal privacy. For now, the model is only
validated on American sign language datasets, cur-
rently it’s not able to help people that do not use
ASL.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
861,"This paper is dedicated to deep sentence embed-
ding learning and proposes a new unsupervised
sentence representation model, it does not involve
any ethical problems. In addition, we are willing
to open source our code and data to promote the
better development of this research direction
",no ethical concerns,,no ethical concerns,2023,statement
862,"Regarding ethical concerns, we would like to note
that our contributions are mainly about methodolo-
gies. The datasets and evaluation metrics in our
work are also widely used in prior works. One ethi-
cal concern is the binarization of the genders and
races, which is an over-simplification and is not
proper to practical situations. Binarization is the
common problem among most debiasing methods
and we totally agree and support the development
of more inclusive methodological tools, datasets,
and evaluation methods.
Under our framework, we consider gender or
race isolatedly and neglect the particular intersec-
tional biases. It is apparent that the pretrained lan-
guage model cannot be applied or operated in an
ideal environment, and should be able to handle
complex combinations of biases simultaneously.
Another ethical consideration is that Causal-
Debias is entirely based on a English system. Such
an assumption may not be a problem now but
sooner or later it will be. The debias studies have
to be situated on the high-resource languages while
considering not only high-resource language sys-
tems but how to debias on the low-resource lan-
guages.
For instance, some languages such as Span-
ish, German, Chinese, or Japanese contain vari-
ous words to describe masculine or feminine forms.
The detection and removal of biases are greatly
complicated by the need to consider both linguistic
and social gender.
For the reasons above, practitioners should be
very cautious when applying our framework to
real-world use cases. In its current state, Causal-
Debias should be seen not as a panacea for ad-
dressing biases in NLP, but rather as another initial
effort to illuminate and undercut a critical, elusive,
multifaceted problem.
",bias,,bias,2023,concerns
863,"We would like to thank Malaviya et al. (2020) and
Wang et al. (2021) for their code on commonsense
knowledge graph completion. Their models are
licensed under MIT, which allows copying, modi-
fying, merging, publishing, and distributing of the
material.
In the stage of human evaluation, we employed
three graduate students experienced in natural lan-
guage processing for human evaluation. We paid
the graduate students about $8 per hour, well above
the local average wage, and engaged in constructive
discussions if they had concerns about the process.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
864,"We perform experiments on three datasets formerly
established by (Liu et al., 2021), namely Camera-
COQE, Car-COQE, and Ele-COQE. These datasets
do not include personal information or contain any
objectionable content that could potentially harm
individuals or communities. It’s important to note
that certain product reviews may include subjec-
tive comparisons between products given by anony-
mous customers, which do not necessarily reflect
the preferences of this study.
",no ethical concerns,,no ethical concerns,2023,statement
865,"Our methodology encompasses the creation of a
Chinese emotion cause dataset using a rule-based
approach, accompanied by the development of a
human-annotated dataset. Importantly, all the posts
we gathered are openly accessible, ensuring trans-
parency and accessibility for fellow researchers. To
ensure the privacy and confidentiality of our anno-
tators and experts, we have implemented rigorous
anonymization measures, retaining only their ID
for research purposes. Moreover, we have ensured
fair compensation for the efforts invested by our
annotators, recognizing and valuing their contribu-
tions to the project.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
866,"We would like to thank Poria et al. (2019) and Kol-
lias and Zafeiriou (2019) for their valuable workin constructing and sharing the MELD and Aff-
Wild2 datasets. MELD is licensed under the GNU
General Public License v3.02. For the Aff-Wild2,
we have signed an End User License Agreement3.
Since MELD is built on the sitcom Friends, we
manually annotate 20 different face images occur-
ring in the sitcom for each of the six leading roles
without accessing to any personal information. We
do not share personal information and do not re-
lease sensitive content that can be harmful to any
individual or community.
If applying our framework to real-world sce-
narios in the future, it could potentially involve
some ethical issues such as user privacy and ethical
biases, as pointed out by Stark and Hoey (2021)
and Stark and Hutson (2021). While the ethical
issues faced in emotion recognition are common,
we will engage with the concerns raised about emo-
tion recognition in the references and strictly com-
ply with relevant regulations and ethical standards.
Specifically, our work is based on publicly avail-
able datasets, and if we construct new MERMC
datasets in the future, we will carefully consider
user privacy issues, anonymize or obfuscate facial
data, and ensure that the framework is only used
in contexts where explicit consent for facial data
processing has been obtained. Moreover, we will
refer to the recommendations in Stark and Hoey
(2021) and develop a comprehensive ethical frame-
work that guides our research process. We are also
committed to being transparent about our research
methods, data sources, and potential limitations.
Regarding potential biases, we plan to evaluate our
framework on more diverse datasets in the future,
and propose appropriate solutions to alleviate the
bias issues.
","privacy, bias",,"privacy, bias",2023,"concerns, advice, suggestions"
867,"We would like to thank the Allen Institute for AI
for their valuable work on ATOMIC . The ATOMICis licensed under a license of CC BY , which al-
lows remixing, transforming, and building upon the
material for any purpose. We will also make our
Dense- ATOMIC publicly available later. Mehrabi
et al. (2021) have found representational harms in
common sense resources. We acknowledge that the
generated commonsense from our models might
contain biases. All of the datasets and models are
in English, which benefits English speakers more.
We have employed 3 postgraduates experienced
in natural language processing for annotation and
human evaluation. We pay postgraduates around
$8 per hour, well above the local average wage,
and engage in constructive discussions if they have
concerns about the process.
",bias,,"bias, fairness",2023,"concerns, actions"
868,"Our dataset is constructed based on two public
MNER datasets, i.e., Twitter-15 (Zhang et al., 2018)
andTwitter-17 (Yu et al., 2020). Three graduate stu-
dents are employed as our annotators. The average
time to annotate every 1,000 samples for each an-
notator is around 17 hours. Since the two datasets
publicly released the text, images, and named enti-
ties, each annotator is asked to independently an-
notate the bounding box groundings for each entity
without accessing to the user account. To ensure
that the annotators were fairly compensated, we
paid them at an hourly rate of CNY 36 (i.e., USD
5.2 per hour), which is higher than the current av-
erage wage in Jiangsu Province, China. We do not
share personal information and do not release sen-
sitive content that can be harmful to any individual
or community. Because it is easy to retrieve multi-
modal tweets via image IDs from the two MNER
datasets, we will release our annotation based on
the textual modality and unique image IDs.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
869,"We conduct experiments on four publicly available
datasets, i.e., Laptop (L), Restaurant (R), Device
(D), and Service (S). These datasets do not share
personal information and do not contain sensitive
content that can be harmful to any individual or
community. Due to the lack of ethics and bias con-
straint in the data generation process, the generated
data from our trained Domain-Adaptive Language
Model may contain sensitive and misleading con-
tent. Therefore, it is necessary to manually check
these generated data when applying them to real-
world applications.
","sensitive content, misleading content",,"sensitive content, misleading content",2023,"concerns, advice"
870,"The dataset we use is from Reddit, a forum in-
tended for anonymous posting, users’ IDs are
anonymized. In addition, all sample posts shown
throughout this work are anonymized, obfuscated,
and paraphrased for user privacy and to prevent
misuse. Thus, this study does not require ethical
approval. Due to the subjective nature of annota-
tion, we expect some biases in our gold-labeled
data and the distribution of labels in our dataset.
Examples from a wide range of users and groups
are collected, as well as clearly defined instruc-
tions, in order to address these concerns. Due to
high inter-annotator agreement ( κscore), we are
confident that the annotation instructions are cor-
rectly assigned in most of the data points. It is
reproducible with the dataset and the source code
to reproduce the baseline results which is available
on Github.
To address concerns around potential harms, we
believe that the tool should be used by professionals
who are trained to handle and interpret the results.
We recognize the huge impact of false negatives in
practical use of applications such as mental health
triaging, and we shall continue working towards
improving its accuracy and reducing the likelihood
of false negatives. We further acknowledge that our
work is empirical in nature and we do not claim toprovide any solution for clinical diagnosis at this
stage.
","subjectivity bias, inaccuracies",,"subjectivity bias, inaccuracies",2023,"concerns, advice, suggestions"
871,"We presented the task of simulating NL feedback
for interactive semantic parsing. The dataset we
used in this project is publicly available. While it is
possible that our feedback simulator may generate
texts that do not perfectly align with the intended
error correction, it is important to note that these
generated texts are exclusively used for training
the error correction model and are not exposed to
real human users. Hence, we do not anticipate
any ethical issues resulting from our work. On the
other hand, we emphasize the positive impact of
our work when it aims to facilitate feedback-driven
human-AI interaction. As shown in this and prior
work, human feedback allows for correcting model
mistakes before their negative impact takes place,
which can play a key role toward enabling safe and
trustworthy AI/NLP applications.
",no ethical concerns,,no ethical concerns,2023,statement
872,"For the present work, we used an existing
anonymised dataset from BioNLP 2023 Shared
Task 1A without any data protection issues. In addi-
tion, data augmentation only uses an open-source,
off-line model which is not offensive to the data
user agreement that is shared with a third party.
",no ethical concerns,,no ethical concerns,2023,statement
873,"For the present work, we used an existing
anonymised dataset without any data protection
issues. In addition, all annotators were systemati-
cally trained and explicitly informed that their work
would be used in the study before human evalua-
tion. The annotators’ work was only taken into
account if they clearly understood the task and con-
sented to how their work will be used. In addition,
we do not collect their names or personal infor-
mation, only their ratings. Therefore, institutional
ethical approval was not required.
",no ethical concerns,,no ethical concerns,2023,statement
874,"This work is related to and partially inspired by the
real-world task of legal text classification. As legal
matters can affect the life of real people, and we
are yet to fully understand the behaviors of deep-
learning-based models, relying more on human ex-
pert opinions is still a more prudent choice. While
the proposed approach can be utilized for automat-
ing the process of legal text, care must be taken
before using or referring to the result produced by
any machine in legal domain.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
875,"Our BADGE framework is designated to im-
prove the training of multi-exit BERT and dynamic
early exiting performances. Our work can facili-
tate the deployment and applications of pre-trained
models on devices with less powerful computation
capabilities, making the state-of-the-art models ac-
cessible for everyone. In addition, we hope this
technology can help reduce the carbon footprints of
NLP-based applications. Furthermore, the GLUE
datasets we experiment with are widely used in
previous work. Thus, to our knowledge, our work
does not introduce new ethical concerns.
",no ethical concerns,,no ethical concerns,2023,statement
876,"This paper finds universal adversarial translations
that can be used to attack metrics and lead to se-
curity risks. However, this paper also proposes
methods to improve metric robustness to avoid this
situation.
B Training Duration for MLE and MRT
We list the training duration for MLE and MRT in
Table 8 and Table 9, respectively. Table 8 shows
the number of training epochs, while Table 9 shows
the number of training steps. It can be seen that the
training duration for MRT is much shorter than that
for MLE. The improvement of translation perfor-
mance of the model mainly lies in the MLE stage,
while MRT fine-tuning makes the model inclined
towards specific metrics.
C MRT Training Process Figures
From Figure 5 to Figure 9, we can see how all
metrics change when the translation model is opti-
mized with each metric on different language pairs.
From the trends of different metrics, we can ob-
serve the differences between the metrics and the
impact of the metrics used for optimization on the
translation model.
In each figure, the horizontal axis represents the
training steps, and the vertical axis is the score
of each metric (except for BARTScore on the
right axis, which is a negative number because
it calculates the logarithmic probability of transla-
tions); metrics other than BARTScore and BLEU
are mostly distributed between 0 and 1, and we
multiply them uniformly by 100 for ease of obser-
vation. The asterisk represents the highest value
achieved by the optimized metric.
D MRT Training Process Statistics
Table 10 to Table 14 display the change range in
all metrics when optimizing the translation model
with a specific metric to the highest point across
different language pairs. The results correspond
to figures in Appendix C. 0.00% means that the
optimized metric does not continue to improve, and
the highest value remains the same as the result of
MLE training; a negative number means that the
metric score goes from positive to negative, which
means it decreases a lot.Examples of Hotel Review Sentences from
WMT14 En ⇔De
The location of the hotel was excellent. The
room was clean and comfortable.
The room was clean and comfortable, the ho-
tel was situated close to the center but in the
tourist center. The food was excellent and the
service second to none.
The location of the hotel is great, the atmo-
sphere is quite pleasant, the staff is efficient
and friendly, the room was clean and com-
fortable, the price was fair. In short words,
everything was perfect.
The room was clean and comfortable.
the location of the hotel is ideal for sightsee-
ing,the room was clean and comfortable, the
staff were helpful.
The room was clean and comfortable. Staff
friendly.
the employees were very helpful at all times
the room was clean and comfortable and the
restaurant was very nice.
The room was clean and comfortable and the
staff friendly and courteous.
This is a great hotel .The room was clean and
comfortable .With small budget but we have
a comfortable stay .Good value, we will rec-
commend this hotel for anyone looking for a
hotel in Hanoi .
Table 7: Examples of Hotel Review Sentences from
WMT14 En ⇔De.
E High Frequency Samples
Table 7 displays some hotel review examples in
the WMT14 En ⇔De dataset, and the semantics are
very similar to universal translations of BLEURT
on En⇒De. For ease of understanding, English is
shown here.
En⇒De De ⇒En En ⇒Zh Zh ⇒En En ⇒Fi Fi ⇒En
MLE 33 28 32 40 55 36
MRTBLEU 1 1 1 1 1 1
BERTScore 1 1 1 1 1 1
BARTScore 1 1 1 1 1 1
BLEURT 4 1 1 1 1 1
COMET 1 1 1 1 1 1
UniTE_ref 1 1 1 1 1 1
UniTE_src_ref 1 1 1 1 1 1
Table 8: Comparison of the number of epochs trained by MLE and MRT. The number of epochs for MLE is the
epoch number trained until early stop, while the number of epochs displayed in MRT is the epoch number when the
model is optimized to the highest metric score.
En⇒De De ⇒En En ⇒Zh Zh ⇒En En ⇒Fi Fi ⇒En
Steps in one epoch 2403 2127 7927 7929 10906 10910
MLE 163000 61000 51000 64000 126000 40000
MRTBLEU 0 50 0 200 100 50
BERTScore 100 50 250 200 250 300
BARTScore 1950 1900 1800 1450 1050 550
BLEURT 5750 100 3500 550 1400 250
COMET 550 450 500 550 800 300
UniTE_ref 400 650 750 750 600 600
UniTE_src_ref 600 350 700 500 800 550
Table 9: Comparison of training steps between MLE and MRT. The number of steps for MLE is the number of
steps trained until early stop, while the number of steps displayed in MRT is the number of steps when the model is
optimized to the highest metric score.
Figure 5: The training process of MRT optimized by each metric on En ⇒De.
Figure 6: The training process of MRT optimized by each metric on De ⇒En.5439",no ethical concerns,,no ethical concerns,2023,statement
877,"WACO has the potential to beneﬁt speakers of low-
resource languages. For example, their published
video or speech can be better translated into other
languages, so more viewers in the world can un-
derstand them, enabling deeper communication be-
tween different cultures. Though WACO may be
beneﬁcial to cross-language communication, we
do not encourage users to treat the translation gen-
erated by the E2E ST model as fully correct since
they are far from perfect in practice.
",inaccuracies,,model inaccuracies,2023,"statement, advice"
878,"We propose a novel view to interpret continu-
ous prompts, which have been considered ""black
boxes"", as combinations of human-understandable
discrete tokens. Since the method itself is unbiased
and faithful, and all experiments are conducted
on publicly available datasets, we believe that our
work does not create any potential ethical risk.
Further, we discover shortcuts latent in continu-
ous prompts, implying that systematic biases or dis-
crimination may also exist in continuous prompts.
These biases may originate from training datasets
which are exploited by continuous prompts as a
shortcut to the acquisition of true labels, or even
originate from artificially implanted backdoors. We
hope this work will provide the possibility to detect
these potential biases in continuous prompts.
Our created artifacts are intended to provide re-
searchers or users with a tool for understanding
decision-making and detecting possible unexpected
shortcuts of continuous prompts, while at the same
time offering the feasibility of cross-model transfer
without extra training signals on target PLMs. They
are compatible with the original access conditions.
All use of existing artifacts is consistent with their
intended use in this paper.
",bias,,bias,2023,statement
879,"Our dataset is constructed based on Wiki20 and
Wikidata. The two sources are both publicly avail-
able. Wiki20 is under the MIT Licence and the
Wikidata is under the Creative Commons CC0 Li-
cense. Both of them allow modification and distri-
bution. Regarding human revision during dataset
construction, the annotators were properly paid.
The annotating procedure lasted 12 days and the
daily workload was relatively light: around 2.5
hours per day. During human inspection, we did
not identify any unethical instances in our dataset.
Regarding baseline models, we use PLMs as our
text encoder and our task is inherently unsuper-
vised. As PLMs are learned on large corpora, our
method can potentially create biased clustering re-
sults. How to de-bias PLM embedding is worth
further investigation.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
880,"This work shows that with proper plain textual
prompts, instance-level desired results can be
prompted from PLMs. This inherent feature of
PLMs means attacks can be launched to produce
rude or discriminated words. On the other hand,
however, we believe it can also be a technique used
for debiasing a PLM. Overall, this effect depends
on the intention of the users and the pre-training
corpus of the corresponding PLMs. The analysis of
this study can be used to facilitate the community to
develop more specifications for the rational use of
PLMs (especially the super-large ones), and more
approaches to effectively prevent potential ethical
issues. For example, we can use this technique to
analyze which outputs that may have ethical issues
are easily triggered by which contexts (prompts)
and develop a set of intervention methods to make
these tokens unavailable for output.
",no ethical concerns,,no ethical concerns,2023,statement
881,"We propose our ethics statement of the work in this
section: (1) Dataset. Our data is obtained from DB-
pedia and Wikidata, two publicly available linked
open data projects related to Wikipedia. Wikidata
is under the Creative Commons CC0 License, and
DBpedia is licensed under the terms of the Cre-
ative Commons Attribution-ShareAlike 3.0 license
and the GNU Free Documentation License. We
believe the privacy policies of DBpedia3and Wiki-
data4are well carried out. We inspect whether our
dataset, especially instances collected, contains any
unethical content. No private information or of-
fensive topics are found during human inspection.
(2) Labor considerations. During dataset construc-
tion, the authors voluntarily undertake works re-
quiring human efforts, including data collection,
cleansing, revision and design of property patterns.
All the participants are well informed about how
the dataset will be processed, used and released.
(3) Probing results. As PLMs are pretrained on
large corpora, they may give biased results when
being probed. We randomly check some probing
results and find no unethical content in these sam-
ples. Therefore, we believe that our study does not
introduce additional risks.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
882,"As large language models are getting more and
more popular in NLP research and application,
DecT provides a cost-efﬁcient way to adapt these
large models. However, we need also to be cau-tious about the improper adaptation of large lan-
guage models, such as generating toxic and biased
speeches.
",no ethical concerns,,no ethical concerns,2023,statement
883,"Our MTL architectures are trained on multiple pub-
licly available datasets referenced in this paper.
These datasets have been previously collected and
annotated, and no new data collection has been car-
ried out as part of this work. Furthermore, these are
standard benchmarks that have been released in re-
cent WMT shared tasks. No user information was
present in the datasets protecting users’ privacy and
identity. We understand that every dataset is sub-
ject to intrinsic bias and that computational models
will inevitably learn biased information from any
dataset. That said, we also believe that our MTL
models will help diminish biases in QE as they
provide an explainable aspect to the predictions
through token-level labels.
",bias,,bias,2023,statement
884,"For this research, we used multiple datasets refer-
enced in this paper which were previously collected
and annotated. No new data collection has been car-
ried out as part of this work. We have not collected
or processed writers’/users’ information nor have
we carried out any form of user profiling protecting
users’ privacy and anonymity.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
885,"We propose a model trained using DeBERTaV3
style pre-training along with an optimized training
implementation, which reduces training computa-
tion cost when compared to previous models, and
hence greatly reduces the energy cost and environ-
mental impact of language model training. We
trained our model using the CCNet dataset, for
which we direct the reader to for further discussion
on bias and ethical considerations. Our experi-
ments do not include any additional data collection
or human annotators. Like other language models
trained on massive corpora, there may be potential
biases present in the training data, which could af-
fect the output of our models. Therefore, we advise
against using these models in production without
thorough testing. All our experiments were carried
out on clusters with energy sources consisting of
nuclear (65–75%), 20% renewable, and the remain-
ing from gas.",no ethical concerns,,no ethical concerns,2023,"statement, advice"
886,"All our evaluation datasets are collected from pub-
licly available sources. Following privacy protec-
tion policy, all the data we used for model pre-
training and fine-tuning are anonymized. Some
annotations in the downstream data (e.g., for hate
speech tasks) can carry annotator bias. We will
accompany our data and model release with model
cards. We will also provide more detailed ethical
considerations on a dedicated GitHub repository.
All our models will be distributed for research with
a clear purpose justification.
",bias,,bias,2023,statement
887,"Although our proposed algorithm shows more sta-
bility and reduced bias compared to existing ap-
proaches and random sampling, it’s important to
observe the behavior of active learner as the algo-
rithm may not completely eliminate bias, specifi-
cally when the annotation budget is small. This can
be achieved by observing label and error variance
on the evaluation data. It is also important to take
into consideration the necessities of practical sce-
narios. In scenarios where certain type of bias is
desired (e.g., higher precision), the algorithm needs
to be adapted as outlined in Section 3.
",bias,,bias,2023,concerns
888,"We used the NLM Scrubber offered by NIH to
produce HIPAA-compliant deidentified health in-
formation for scientific use, including dates, and
places. Two independent annotators evaluated the
NLM-Scrubber on the dataset to make sure no
events or other people in patients’ posts can allow
patients to be traceable. The de-identified version
of our data will be shared with researchers upon
request who have completed an ethical review from
their institution and a data request application form
from us.
Since the domain of our dataset is specific, the
models trained on our dataset may exhibit subtle
biases on out-of-domain data. Further, pre-trained
models that we use in our work have been shown
to exhibit biases (Li et al., 2021). We hope future
researchers could use these models with caution
regarding the biases that these pre-trained models
have.
The long-term goal of our work is to aidhealth-
care providers to quickly identify poorly engaged
patients to allocate their energy and resources to
provide in-time support. Models trained on our data
should not be deployed in the real world without
human supervision because, despite the potential
of transformer models, they cannot be relied on
completely in sensitive medical scenarios.
",bias,,bias,2023,"statement, concerns, advice, suggestions"
889,"We believe that using a set of countries to represent
cultures is just a proxy for acquiring a more diverse
set of facts that are less skewed toward a speciﬁc
culture. More speciﬁcally, using the terms Arab
cultures, Western cultures, and Asian cultures sim-
pliﬁes the differences between the cultures within
the countries that we have used to represent these
macro-cultures. On the other hand, we still think
that the differences between Asian cultures are less
subtle than between them and Western cultures.
We also acknowledge that the accuracy and valid-
ity of some relation triples queried from Wikidata
might be biased by the views of the people who
added such information to Wikidata. This might
be particularly vibrant for relation triples related to
zones with political/ sectarian wars and conﬂicts.
",bias,,bias,2023,"statement, concerns"
890,"We hired three human raters to annotate WMT21
De→En testing set. The evaluated text is publicly
available machine translation testing set which con-
tains no sensitive or explicit languages. There is
no risk of exposing annotators’ identities and they
are fully aware the usage of their collected dataset.
We use the standard MQM human evaluation pro-
cedures (Freitag et al., 2021a) and all annotaters
are experienced with this evaluation protocol. All
collected human annotations will be released at the
camera ready. The hourly salary for the raters are
well above the minimum wage in the local region.
Details can be found in Appendix F.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
891,"While Twitter allows users to self-manage their pri-
vacy by disabling the geo-tagging functionalities,
“even well informed and rational individuals cannot
appropriately self-manage their privacy” (Solove,
2012) due to lack of awareness on how such data
can be collected or commercially used. However,
geolocation extraction could be justified in the con-
text of social good during natural disaster events
where protecting geographical privacy could be of
least importance as affected people need to rescue
their lives or get basic necessities of life (Crawford
and Finn, 2015). The privacy of affected people
during human-made disaster events is more critical
because revealing their locations during conflicts
and wars could risk their lives. Therefore, we limit
our dataset to natural disaster events and COVID-
19 pandemic and apply a couple of de-identification
steps to protect users’ privacy (refer to Section A).
We further limit the dataset usage to research pur-
poses only by releasing it under the Creative Com-
mons Attribution 4.0 International License.8Addi-
tionally, we emphasize that systems developed for
LMR using IDRISI-RA dataset should implement
proper mechanism for preserving user privacy.
11 Conclusion
We introduced IDRISI-RA, the first Arabic LMR
Twitter dataset. It contains 22 disaster events of dif-
ferent types that happened in the Arab region. We
manually- (gold) and automatically annotated (sil-
ver) about 4.6K and 1.2M tweets. Both versions are
annotated for location mentions and location types
which form the value and uniqueness of IDRISI-
RA. Our analysis showed that IDRISI-RA is second
to none in empowering research for Arabic LMR.
Additionally, the extensive experiments emphasize
the need for developing LMR-specific models for
the disaster domain. The developed LMR base-
lines are simple yet competitive ones. The results
also demonstrated the decent generalizability of
IDRISI-RA. For future work, we plan to extend
the annotations for the Location Mention Disam-
biguation (LMD) task. We further plan to explore
different transfer learning, domain adaptation, and
active learning techniques to tackle the LMR task.
8https://creativecommons.org/licenses/by/4.0/
legalcode
",privacy,,privacy,2023,"concerns, actions"
892,"Due to the nature of our data, there were several
ethical issues to consider. First, we anonymized
all the usernames mentioned in tweets by replacing
them with @anonymized_account token. Despite
the fact that the data is publicly available, we de-
cided to prevent the model from learning sentiment
about specific users based on what the community
writes about them. We did not want the model to
produce harmful output tokens for specific users.
Secondly, there is a great deal of harmful content
in social media, which we could possibly try to
remove from the training corpus as part of data pre-
processing to prevent the model from learning this
kind of language. However, if we are to use such
models to detect hate speech or cyberbullying, they
need to know it. We believe that exposing a model
to harmful content only during the fine-tuning stage
may not be enough.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
893,"All corpora were collected under protocols ap-
proved by an institutional review board, includingthat the data is not publicly available, except the
college data. While the breach of private student
information from the elementary and high school
data will thus not pose any ethical concern, other
researchers can not replicate our results for those
data. However, since the college data with its pur-
pose annotations was already made available by
the original researchers, our new desirability an-
notations can be released upon acceptance of this
study. The claims of the paper match the experi-
mental results and the results can be hypothesized
to generalize. In the future, the proposed models
may be incorporated into AWE systems for student
writers. While identifying and providing feedback
on revision desirability will be helpful to students
in improving their writing, there is the risk that
the system might sometimes provide poor advice
based on incorrect model classifications. Since the
dataset is still fairly small after data augmentation,
it is possible that the model may learn biased rep-
resentation of the revisions (e.g., always predict
longer revisions with more information as desir-
able).
","bias, inaccuracies",,"bias, inaccuracies",2023,"statement, concerns, advice"
894,"We want to discuss three ethical considerations
of our system. First, our system is based on pre-
trained Transformers, which inherit biases from
their training data. For the shared task, these biases
are negligible, but are a concern for real-world ap-
plications. The second consideration relates to fair-
ness concerns. The performance varies strongly be-
tween languages, with more researched languages
typically resulting in better performance. We, thus,
embrace the multilingual setting of the shared task
with one-third zero-shot languages, but similarly
achieved better performance in Latin-based lan-
guages. Third, our system leads to better detection
of media frames, which is an important research di-
rection. However, the system could in theory also
be used in a disputed or even malicious manner,
e.g., for reframing political statements. Hence, we
do not advise specific applications of our system
besides better framing detection.949
","bias, fairness, misuse",,"bias, fairness, misuse",2023,"concerns, advice"
895,"We could not identify potential harm from using
the provided models in this work. However, one
concern is that code-switched ST is yet a challeng-
ing task, and the ST models trained in this work
provide low performance, and thus should not be
deployed as it can mislead the users.",misleading performance,,misleading performance,2023,"statement, concerns, advice"
896,"To ensure the ethics of this work, we followed well-
recognized ethical codes: The Australian Institute
of Aboriginal and Torres Strait Islander Studies (
AIATSIS)16and the DOBES code of conduct17. As
a result, all participants were well informed about
the intent of this work, our aims, and the complete
anonymization of their answers. Moreover, this
work was done with indigenous leadership (as sug-
gested by AIATSIS).
Here we list the ethical issues we found while
working on this work and how we try to minimize
their impact. First, we were concerned with the
data protection of the participants in this study. As
for this study, no personal data is required. There-
fore, we decided to remove any questions contain-
ing any information that could reveal the identity of
the participants. Second, as our study aims to get
substantial input from the communities, we decided
to leave as many open questions as possible and
consider the available comments section of each
question. All participants were informed about the
goals of this project and participated in a free and
informed way. To give proper recognition to the
participants of this study, we offer an option to be
included in the acknowledgment section.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
897,"The motivation of this work is to compare cur-
rent NLP methods in a low-resource setting and
discuss how the different systems might apply in
different contexts based on the results, contribut-
ing overall to the discussion on how NLP methods
can be used to beneﬁt language communities and
support the creation of more linguistic resources.
While the hope is to support the language commu-
nity, the integration of computational methods also
poses the risk of language commodiﬁcation and a
dispossession of intellectual property of a commu-
nity. This study is submitted with the belief that
the current beneﬁts associated with the application
of this research outweigh this risk.
",misuse,,misuse,2023,statement
898,"This article utilizes publicly available resources.
The authors have taken measures to ensure that the
data used is properly cited and attributed to the
original sources and that any potential biases or
limitations in the data are acknowledged.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
899,"This research has drawn on isiXhosa historical text
which is in the public domain and has been digi-
tized and made publicly available.
Human coders and language editors who worked
on the digitized text analyzed here are all employed
as members of the larger archival research project.
All coders and editors are first language isiXhosa
speakers who are students or researchers.
The method outlined in this paper may be of
particular use to under resourced languages. The
approach does not use cutting edge tools, but rather
outlines how accessible methodological approaches
can be used to by-pass more complicated NLP steps
which may not be available for under resourced lan-
guages. For this reason, the author believes thatthis research has limited potential to cause harm.
Instead it may have potential to enhance computa-
tional research in under resourced languages, fos-
tering more equal access to computational text re-
search methods
",no ethical concerns,,no ethical concerns,2023,"statement, concerns"
900,"We provide access to digitized versions of isiXhosa
texts which are all in the public domain. We do so
for free with the intent to make materials available
for both researchers and the general public. It is
thus our aim to expand access to historically diffi-
cult to access materials, in a language which has
faced a lack of research resources. Our hope is that
this gives more access to historically marginalized
researchers and publics. This project is not sub-
ject to ethics review as there are no living subjects
discussed in these materials. The authors have ac-
quired permission from quoted participants to use
these quotes and conversations.
Human coders and language editors who have
worked on this archive are all employed as mem-
bers of the project. All coders and editors are first
language isiXhosa speakers and are either students
or researchers.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
901,"All data used in this study was obtained from pub-
licly available sources. No confidential or sensitive
information was used. The study was cleared for
ethics by North-West University with the ethics
clearance number: NWU-00729-21-A7.
",no ethical concerns,,no ethical concerns,2023,statement
902,"The datasets created and used for the translation
model benchmarks are taken solely from South
African government resources. Therefore it is high-
lighted that if these models are used in production,
they might ignore certain social/societal structures
and will be representative of the dominant political
party at the time the datasets were sourced (Ben-
der et al., 2021). We also note that the benchmark
models and datasets have not been curated to de-
termine any biases that are present. As such, any
existing biases in the system might have the po-
tential to harm specific groups, when used in NLP
downstream production tasks (Bender et al., 2021).
9
",bias,,bias,2023,concerns
903,"We do not foresee a direct use of our work in an un-
ethical setting. However, as with all research using
or relying on LMs, our work is also prone to the
same unwanted biases that these models already
contain (e.g. social biases). Therefore, when con-
trolling contextual attributes, researchers should be
aware of the biases in their data in order to under-
stand the models’ behaviour.
",bias,,bias,2023,"statement, concerns"
904,"Working on an under-resourced language is al-
ways accompanied by the danger of disenfranchis-
ing the language community. However, depen-
dency annotations require a syntactic background,
and there are often not enough speakers of the
language with such a training. We hope that the
Swahili community will adopt and improve our
treebank. We have consciously chosen a corpus
the can be freely distributed, rather than working
with the Helsinki Corpus of Swahili, which comes
with restrictive licensing requirements.","bias, lack of inclusivity",,"bias, lack of inclusivity",2023,concerns
905,"User studies were conducted after review by our
institution’s IRB, and participants were paid a fair
wage in accordance with our local government. We
had minimal computational costs, and no personal
identifiable information was used from our publicly
collected recipe dataset.
",no ethical concerns,,no ethical concerns,2023,statement
906,"This work addresses one piece of the much broader
set of questions surrounding how biases—from
low-level word associations to high-level social
biases—manifest in natural language, and the ef-
fects that they have on the models that we train and
develop as researchers and practitioners. Parsing
out how such biases transfer to models, and when
they are harmful, has been and will continue to be
key to making progress towards understanding the
technologies we create and the scope of what they
can or should do.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
907,"We discuss the ethical considerations and broader
impact of this work here: (1) Intellectual prop-
erty. The copyright of ACE 2005 belongs to LDC6.
We access it through our LDC membership and
strictly adhere to its license. We believe the estab-
lished ACE 2005 dataset is desensitized. In our
consistent evaluation framework, we will only pro-
vide preprocessing scripts rather than preprocessed
datasets for those datasets whose licenses do not
permit redistribution. The ACE-DYGIE preprocess-
ing script7and the used code repositories for DM-
CNN8, DMBERT8, BiLSTM,CRF8, BERT,CRF8,
EEQA9, and Text2Event10are released under MIT
license. These are all public research resources.
We use them for the research purpose in this work,
which is consistent with their intended use. (2) In-
tended use . Our consistent evaluation framework
implements the suggested remedies to avoid the
identified pitfalls in EE evaluation. Researchers are
supposed to use this framework to conduct consis-
tent evaluations for comparing various competing
EE models. (3) Misuse risks . The results reported
in this paper and the evaluation results produced
by our consistent evaluation framework should not
be used for offensive arguments or interpreted as
implying misconduct of other works. The analyzed
pitfalls in this work are inconspicuous and very
easy to be accidentally overlooked. Hence the com-
munity is generally unaware of them or underesti-
mates their influence. The contribution of our work
lies in raising awareness of the pitfalls and helping
to avoid them in future works. (4) Accessibility .
Many widely-used datasets (such as ACE 2005,
KBP, etc.) are not freely available to everyone. The
financial fairness issue may influence the broader
usage of the data for EE research.
","research misuse, fairness",,"research misuse, fairness",2023,"statement, actions, advice"
908,"This study is conducted under the guidance of the
ACL code of Ethics. We manually filtered out
potential offensive content and removed all infor-
mation related to the identification of annotators.
The annotators are all fairly paid based on the Aus-
tralian minimum wage. The annotation protocol is
approved under Human Ethics LNR Application
with reference number 2022-24233-30104-3.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
909,"In this work, we introduce a new NLP task related
to fMRI and a unified approach for decoding vari-
ous types of cognitive signals into human language.
We conduct our experiments on the public cog-
nition datasets Narratives andZuCo1.0 with the
authorization from the respective maintainers of
the datasets. All experimental datasets involved
have been de-identified by dataset providers and
used for research only.
",no ethical concerns,,no ethical concerns,2023,statement
910,"The dataset originally used for testing the search
engine partially consisted of grammars subject to
copyright. In order to avoid any form of copyright
infringement, we left only ten grammars in Google
Drive and in the source code repository. The gram-
mars are stored in the dataset solely for the purpose
of demonstrating the functionality of the search
engine. Each of the ten grammars is part of the
open-access set maintained by Språkbanken15, is at
least 100 years old, and is not subject to copyright.
15https://spraakbanken.gu.se/blogg/index.php/2
020/04/07/a-multilingual-annotated-corpus-of-wor
lds-natural-language-descriptions/
",no ethical concerns,,no ethical concerns,2023,statement
911,"Data on Nen were gathered by Evans under the
projects Language and Social Cognition (ANU
Aries protocol 2008/253), Languages of South-
ern New Guinea (ANU Aries protocol 2011/313)
and The Wellsprings of Linguistic Diversity (ANU
Aries Protocol 2014/224). Nen data are lodged on
open access in the PARADISEC archive.
",no ethical concerns,,no ethical concerns,2023,statement
912,"In compliance with the ACL Ethics Policy, we ac-
knowledge the potential ethical considerations as-
sociated with this research on automatic speech
recognition for dialectical Arabic. The proposed
AraDiaWER metric is intended to provide a more
comprehensive and explainable evaluation of DA
ASR systems that can better account for dialectical
variations. However, we acknowledge the potential
impact of any inaccuracies in the system, particu-
larly regarding sociocultural implications. As such,
we urge caution in the use and application of this
metric and encourage future research to further
explore the impact of such technology on diverse
groups and communities. We are committed to
ethical research practices and will prioritize trans-
parency and accountability in all future studies.
","inaccuracies, sociocultural implications",,"inaccuracies, sociocultural implications",2023,"concerns, suggestions"
913,"All four participants have signed an informed con-
sent form with Athena R.C. for their contribution
to the narration of the voice samples.
",no ethical concerns,,no ethical concerns,2023,statement
914,"The collection of audio which is stored and made
available in Speech-DB is covered by an ethics
review and approval at the University of Alberta
(Study ID: Pro00023436). The platform described
in this manuscript has been developed in order to
support the explicit objectives of the language com-
munities in question to record how their language
is spoken in their communities and make that avail-
able for their next generations.
",no ethical concerns,,no ethical concerns,2023,statement
915,"This study stems from a wider project to collect
various documentation materials for Kolyma Yuk-
aghir, and its close relative Tundra Yukaghir, and
standardize them in the practical orthographies to
make them more accessible to community members.
With these materials, different studies are being car-
ried out using machine learning methods in order to
deepen our understanding of the grammatical struc-
ture of the languages. Ultimately, the goal is to
use this knowledge to support language revitaliza-
tion initiatives under way in the community.
Additionally, in this article we refrain from engag-
ing in a “numbers game” to characterize the context
of language endangerment in the Yukaghir commu-
nity, as numbers are not well equipped to describe,
explain or contextualize the factors that cause pro-
cesses of language shift ( Dobrin et al. ,2009 ;Moore
et al. ,2010 ;Davis ,2017 ).
Abbreviations
1 first person
3 third person
ADVZ adverbializer
ATTR attributive
CTX contextual
CVB converb
EP epenthesis
EV evidential
GEN genitive
IMPF imperfective
INCH inchoativeLOC locative
NMLZ nominalizer
NONIT noniterative
PL plural
PROL prolative
PTCP participle
RES resultative
SG singular
TEMP temporal
UNK unknown/unclear
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
916,"The audio (and visual) data from the three YouTube
channels was transferred by participants after dis-
cussing the project and possible impacts of shar-
ing their data (Ethics Approval No. 2017/889
of the Australian National University Human Re-
search Committee, Speech Recognition; Building
Datasets from Indonesian Language Classrooms
and Resources protocol). Files were screened for
intelligible speech from people other than the par-
ticipant and those containing such data were re-
moved from the dataset. The non-author transcriber
referred to in Section 2 completed the transcription
as part of an exchange of editing and proof reading.
Our appreciation for his contribution to the project
is expressed in our
",no ethical concerns,,no ethical concerns,2023,statement
917,"As stated in section 1, trustworthy automatic eval-
uation metrics are indispensable for selecting and
deploying large language models. Metrics that cap-
ture negations and reduce the overall score for mod-
els that mix up negated and original sentences are,
therefore, an important step to increase trust in the
metrics themselves, but also in the evaluated mod-
els. Moreover, models can be trained to improvenegation sensitivity with metrics that detect nega-
tion insensitivity, as well as the CANNOT dataset.
Therefore, we do not see ethical concerns with our
negation-aware metrics or datasets.
However, our negation tool can add or remove
a negation to any input sentence. If applied to sen-
tences from the Internet, such as news articles or
Twitter posts, it can easily alter the information
provided. The negated and original versions still
look very similar, and thus, people might oversee
the missing or added negation cues when compar-
ing the provided information with other sources.
Consequently, we are aware that our negator may
be used in a malicious way to spread misinforma-
tion. Nevertheless, negation-aware sentence em-
beddings and evaluation metrics could again detect
such modifications. We believe that the benefits of
an open-source tool for researchers, as well as the
simplified dataset creation it enables, outweigh the
drawbacks of potential misuse.
Supplementary Materials Availability State-
ment: All material used in this paper is available
to the research community. The sentence nega-
tion tool is published as a Python package and in
a GitHub repository. The dataset and source code
for fine-tuning on this data is also open-sourced
on GitHub and Hugging Face. The checkpoints
of our models are available on the Hugging Face
Model Hub. The links to the individual resources
are referenced in their respective paper sections.
",misuse,,misuse,2023,"statement, concerns"
918,"ATS systems can provide more accessible versions
of texts, however, a good text simplification is tar-
geted to the knowledge and language level of its
audience. Therefore, to utilize these systems for
the target group directly, the systems need to be de-
ployed in a controllable setting where the user can
set the level of simplification or ask for additional
explanations if necessary. Nevertheless, there are
also applications where ATS systems can increase
the amount of accessible information on the internt
withput being used by the target group directly. For
example, these systems can yield a draft simplifi-
cation for professional translators or can be help-
ful for public state authorities that are forced by
law to offer online information in Easy Language.
Another problem is the possible stigmatization of
users if they request a simplified version of the data
(Hansen-Schirra, 2020). Finally, the availability of
information in Easy Language is very sparse; thus,
it is hard to fact-check material on the internet with
other sources. This makes the target group of Easy
Language highly vulnerable to misinformation and
fake news. Hence, our generative models must be
used with care as they do not provide hallucination
control.
Among the sources of our dataset, there is a sig-
nificant bias towards news articles as well as some
regional bias due to the large proportion of arti-
cles related to Austria, Switzerland, and northern
Germany. As all sources are from official website
articles, and the dataset does not include user com-
ments, we expect the data to be unoffensive and of
high quality. Nevertheless, we find topical biases
such as the COVID-19 pandemic due to the years
from which the articles were scraped. In respect
of any intellectual property laws, we published the
scrapers used to obtain the data but not the data
itself.
","misuse, stigmatization, misinformation, bias",,"misuse, user stigmatization, misinformation, bias",2023,"concerns, actions, advice"
919,"We artificially increased the size of our knowledge
data set with few-shot data augmentation to cover
a broader range of traveller types and domains.
The synthetic reviews were created using GPT-3
and ChatGPT. There exists a potential for the in-
troduction of biases in this newly generated data
due to inherent biases that might have been
propagated into the reviews. This is especially con-
cerning for commercial LLMs, like the ones used,
since their training data is not made public.
",bias,,bias,2023,statement
920,"We artificially increased the size of our knowledge
data set with few-shot data augmentation to cover
a broader range of traveller types and domains.
The synthetic reviews were created using GPT-3
and ChatGPT. There exists a potential for the in-
troduction of biases in this newly generated data
due to inherent biases that might have been
propagated into the reviews. This is especially con-
cerning for commercial LLMs, like the ones used,
since their training data is not made public.
",bias,,bias,2023,statement
921,"Using public LLMs for KG-to-text poses a chal-
lenge in extracting explanations for the choices
made by the system. Even if LLMs at some point
in the near future outperform task-specific models
on any NLG task, it may be worth using smaller
models specifically to retain control over the model
or to achieve explainability.
The use of crowdworkers for the types of anno-
tation and evaluation we presented in Section 4.3.1
did not require ethics approval at our institution.
We made an attempt to filter our WikiData
dataset such that it would not contain offensive
statements. The Wikidata data synthesis process
described in Section 4.2 was rerun when an ear-
lier version of our dataset was found to contain
statements about individuals connected to histori-
cal events – specifically the Holocaust – that could
be interpreted as Holocaust denial. It is nonetheless
possible that counterfactual or factual statements in
our current dataset, or LLM hallucinations relating
to them, could have been perceived as offensive to
our Mechanical Turk annotators, on account of the
random nature of the process.
9 Data Availability statement
Data files containing model output as well as
annotator judgements have been made available
on GitHub at https://github.com/Agnesion/
zero-shot-NLG-from-KGs-data .
10 ",no ethical concerns,,no ethical concerns,2023,"statement, actions"
922,"Our results indicate that compression does harm
fairness, particularly in the monolingual setting.
The potential harm that the system may cause and
the application it will be used for should be con-
sidered when selecting a model compression tech-
nique, in addition to factors like accuracy, latency,
and size. Although we have not observed absolute
trends across models, datasets, and compression
techniques, it is especially crucial to evaluate com-
pressed models for fairness and accuracy before
deployment and, on a broader note, to understand
why compressed models might exhibit issues with
respect to fairness.
In our paper, we conducted evaluations of multi-
lingual language models using fairness metrics for
various languages, including English. We observed
varying trends regarding their performance on fair-
ness metrics across different languages. However,
it is vital to consider the potential influence of the
lack of well-optimized models for these specific
tasks, which may mitigate some of these issues.
Additionally, evaluation datasets are scarce for as-
sessing bias in languages other than English and for
different fairness definitions. We also acknowledge
that fairness trends identified in English evaluations
may not necessarily be true for all languages.
While our benchmarking encompassed multiple
intrinsic and extrinsic metrics, it is important to
acknowledge their limitations in capturing all di-
mensions of fairness. Further research is needed
to develop comprehensive extrinsic metrics across
diverse tasks. Although our work has been cen-
tered around fairness in allocation-based (classifi-
cation) applications, addressing fairness concerns
in other types of language models, such as natural
language generation models, is necessary. In gen-
erative tasks, the measurement of unfair outcomes
would be distinct from the methods we have used.
Another area of potential future work could involve
benchmarking debiasing methods for compressed
models and developing new compression-aware
methods.
","fairness, bias",,"fairness, bias",2023,"concerns, actions, suggestions"
923,"This research complies with the principles of the
ACL Ethics Policy. Cross-linguistic morphosyntac-
tic resources have the potential to aid in the expan-
sion of computational resources to less-resourced
languages, but we note that the needs and inter-
ests of language communities vary and that digital
equity and inclusivity require the involvement of
those communities in research and development of
technologies.
",no ethical concerns,,no ethical concerns,2023,"statement, advice for researchers"
924,"In our experiments, we find that our methods usu-
ally copy more words from the context or encoder
input. The tendency might have some potential
issues. For example, our improvements might be
reduced on the languages with more morphology.
Furthermore, in some summarization applications,
increasing the factuality by increasing the extrac-
tiveness might not be ideal (Ladhak et al., 2022;
Goyal et al., 2022a).
As described in Section 2.1, one major limita-
tion of the popular softmax layer is its global wordembeddings. The problem would become more se-
rious when there are more tokens whose meanings
are locally defined (e.g., names in the BookSum
dataset). Our methods would be more useful in
those circumstances and might alleviate some bi-
ases described in Shwartz et al. (2020) and Ladhak
et al. (2023). Moreover, the meaning of tokens
are also locally defined in many other applications
such as variables in code or math problems, the
new terminologies in a scientific paper, or the prod-
ucts in a sequential recommendation problem. We
believe that our methods could become an efficient
alternative of reranker (Cobbe et al., 2021; Welleck
et al., 2022) and create impacts in those areas.
Finally, our results show that when there are
some uncertainties in the next word (e.g., could be
king orwoman ), existing LMs could have some
difficulties of copying the words from the context
and our methods alleviate the problem. Thus, our
methods should also be able to improve the lexi-
cally controllable language generation models that
put the desired keywords into the context such as
Goldfarb-Tarrant et al. (2019) and Lu et al. (2021).
",inefficiency,,inefficiency,2023,"concerns, suggestions"
925,"The use of crowdsourcing: We recruited human
evaluators in Amazon Mechanical Turk. Our evalu-
ation task does not collect any personal information
other than anonymized worker IDs and country of
residence (due to our location-based worker qualifi-
cation). We do not plan to release this information
to the public. We set the task reward based on
trial studies so that the estimated hourly rate would
reach at least $9.00.
The risk in the inclusion of situational infor-
mation: While we believe that incorporating sit-
uational information can have a positive impact
on conversational technologies in general, as pre-
viously mentioned, it is not intended to address
well-known issues concerning the toxic behavior
of language generation models. Rather, it may in-
troduce another source for models to learn undesir-
able associations between concepts and language.
Therefore, the data and system output should be
closely monitored, either manually or through au-
tomatic methods such as debiasing techniques (Liu
et al., 2020; Dinan et al., 2020a).
",bias,,bias,2023,"statement, advice"
926,"Since IDAS automatically recovers intents from ut-
terances, e.g., those exchanged between users and
support agents, any prejudices that may be present
in these utterances may become apparent or even
amplified in intents inferred by our model, since
clearly IDAS does not eliminate such prejudices.
Hence, when designing conversational systems
based on such inferred intents, extra care should
be taken to prevent them from carrying over to
conversational systems deployed in the wild.
Moreover, since IDAS ’s label generation process
relies on LLMs, biases that exist in the data used
to train these LLMs may be reinforced, leading to
generated labels that may discriminate against or
be harmful to certain demographics.
",bias,,bias,2023,"statement, suggestions"
927,"We acknowledge that there are potential ethical
concerns associated with the use of large language
models in our evaluation method.
A primary concern is the biases present in large
language models. These biases are introduced dur-
ing training, as the models learn from textual data
that may contain biased information, stereotypes,
or misinformation. When using these biased mod-
els for evaluation, it is possible that the evaluation
scores produced by LLM-E VAL may reflect and
perpetuate these biases, potentially leading to bi-
ased evaluations of dialogue system outputs. This
could, in turn, affect the development of future
dialogue systems by encouraging biased behavior.
To mitigate this concern, researchers and devel-
opers should be cautious when interpreting the
evaluation results obtained through LLM-E VAL
and consider potential biases in the large language
models used. Moreover, future work could explore
techniques to debias language models or employ al-
ternative evaluation schemas that actively account
for biases in the evaluation process.
",bias,,bias,2023,"concerns, advice, suggestions"
928,"Undesired bias and abusive content: A mul-
titude of sources have reported that data-driven
conversational systems can (re)produce undesired
bias or abusive language existing in language re-
sources used for development. To minimize such
a risk, we carefully curated conversation exam-
ples in SUGAR. Our target task is response se-
lection, where systems only produce language in
a pre-compiled response list, and therefore, it is
not likely that resulting systems yield harmful con-
tent. However, users of SUGAR should be cautious
when it is used for developing generation systems
in future work.
Human subjects: Crowd workers in Amazon
Mechanical Turk (MTurk) participated in our data
collection pipeline. Our annotation tasks were re-
viewed by the institutional review process before
being published in MTurk to avoid ethical issues.
We did not collect any personally identifiable infor-
mation of workers other than (anonymized) Turker
IDs. Task rewards were decided by several rounds
of trials so that workers can receive at least $6.50
hourly.
Use of external data and tools: We used exter-
nal datasets such as ATOMIC20
20and ConceptNet
and tools such as spaCy and Transformers library.
We have confirmed that the use of these resources
for our research does not violate usage restrictions.
",no ethical concerns,,no ethical concerns,2023,"statement, advice, actions"
929,"We foresee no ethical concerns with this work.
",no ethical concerns,,no ethical concerns,2023,statement
930,"We foresee no ethical concerns with this work. Our
work aims to make the inner workings of neural net-
work models more interpretable. On this account,
we hope to contribute to reducing biases inherent
in model architectures, pre-trained model weights,
and tasks by increasing overall transparency.
",no ethical concerns,,no ethical concerns,2023,statement
931,"Our data are taken from publicly available sources.
For this reason, we do not expect that there are
ethical issues or conflicts of interest in our work.
Supplementary Material
The supplementary material accompanying this
study contains the data and code needed to
replicate the results reported here, along with
detailed information on installing and using the
software. It is curated on GitHub ( https://
github.com/pano-tacanan-history/
trimming-paper , Version 1.1) and
has been archived with Zenodo ( https:
//doi.org/10.5281/zenodo.7780719 ).
",no ethical concerns,,no ethical concerns,2023,statement
932,"In the beginning of the challenge, we discovered
the answers of the experimental test data were acci-
dentally released early by the organisers. We imme-
diately informed the organisers and as requested,
we deleted the test data and committed to not use
it until it was officially released. Our work was
trained on speech corpora of adults which were
recorded with ethics approval. The broader impact
of the work includes i) improving how morpho-
logical inflection models can be trained with low-
resource languages or phenomena, ii) developing
speaker-specific morphological inflection models,
iii) establishing a new baseline model architecture
(LDL) that has a low carbon footprint.CRediT authorship contribution statement
CJ, DS, and AKR contributed equally to this work.
KT served as the senior and corresponding author
on this paper.
We follow the CRediT taxonomy2. Conceptual-
ization: KT; Data curation: CJ, AKR; Formal Anal-
ysis: AS, DS; Investigation: KT, DS, CJ, AKR;
Methodology: KT; Supervision: KT; Visualization:
AS; and Writing – original draft: KT and Writing –
review & editing: KT, AS, CJ, DS, AKR.
",carbon footprint,,carbon footprint,2023,"statement, actions"
933,"All experiments are based on publicly available
corpora. Even though some of the corpora contain
personal information, they have been cleared for
publication. The reported experiments do not intro-
duce any new artifacts that would be problematic
from an ethical point of view.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
934,"We have used existing speech datasets and off-the-
shelf tools for speech recognition and synthesis.
The use of the existing voices of the native speaker
of one language, in this case French, to synthesiseartificial non-native English speech is taken as rep-
resentative of an L2 learner speaking English for
the first time. There is much to be learned about
speech variation from such artificially generated
speech but is should not be regarded as mocking
non-native speaker endeavours to learn a language.
Indeed the variants learned from such data can pro-
vide useful insights for speaker accommodation.
",no ethical concerns,,no ethical concerns,2023,statement
935,"To the best of our knowledge, all results published
in this paper are accurate, and we have represented
prior work fairly to the best of our abilities. All
data sources are free and publicly available, except
for the Penn Arabic Treebank (Maamouri et al.,
2004), which is accessible through the LDC.7No
sensitive data was used which could violate indi-
viduals’ privacy or confidentiality. Authorship and
acknowledgements fairly reflect contributions.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
936,"Our work is directly applicable to low- and very
low-resource languages. This carries great promise
of giving more groups access to technology; how-
ever, in developing the resources, there is also the
danger of disenfranchising native speaker infor-
mants and making unwanted normative linguistic
decisions. As part of our work so far, we are re-
lying on previously collected datasets (except for
the Sudanese dataset which we created ourselves),
but in the future, if we decide to gather data from
unstudied Arabic dialects, we will be cognizant of
the dangers inherent in data collection.
Our work is fundamental research which aims
at creating a system which generates human-
inspectable rules which do not over-generalize.
These rules cannot themselves be used without a
further system (such as a morphological generator
or analyzer). We recognize that our work could
be used to identify non-standard speech commu-
nities with the goal of forcing standard speech on
them; any linguistic field work runs the same dan-
ger. We believe any attempt to homogenize dialec-
tal variation (in the name of political nationalism,for example) does not require NLP; for example,
European nation states like France and Germany
were quite successful in repressing dialectal varia-
tion in the 19th and 20th centuries before NLP. It
seems far-fetched to believe that our work would
enable language homogenization.
","bias, misuse",,"bias, misuse",2023,"statement, concerns, suggestions"
937,"The authors have carefully considered the ethics
of conducting this kind of study regarding racial
stereotypes on social media, and include here their
assessment of the ethical issues raised and how to
approach them. Throughout the project, they will
be critically reflexive about unanticipated ethical
issues arising from its sensitive, qualitative and
digital nature.
The research presented in this work does not
include any studies with human participants car-
ried out by any of the authors. Furthermore, the
data that was used is textual content from social
media extracted from datasets publicly available
to the research community and which also con-
form to the Twitter Developer Agreement and Pol-
icy, which allows for the unlimited distribution of
the numeric identification number of each tweet.
Each tweet in Italian, Spanish, and French has
been translated and adapted into English in order
to ensure the anonymity of the author.
—
Hiring policy: beside the authors of this article,
other researchers were involved. We hired two
Italian native-speaking annotators (one male and
one female: a master’s degree student in Linguis-
tics, and a pre-doctoral student). We hired two
Spanish native-speaking annotators (one male and
one female, both Linguistics undergraduate stu-
dents in their last year). We hired three French
native-speaking annotators (three females, two
master’s degree students in “Linguistics, Commu-
nication and Gender”, and one Linguistics under-
graduate student). All hired workers have either
received a monetary compensation or university
credits valid for their career.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
938,"The research presented in this paper relies on the
labour of numerous contributors who annotated the
dataset. We recruited and rewarded our contrib-
utors through Prolific, a crowdsourcing platform
we selected specifically for its attention to fair and
ethic treatment of crowdworkers. The contribu-
tors were paid on average an hourly wage of 12.66
GBP (about 14.95 USD). Additionally, fixed bonus
payments were provided for contributors who aban-
doned the task but still provided valuable feedback.
The data perspectivist approach in general, and
this work in particular, aims at “giving voice to
the few who hold a minority view” (Basile et al.,
2021a). Applied to the creation of a language re-
source, this principle leads to resources (and there-
fore models) where bias is a controlled factor rather
than undesirable criticality.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
939,"Broader impact Although explainability and
fairness are broadly viewed as intertwined subjects,
very little work has studied the two concepts to-
gether (Feng and Boyd-Graber, 2019; González
et al., 2021; Ruder et al., 2022). This study is a first
of its kind to examine fairness issues of explainabil-
ity methods and to publish human rationales with
diverse socio-demographic information. We hope
this work will impact the NLP research community
towards more data-aware and multi-dimensional
investigations of models and methods, and towards
further studies of biases in NLP.
Personal and sensitive data This study deals
with personal and sensitive information. The re-
sponses are anonymous and cannot be used to iden-
tify any individual.
Informed consent The participants were in-
formed of the study’s overall aim, the procedure
and confidentiality of their responses. With this
information, the participants consented to the use
and sharing of their responses.
Potential risks We do not anticipate any risks
of participation in the study, yet we do note a re-
cent awareness of poor working conditions among
crowdworkers for AI data labeling in some coun-
tries (Williams et al., 2022). The recruitment plat-
form Prolific, used in this study, is targeted to-
wards research (rather than AI development) and
has stricter rules on participant screening and mini-
mum wages (Palan and Schitter, 2017), compared
to other popular platforms, which we hope reduce
the risk of such poor working conditions.
Remuneration The participants were paid an av-
erage of 7.1£/hour ( ≈8.8$/hour).
Intended use The collected annotations and de-
mographic information will be publicly available
to be used for research purposes only.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
940,"Our work aims to broaden NLP coverage by al-
lowing practitioners to identify relevant data in
more languages. However, we note that LID is
inherently a normative activity that risks excluding
minority dialects, scripts, or entire microlanguages
from a macrolanguage. Choosing which languagesto cover may reinforce power imbalances, as only
some groups gain access to NLP technologies.
In addition, errors in LID can have a significant
impact on downstream performance, particularly
(as is often the case) when a system is used as a
‘black box’. The performance of our classifier is not
equal across languages which could lead to worse
downstream performance for particular groups. We
mitigate this by providing metrics by class.
","misrepresentation, bias, inaccuracies",,"misrepresentation, bias, inaccuracies",2023,"concerns, actions"
941,"There has been significant concern recently over
massive multilingual NLP models learning racial
andgenderbiasesduringpretraining( TanandCelis ,
2019;Bender et al. ,2021). A technique like CCS
thatleveragesmassiveNMTmodelscouldbeatrisk
of propagating any such biases present in the `base'
model. While such a limitation is not unique to
CCS and could apply to any technique harnessing
large models (such as Knowledge Distillation ap-
proaches), it is an important ethical concern since
biases that are propagated this way could be
harder to detect and control - as compared to data
biases. In such a situation, it could be worthwhile
to invest effort into curating more `unbiased' data,
and then using models trained from scratch on this
data as the base models for CCS (see Table 4) - giv-
ingagreaterdegreeofcontrolthanmassivemodels
likemBART50butpotentiallyyieldingcomparable
performance.
8
",bias,,bias,2023,"concerns, actions"
942,"We will follow the licenses of GossipCop (Shu
et al., 2018) and CoAID (Cui and Lee, 2020) to
share the training, development, and test datsets in
our experiments.
",no ethical concerns,,no ethical concerns,2023,statement
943,"We follow the ACL Code of Ethics. In our work,
there are no human subjects and informed consent
is not applicable.
",no ethical concerns,,no ethical concerns,2023,statement
944,"In this paper, we propose a pre-training framework
based on synthetic data to improve the ability of the
model to generate explanation graphs. The datasets
and model we used are all open-source and all
the references that draw on the work of others are
marked with citations. In the process of construct-
ing the corpus, we ensure that all the triples come
from ConceptNet, an open source knowledge base.
All the steps involving selection are completely ran-
dom, so that nothing such as bias or discrimination
is introduced in following steps. Finally, our ap-
proach is designed to improve the interpretability
of the model and won’t deviate from the semantics
in the input text, so there is no ethical issues in this
work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
945,"All datasets used in this paper are publicly avail-
able. For CLUTRR dataset, the gender informa-
tion is essential to tell if, e.g., A is B’s uncle or
niece. We used GPT-3 to predict the genders of
persons in each story. Since each story is systemati-
cally generated using sampled common first names
and sampled sentence templates, it does not reveal
any identity. As mentioned, the original CLUTRR
dataset had some errors, and we describe carefully
the codes and settings of the generated CLUTRR
1.3 dataset in Appendix B.1.
",no ethical concerns,,no ethical concerns,2023,statement
946,"The finding and proposed method aims to im-
prove the adapter based tuning in terms of tuning
parameters and performances. The used datasets
are widely used in previous work and, to our knowl-
edge, do not have any attached privacy or ethical
issues.
",no ethical concerns,,no ethical concerns,2023,statement
947,"Our proposed benchmark is a free and open re-
source for the community to study table question
answering over implicit and multi-type tables. We
follow existing works AITQA (Katsis et al., 2022)
and DuSQL (Wang et al., 2020b) to select data
sources when building our dataset. Tables are col-
lected from the public organization China Securi-
ties Regulatory Commission and Baidu Encyclo-
pedia of Baidu Company. We got the reply and
permission from these two organizations which
allow us to share and redistribute data for non-
commercial use. Tables in these annual reports
and web pages are open to public so there is no
privacy risk.
In the annotation process, we asked annotators
to check if there exist any offensive content such
as insulting or discriminatory speech. They did
not find any such content in our benchmark. We
also checked for identifiers and replaced identi-
fying information with mono-directional hashes.
We recruit 10 professional annotators with bache-
lor degree (6 males and 4 females) and pay them
at a price of 6.5 dollars per hour (above the av-
erage local payment of similar jobs). The total
time cost for annotation is 1,200 working hours.
Annotators were informed that these labeled data
would be used as a table question answering dataset.
The data collection protocol was approved by an
ethics review board of an IT company. Main exper-
iments in this paper can be run on a single NVIDIA
GeForce RTX 3090 GPU. Our dataset follows the
Computational Use of Data Agreement v1.06.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
948,"The datasets we use are all public, and our experi-
ment processes have no privacy disclosure issues.
As for human evaluation, all participants are vol-
untary and paid, and come from master or doctoral
students with a background in computer science
or computational linguistics, and all of them are
proficient in English. They first need to read the
instructions and evaluate without revealing which
model generates which summary.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
949,"Although the ethical waters of the development and
deployment of LLMs are difficult to nagivate, we
can ascertain that our study does not bring forth
further complications. The datasets we use in this
study are established benchmark datasets from pub-
licly accessible websites and do not contain any
personally identifiable information. Our analyses
does not constitute human subjects and thus is not
within the purview of the IRB. Further, in the land-
scape of increasing emission costs from large-scale
computation, our study offers avenues for severely
restricting the size of the training data - both lin-
guistic and non-linguistic.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
950,"We present a task bot that is able to converse with
users to complete real-world tasks. No personal
or identifying information is included throughout
conversations. In addition, our bot includes a safety
check to ensure safe conversations. We reject inap-
propriate task requests and prevent showing danger-
ous tasks, where users and their properties may get
hurt. To this end, we perform rule-based matching
against a keyword blacklist to filter out inappropri-
ate tasks. Meanwhile, for response generation, we
don’t directly use LLMs (such as ChatGPT) to gen-
erate answers for users’ questions, which will have
the risk of leaking user data to third-party APIs.
Instead, we utilize LLMs to do data augmentation
and domain adaptation, and train models locally
for the sake of privacy protection.
Author Contributions
In this work, each author makes significant contri-
butions that collectively enhance the final outcome.
Lingbo Mo played a crucial role in constructing
and organizing the codebase, and building an inter-
active interface for the demo. Shijie Chen and Ziru
Chen co-led the team during the challenge, laying
the groundwork for the bot’s development. Xiang
Deng and Tianshu Zhang were responsible for the
NLU pipeline. Xiang Yue mainly developed the
QA module. Lingbo Mo and Zhen Wang worked
together to build the backend knowledge base and
the search engine. Samuel Stevens provided engi-
neering support for constructing an automated test
suite. Ashley Lewis focused on enhancing user en-
gagement. Chang-You Tai contributed to chit-chat
and PAK features, while Sunit Singh assisted in
designing the demo interface. Huan Sun and Yu Su
are faculty advisors and offered valuable guidance
and feedback.
",no ethical concerns,,no ethical concerns,2023,"statement, actions "
951,"Our work contributes to the line of research that
focuses on improving the adversarial robustness
of language models. We also explore novel ways
to integrate human explanations into the training
paradigm. We believe robustness to adversarial at-
tacks is essential to the deployment of trustworthy
models in the wild, and we hope this work brings
current research a step closer to this objective. To
avoid ethical concerns related to over-claiming re-
sults, we emphasize in both our concluding dis-
cussion and the limitations section that our work
builds on the assumption that we know the type
of attack and only experiments with ADDSENT.
Furthermore, our approach tends to increase the
computational cost compared to adversarial train-
ing both during training and inference. One should
consider the tradeoff between robustness and com-
putation.
",computational cost,,computational cost,2023,"statement, actions"
952,"We comply with the ACL Ethics Policy. First,
document-level event factuality identification is a
fundamental research of natural language process-
ing that benefits many downstream tasks. It reveals
the factual property of an event in the document
and does not generate any uncontrolled biased or
toxic text. Second, the source data we collect come
from open sources, and everyone can access them.
Finally, we guarantee that the paper is completely
written by authors and not automatically generated
by AI models to ensure authenticity.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
953,"It is worth noting that all the data used in this paper
are publicly available, and we utilize the same eval-
uation scripts to make sure that all the comparisons
are fair. We have replaced the people names in
the corpora with special placeholders to mitigate
the problematic biases (Radford et al.) issue of
generation results. Although we have taken some
methods to mitigate the problematic biases, such a
problem cannot be solved completely. We urge the
users to cautiously apply our methods in the real
world and carefully check the generation results.
",bias,,bias,2023,"concerns, actions, advice"
954,"Our method heavily relies on the pre-trained lan-
guage models, e.g., RoBERTa, which may inherit
the problematic biases (Radford et al.). We haveattempted to mitigate these issues by conducting
experiments on comparatively innocuous story gen-
eration and opinion generation tasks. Furthermore,
we have replaced all the names in those corpora
with special placeholders. Although some mea-
sures are taken to mitigate the problematic biases,
such issues cannot be solved completely. Thus,
we urge the users to carefully examine the gener-
ation results and cautiously apply our method in
real-world applications. Additionally, it is worth
noting that all the corpora used in our experiments
are only for scientific research.
As for the human evaluation process, we resort
to open source web library Django||to build our
own human evaluation interface. Before releasing
the human evaluation cases, we carefully check that
there is no private information or other problematic
biases in the cases. Besides, we did not collect per-
sonal information or ask the annotators about their
private information during the annotation process.
We hired three annotators and paid each of them
$0.29 for each case comparison. The payment is
reasonable since there are only 100 cases for anno-
tation, and it would cost average 4 hours for one to
finish all the comparisons.
",bias,,bias,2023,"concerns, actions, advice"
955,"In this paper, we present a defense mechanism to
counter the impact of backdoor attacks. Our code
and datasets will be publicly available. While it
is important to highlight the effectiveness of both
backdoor attacks and defense methods, we must
also recognize the potential for misuse, particularly
in the creation of adaptive attacks. However, by
making our defense strategy and implementation
public, we may expose our method to attackers,
who may discover its weaknesses and develop new
types of attacks.
",misuse,,misuse,2023,"statement, concerns"
956,"Our Distillation from Weak Teacher (DWT) frame-
work facilitates enhancing larger student models
through knowledge transfer from smaller, weaker
teacher models. However, our research findings
indicate that the effectiveness of the teacher model,
particularly when it is extremely weak, can have a
negative impact on the quality of the student model.
Consequently, the utilization of our DWT frame-
work should be approached with caution, particu-
larly in high-risk domains like biomedicine. Eval-
uating performance prior to making critical deci-
sions may be necessary.
",inefficiency,,inefficiency,2023,"concerns, advice"
957,"The main objective of RESIDUAL PROMPT TUN-
ING is to improve parameter-efficient tuning of
large language models, which makes state-of-the-
art models more accessible to groups with limited
computational and data-labeling resources. We do
not believe there is any potential risk in the pub-
lished code or models in this work, as all of our
experiments are based on public data that is widely
used in the research community.
",no ethical concerns,,no ethical concerns,2023,statement
958,"This paper uses publicly available datasets and
models and therefore could carry on their potential
biases (Meister et al., 2022; Garcia et al., 2023) and
imperfections. However, the method presented in
this paper enables model and dataset interpretation
and we hope that it can help future work locate
harmful biases .
","bias, imperfections",,"bias, imperfections",2023,"statement, suggestions"
959,"This study was conducted only on observational
data and did not require any human intervention.
We did not use any information that could identify
individuals or specific demographic groups, and
all of our presented results were obtained through
aggregation from millions of users and comments.
",no ethical concerns,,no ethical concerns,2023,statement
960,"In this work, we construct new datasets for the
proposed Subsumption Inference (SI) task from
publicly available ontologies: Schema.org ,DOID ,
FoodOn , and GO, with their download links spec-
iﬁed in Section 4. The biMNLI dataset is con-
structed from the existing open-source MNLI
dataset. We have conﬁrmed that there is no pri-
vacy or license issue in all these datasets.
",no ethical concerns,,no ethical concerns,2023,statement
961,"This work focuses on the generalization issue of
knowledge base question answering, and the con-
tribution is fully methodological. Hence, there are
no direct negative social impacts of this work. For
experiments, this work uses open datasets that have
been widely used in previous work and are without
sensitive information as we know. The authors of
this work follow the ACL Code of Ethics and the
application of this work have no obvious issue that
may lead to the risk of ethics.
",no ethical concerns,,no ethical concerns,2023,statement
962,"Our work innovatively proposes the concept of
matching representation in dense retrieval and de-
signs a generalization improvement strategy that
can be flexibly combined with different dense re-
trieval training methods. Our work has important
implications for improving the performance of neu-
ral information retrieval models. We declare that
our work complies with the ACL Ethics Policy.2
",no ethical concerns,,no ethical concerns,2023,statement
963,"Persona-based dialogue generation aims to im-
prove the consistency of open-domain dialogue
generation while enabling dialogue generation to
be extended to more application scenarios. In
persona-based dialogue, the dialogue model uses
persona information in the process of dialogue gen-
eration. The purpose of using persona information
is to improve the consistency of the dialogue sys-
tem rather than guessing user identities or associat-
ing persona information with real-world users. In
this work, we use public datasets and do not in-
volve aggression or privacy concerns. Furthermore,
we use existing dialogue models for research, so
we have the same concerns as other dialogue gen-
eration research. For example, there is a risk of
generating toxic or biased language.
","bias, toxic content",,"bias, toxic content",2023,"statement, concerns"
964,"We take ethical considerations very seriously, and
strictly adhere to the ACL Ethics Policy. In this
paper, we present a survey of the major works on
datasets, approaches and evaluation metrics that
have been undertaken in ZPT. Resources and meth-
ods used in this paper are publicly available and
have been widely adopted by researches of ma-
chine translation. We ensure that the findings and
conclusions of this paper are reported accurately
and objectively.
",no ethical concerns,,no ethical concerns,2023,statement
965,"In this paper, we investigate the potential vulnera-
bility concerns of the neural information retrieval
(IR) systems and propose a document manipulation
framework that generates adversarial documents
that are not easily detected by both humans and IR
systems. We hope that this study could inspire fur-
ther exploration and design of adversarial ranking
defense/detection methods and aid in the develop-
ment of robust real-world search engines.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
966,"This paper proposes a Seq2Seq&set model for text-
to-table generation. We take ethical considerations
seriously and ensure that the research and meth-
ods used in this study are conducted in an ethical
and responsible manner. The datasets used in this
paper are publicly available and have been widely
adopted by researchers for testing the performance
of text-to-table generation. This study does not in-
volve any data collection or release, and thus there
exist no privacy issues. We also take steps to ensure
that the findings and conclusions of this study are
reported accurately and objectively.
",no ethical concerns,,no ethical concerns,2023,statement
967,"This work presents RaAS, an effective framework
for zero-shot semantic parsing. All of the involved
datasets come from publicly available sources. The
MRs and NLs are derived from several common
public datasets (Kate et al., 2005; Wang et al., 2015;
Herzig and Berant, 2019). The SCFGs are used for
canonicalizing MRs, which are from OVERNIGHT
andGEOGRANNO (Wang et al., 2015; Herzig and
Berant, 2019). Pre-trained models and evalua-
tion codes are all publicly accessible. The hyper-
parameter settings are given in this paper. Our code
and specification of dependencies will be released
in the future.
",no ethical concerns,,no ethical concerns,2023,statement
968,"Our method is first limited by the proposed gram-
mar that doesn’t cover all the realistic cases. As
shown in Table 1, there are still a few cases in the
randomly sampled 100 examples that none of the
defined rules can explain. Secondly, the time com-
plexity of our method is the cube of the sentence
length, limiting its direct applications on long doc-
uments. So we have to classify the document based
on classification of individual sentences, which
might be problematic since the sentiment of dif-
ferent sentences in the document may affect each
other.
All the experiments in this paper are conducted
on public available datasets, which has no data
privacy concerns. Meanwhile, this paper doesn’t
involve human annotations, so there are no related
ethical concerns.
",inaccuracies,,inaccuracies,2023,"statement, concerns"
969,"Synthetic data generated by language models may
involve potential ethical risks regarding fairness
and bias (Bommasani et al., 2021; Blodgett et al.,
2020), which results in further consideration when
they are employed in downstream NLP tasks. Al-
though the scope of this paper remains how to pro-
duce and leverage such synthetic data to improve
relation extraction system, it is worth further in-
vestigation to investigate in conjunction with well-
established methods that can measure (Nadeem
et al., 2021) and mitigate (Nadeem et al., 2021;
Gupta et al., 2022) such ethical risks.
","fairness, bias",,"fairness, bias",2023,"statement, suggestions"
970,"Text style transfer task is widely used in the field
of controllable text generation. However, because
of the diversified corpus of style, the model has the
potential to be both used for good and used with
malicious intent. For example, if one intentionally
changes the style (news) in the news field, fake
news may be generated. Moreover, in the realm of
politics, the transformation can give rise to fabri-
cated political statements, thereby engendering a
climate of misinformation and deceit.
We hired human annotators to evaluate our
method and two basic TST models. Here we show
the details of the employed annotators. The an-
notators were asked to score the model-generated
sentence. Considering the difference between the
two datasets, annotators will get $0.1 for each sen-
tence in the Yelp dataset and $0.2 for each sentence
in the IMDb dataset. There are 1600 sentences eval-
uated (800 sentences per dataset), so each annotator
was rewarded $240 in total.
",misuse,,misuse,2023,"statement, concerns, actions"
971,"In consideration of ethical concerns, we provide
the following detailed description:
1.All of the collected web documents come from
publicly available sources. Our legal advisor
and/or the web platform confirms that our data
source are freely accessible online without copy-right constraint to academic use. The data col-
lection protocol has examined and approved by
ethics review board.
2.WEBDOCS contains 300 annotated documents.
We have done double-checking to guarantee that
WEBDOCS contains no sample that may cause
ethic issues or involve any personal sensitive in-
formation. We also manually check the content
of each document instance to exclude any hate
speech or attack on vulnerable groups.
3.We hired 2 annotators who have bachelor de-
grees. Before formal annotation, annotators
were asked to annotate 20 document instances
randomly extracted from the dataset, and based
on average annotation time we set a fair salary
(i.e., 35 dollars per hour, which is adequate
given the their demographic) for them. Dur-
ing the annotation training process, they were
paid as well.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
972,"One potential ethical consideration related to a LS
method based on a paraphraser is the potential for
biased or unfair language generation. If the training
data used to develop the paraphraser is biased in
some way (e.g., it disproportionately represents
certain groups of people or uses certain words and
phrases in a biased manner), this could lead to
biased substitutions being generated by the model.
It is important to ensure that the training data used
to develop the model is diverse and free of bias in
order to minimize the potential for unfair or biased
language generation.
Another ethical consideration is the potential for
the LS method to be used for malicious purposes,
such as creating fake or misleading content. It is
important to consider the potential consequences
of the LS method’s outputs and to put safeguards
in place to prevent the LS method from being used
for nefarious purposes.
","bias, fairness, misuse, fake_content, misleading_content",,"bias, fairness, misuse",2023,"concerns, advice"
973,"The progress in deep neural network architectures
and the availability of large pre-trained language
models have led to significant advancements with
single document summarization. However, current
state-of-the-art natural language processing (NLP)
solutions still face challenges in consistently gen-
erating factual and faithful summaries without any
instances of hallucination (Maynez et al., 2020).
Therefore, it is imperative to acknowledge that our
proposed solution, like previous approaches, is not
yet suitable for deployment as it does not specifi-
cally address the issue of hallucination. To bridge
this gap, future research efforts should prioritize
the development of more effective evaluation mea-
sures and solutions for text summarization, aiming
to ensure highly faithful summaries that accurately
represent the source content and enhance the over-
all trustworthiness of summarization systems. Ad-
ditionally, in the case of applying the proposed
method to sensitive data domains such as medical
patient records and legal documents, it becomes
essential to incorporate privacy-preserving policies
to safeguard the confidentiality of personal infor-
mation (Da Silva et al., 2006). These measures are
critical to instill confidence in the practical imple-
mentation of text summarization techniques.
","hallucinations, inaccuracies, privacy",,"hallucinations, inaccuracies, privacy",2023,"concerns, advice, suggestions"
974,"This study employed a binary classification of gen-
der in our experimentation and description of the
methodology. It is our firm stance that such beliefs
have no place in the community, especially consid-
ering that language evolves with its users. However,
we believe that this narrow view of gender is neces-
sary as a step in the broader direction of full equity.
We hope that when high quality datasets displaying
non-binary genders are released in a form usable
by PCGU, researchers may revisit this paper and
study an inductive extension of PCGU.
We also recognize the fact that any method used
for debiasing may possibly be reversed to train
extremely bigoted models, possibly for trolling or
targeted harassment. However, we believe that any
such practice for PCGU would not be more harmful
than existing training methods. As observed in our
experiments, even when looking to increase the
probability of logits only (as opposed to explicitly
decreasing the advantaged sentence), the language
modeling score still suffers. Therefore, there seems
to be no reason to believe that PCGU could create a
more biased model than simply finetuning on many
bigoted examples.
Due to the problems with StereoSet and CrowS
alluded to in Section 6, we recognize that experi-
mental results based on those metrics are not con-
clusive evidence that a model is biased or unbiased
(or good at modeling). We urge any reader to make
their own judgment about these models through
their own qualitative analyses.
","bias, harmfulness",,"bias, harmfulness",2023,"concerns, advice, suggestions"
975,"Our dataset builds on the NLU,, dataset
(Casanueva et al., 2022) which was collected by
removing any personal information. All the names
in the dataset are also collected by randomly com-
bining names and surnames from the list of the top
10K names from the US registry.
Our data collection process was thoroughly re-
viewed by the School of Informatics, University
of Edinburgh under the number 2019/59295. Our
translators are on legal contracts with the respective
translation agencies (See Appendix C) for details.
Although we have carefully vetted our datasets
to exclude problematic examples, the larger prob-
lem of unethical uses and unfairness in conversa-
tional systems cannot be neglected (Dinan et al.,
2021). Our work also uses multilingual language
models that are shown to harm marginalised pop-
ulations (Kaneko et al., 2022). Our dataset is pub-
licly available under Creative Commons Attribu-
tion 4.0 International (CC-BY-4.0).","fairness, harmfulness",,"fairness, harmfulness",2023,"statement, actions"
976,"This work uses datasets, models, and metrics that
are publicly available. Although the scope of this
work does not allow us to have an in-depth dis-
cussion of biases associated with metrics (Am-
rhein et al., 2022), we caution the readers of draw-
backs of metrics that cause unfair evaluation to
marginalised subpopulations which are discovered
or yet to be discovered. We will release the transla-
tions, metrics scores, and corresponding task out-
puts for reproducibility.
9 ",bias,,bias,2023,"statement, concerns, advice"
977,"In this work we are performing experiments on sev-
eral low-resource African languages. Our intent is
to learn from this language diversity, and contribute
towards a stronger digital presence for these lan-
guages. This can be viewed as giving people stuff
they have not asked for, as we do not know to what
degree this is a felt need among the actual language
communities. But we also consider all languages
to be worth studying and learning from, whether
or not this study is of immediate experienced ben-
efit to the language users or not. We are therefore
thankful to the organizers for allowing us to work
on these languages, and we do not assume that our
work is of direct benefit to others than ourselves.
We have only conducted work that we ourselves
appreciate, when others conduct similar work on
our own not-so-highly resourced native language.",no ethical concerns,,no ethical concerns,2023,statement
978,"Dataset Biases The domain-general pseudo
phrases were produced based on public web-scale
data (Wikipedia), and it mainly represents the cul-
ture of the Englishspeaking populace. Political or
gender biases may also exist in the dataset, and
models trained on these datasets may propagate
these biases. Additionally, the pretrained BART
models can carry biases from the data it was pre-
trained on.
Environmental Cost The experiments describedin the paper primarily make use of V100 GPUs.
We typically used four GPUs per experiment, and
the first-stage pretraining may take up to four days.
The backbone model BART- LARGE 400 million
parameters. While our work required extensive ex-
periments, future work and applications can draw
upon ourinsights and need not repeat these compar-
isons.
","bias, environmental_impact",,"bias, environmental_impact",2023,concerns
979,"The data collection process, described in Sec-
tion 3.2, and the projected or otherwise possible
applications of our data have been approved by
the ethics committees of the institutions funding
and hosting this research, and they conform to the
ACL Ethics Policy. The language consultants who
have provided their introspective judgments have
been compensated in accordance with the laws in
place in the UK and in Germany. The database only
contains anonymized consultant IDs, and our con-
sultants have been offered the option of remaining
anonymous, or of being authors on or being ac-
knowledged by name in relevant publications—the
latter two options being relevant for the academic
recognition of some of our consultants, who are
also professional linguists.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
980,"Our system will help facilitate the creation/re-
creation of lyrics for song composers. In addition,
although our system is implemented in the direction
of English-to-Chinese, the controlling aspects and
approaches are universal because we did not take
any language-speciﬁc aspects into account; hence
can be easily implemented in other language pairs.
Besides, the method and system discussed in this
paper are suitable for creating/re-creating singable
song lyrics in languages beyond the original ver-
sion. They also have the potential to beneﬁt lan-
guage learning by translating domestic languages
into other languages the learner is studying and
facilitating learning by singing.
This methodology has limitations by putting the
singability into priority. Translations from this sys-
tem may sometimes not convey the exact meaning
of the lyrics in the source language, causing misun-
derstanding in this case. For cases where conveying
the original meaning is crucial, e.g., advertising and
serious art songs, the translation outputs need to be
checked and revised when necessary by the user
before further usage.
For the training and evaluation of our system,
all data is publicly available online. Speciﬁcally,
Chinese Lyric Corpus11is a public GitHub repos-
itory with an MIT license. Lyricstranslate.com is
a lyric translation sharing platform, where all par-
allel lyrics we obtained are publicly available in
this website. We adhere to the rules speciﬁed in
the website’s robots.txt ﬁle when crawling. For all
existing scientiﬁc artifacts used in this research, in-
cluding datasets, models, and code, we ensure they
are used in their original intended usage. For hu-
man evaluation, we collect evaluation scores with-
out personal identiﬁers for subjective evaluation
to ensure a fair comparison. We ensure that the
questionnaire does not contain any offensive con-
tent. Please refer to Appendix Efor more details
of subjective evaluation.",misunderstandings,,misunderstandings,2023,"concerns, advice"
981,"This paper aims to compress language models us-
ing knowledge distillation methods, and the pro-
posed method do not raise ethical problems or po-
tential biases. All language models, baselines, and
datasets used in this work are publicly available
and widely used.
",no ethical concerns,,no ethical concerns,2023,statement
982,"In this work, we propose a new MMRE framework
that captures deeper correlations and fuses helpful
visual information to benchmark our architecture
with baseline architectures on the MNRE dataset.
bias. Our framework is designed for multi-
modal relation extraction for Twitter data. How-
ever, when applied to data with vastly different
distributions or in new domains, the model’s per-
formance may be biased. The results reported in
the experiment section are based on specific bench-
mark datasets, which may be affected by these bi-
ases. Therefore, caution should be taken when
evaluating the generalizability and fairness.
Computing Cost/Emission. Our research, which
entails the utilization of large language models, ne-
cessitates a significant computational burden. We
recognize that this computational burden results in
a environmental_impact in terms of carbon
emissions. Specifically, our work required a cu-
mulative 425 GPU hours of computation utilizing
Tesla V100 GPUs. The total emissions generated
by this computational process are estimated to be
47.18 kg of CO 2per run, with a total of two runs
being performed.

","environmental_impact, bias, fairness",,"environmental_impact, bias",2023,"concerns, statement"
983,"In this work, we propose a method to improve LLM
performance on the important and fundamental task
of relation extraction. We do not anticipate any
ethical issues regarding the topics of this research.
",no ethical concerns,,no ethical concerns,2023,statement
984,"We believe this work has a broader impact outside
the task and datasets in the discussion. The stud-
ied textual bias problem in our motivating anal-
ysis and the potential of training a multimodal
model with weakly-supervised labels from text-
established models are not restricted to a specific
task. Also, it becomes common in the NLP domain
that some tasks first established based on pure text
input are expected to further include the consid-
eration multimodal input. The discussion in this
work can be generalized to a lot of other application
scenarios. The proposed solutions for multimodal
integration and modality bias mitigation are inde-
pendent of model architecture, which we expect
can be applied to other downstream tasks or inspire
designs with similar needs.
Regarding the human annotation involved in this
work, we create three benchmark datasets that are
manually labeled by human laborers to facilitate the
source-aware evaluation. The annotation includes
both gold attribute value as well as label sources,
i.e., image or text. The profiles and images are all
collected based on the publicly accessible Amazon
shopping website. We depend on internal quality-
assured annotators with balanced demographic and
geographic characteristics, who consent and are
paid adequately based in the US. The data collec-
tion protocol is approved by the ethics review board.
We attach detailed human annotation instructions
and usage explanations provided to the annotators
in Appendix F for reference.
",bias,,bias,2023,"statement, actions"
985,"We use publicly available source documents from
existing general datasets for annotations, so the
ethics issues of the source texts are non-existent.
For the generated contents with LLMs, e.g. GPT-
3, prior work (Brown et al., 2020; Chan, 2022)
has elaborated on their inevitable potential toxic-
ity, such as issues of bias and fairness. Moreover,
this is the first work to apply the chain-of-thought
technique to open-end generation tasks, so we com-
pletely keep the prompts neutral and task-specific
to avoid toxic language generation, and there were
no toxic texts that appeared in our experiments.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
986,"All datasets used in our work are from public
sources, which do not consist private information.
We strictly followed the data usage policy. Any
research based on our work must sign an ethical
statement and ensure that they do not infer user
privacy from it.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
987,"We declare that all authors of this work com-
ply with the ACL Ethics Policy as published
inhttps://www.aclweb.org/portal/content/
acl-code-ethics .
",no ethical concerns,,no ethical concerns,2023,statement
988,"We conduct the experiments on the public dataset,
which is exclusively about E-commerce and does
not contain any information that names or uniquely
identifies individual people or offensive content.
Therefore, we ensure that our paper conforms to
the ethics review guidelines.
",no ethical concerns,,no ethical concerns,2023,statement
989,"We conduct the experiments on public datasets,
which are exclusively about natural images, videos,
and captions. These datasets have been carefully
pre-processed for the academic study purpose, and
therefore do not contain any information that names
or uniquely identifies individual people or offensive
content. It is noteworthy that our approach inher-
its the drawback of the pre-trained backbone, i.e.,
CLIP, which has demonstrated that improper class
design used for prompting may raise unwanted
biases (Radford et al., 2021). Therefore, careful
examination is needed before employing our ap-
proach in real-world scenarios to avoid prejudices.
",bias,,bias,2023,"statement, advice"
990,"The authors declare that they have no conﬂict of in-
terest. This paper introduces a novel method namedDDG to overcome the bias issue in Visual Question
Answering (VQA). Mitigating the biases can impel
the VQA models to adopt real reasoning ability to
answer the questions, instead of using the captured
biases. Hence, this research can promote the de-
velopment of the AI robot, e.g., dialogue robots,
and facilitate people’s daily lives. The failure ofthe debiased technique may result in the collapse
of the VQA system in environments that have seen
less or even never seen. Moreover, we evaluate our
DDG on the benchmark out-of-distribution (OOD)
dataset and demonstrate the remarkable debiased
ability.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
991,"In the proposition-level entailment task ( T2), the
inference of the entailment relation between a
premise document and a hypothesis proposition
uses the assumption that the premise document is
true. The assumption is common to NLI datasets
(Dagan et al., 2005; Bowman et al., 2015; Williams
et al., 2018), and is necessary for the task’s struc-
ture. With the documents in PROPSEGM ENT, we
make the assumption only for the experimental pur-
pose ofT2, and make no claim about the actual
veracity of the premise documents.
",inaccuracies,,inaccuracies,2023,statement
992,"Although our method improves accessibility for
IR systems, it is essential to evaluate whether the
proposed approach might introduce biases or un-
fairness in the retrieval results. As our work lacks
extensive error analysis, we cannot entirely rule out
the possibility that distilled query encoders may
discard certain hard-to-process cases critical for en-
suring fairness across various query topics and user
groups. A comprehensive error analysis would be
beneficial in future research to identify and address
potential biases in the distilled query encoders, ulti-
mately fostering fair and unbiased retrieval results
for all users.
",fairness,,"bias, unfairness",2023,concerns
993,"The data collection protocol was reviewed by an
ethics panel to remove potential ethical concerns. A
few ethical concerns were mentioned by the panel
which were then judged to be handled well. These
included ensuring that the entities, texts and REs
were free from biased and sensitive language. We
address this by ﬁltering using a list of sensitive
words (see Section 3.4.1 and Table 12). The panel
also recommended a diverse representation of en-
tities and domains. Thus our data comes from
diverse domains and the entities are sampled from
a large set of Wikipedia articles.
Still, we note that the limitations mentioned in
Section 8 need to be considered and addressed care-
fully when using our dataset or models for evalua-
tion or training of a deployed system. In addition,
a biased corpus may lead to an evaluation that is
unaware of RE language forms used in other cul-
tures and languages, or that refer to other types of
items. We expect this consideration to be importantin practical settings.
",bias,,bias,2023,"concerns, advice"
994,"In our paper, we focus on the problem of forecast-
ing conversation derailment. The practical employ-
ment of any such system on online platforms has
potential positive impact, but several things would
be important to first consider, including whether
forecasting is fair (Williamson and Menon, 2019),
how to inform users about the forecasting (in ad-
vance, and when the forecasting affects users), and
finally what other action is taken when derailment
is forecast. Please refer to (Kiritchenko et al., 2020)
for a related overview of such considerations, in
the context of abusive language detection.
",misuse,,misuse,2023,concerns
995,"Pretraining language models is a very compute-
heavy process and thus leaves a large carbon foot-
print (Strubell et al., 2019; Patterson et al., 2021).
Our method makes significantly reduces the com-
pute requirements and thus the carbon footprint.
As with any large LMthere is the risk of it pro-
ducing biased or unfair output. Researchers using
the model should put into place respective safe-
guards to identify biased and/or toxic language.
","bias, toxic content, carbon footprint",,"bias, toxic content, carbon footprint",2023,"concerns, actions, advice"
996,"We bring up two main ethical considerations. First,
this empirical study uses a large language model re-
quiring a considerable amount of computations (at
most 1,500 GPU hours), which is not without envi-
ronmental consequences. However, since this study
aims at making training more efficient, it will help
reduce energy consumption in the future. More-
over, this study focuses on accuracy as a measure of
performance, which can hide pervasive effects on
under-represented marginalized groups. However,
since our method is about evaluating importance of
training examples over train steps, it can lead to im-
proving techniques to decrease bias in the training
process, particularly when marginalized groups are
under-represented in the data and therefore chal-
lenging to identify accurately.
",environmental_impact,,environmental_impact,2023,"concerns, suggestions"
997,"As discussed in Strubell et al. (2019), recent neural
models require substantial energy consumption. To
address this issue, we explore a parameter efficient
way for Transformers in this study. We believe that
our proposed strategies are effective to reduce the
energy consumption.
On the other hand, we spent a large amount of
computational costs to investigate the usefulness of
our proposed strategies in various situations. Ap-
pendix B indicates our used GPUs and the number
of updates that correspond to the computational
costs.
",computational cost,,computational cost,2023,"statement, actions"
998,"As discussed in Strubell et al. (2019), recent neural
models require substantial energy consumption. To
address this issue, we explore a parameter efficient
way for Transformers in this study. We believe that
our proposed strategies are effective to reduce the
energy consumption.
On the other hand, we spent a large amount of
computational costs to investigate the usefulness of
our proposed strategies in various situations. Ap-
pendix B indicates our used GPUs and the number
of updates that correspond to the computational
costs.
",computational cost,,computational cost,2023,"statement, actions"
999,"Our study is based on publicly available datasets
from reputable sources. The augmented datasets
will be made available with open-source code re-
lease. The fine-grained emotional paraphraser ob-
tained through our study is based on existing pre-trained language models and paraphrase datasets;
therefore, it may inherit their drawbacks such as
undesirable social biases. As an unintended use,
the methods proposed by this paper can be utilized
or modified to produce paraphrasers that increase
the emotional intensities of texts, leading to texts
with extreme emotions that can be potentially harm-
ful. While we advocate for voluntary adoption of
emotion moderation to achieve more peaceful cy-
berspaces, we do realize that the proposed methods
can be abused as emotion moderation tools for cen-
sorship. We strongly oppose such applications.
","bias, harmful_outputs, misuse",,"bias, harmul content, misuse",2023,"concerns, advice"
1000,"In this paper, we present a new partially-annotated
dataset. In adherence with the ACL code of con-
duct and recommendations laid out in Bender and
Friedman (2018) it is appropriate to include a data
statement. Our dataset is completely novel, and
was collected specifically to support the develop-
ment of natural language systems. Workers who
are proficient in the EN-US variant of English were
hired through a vendor with a competitive hourly
rate compared to the industry standard for language
consultants. For NATCSSPOKE, these workers spoke
to each other and then transcribed the data. For
NATCSSELFthese workers wrote the conversations.To annotate the data, we used two pools of anno-
tators. Both had formal training in linguistics and
were proficient in the EN-US variant of English.
One pool was hired through a vendor with a com-
petitive hourly rate. The other pool consisted of
full-time employees.
Curation Rationale Our dataset includes all of
the data that was produced by the consultants we
hired. Quality Assurance was done on a subset of
this data. We hope that any concerns would have
shown up in this sample. We annotated a random
subset of the full dataset.
Language Variety The dataset is EN-US. The
speakers (or writers) were all fluent speakers of
EN-US. We did not target a particular sub-type of
theEN-US language variety.
Speaker Demographics We do not have detailed
speaker demographics, however, we do have male
and female speakers from a variety of age ranges.
Annotator Demographics We do not have de-
tailed annotator demographics, however, we do
have male and female speakers from a variety of
age ranges. All annotators had at least some for-
mal linguistics training (ranging from a B.A. to a
Ph.D.).
Speech Situation For NATCSSPOKE, speakers
were talking in real time on the phone to one an-
other. It was semi-scripted. Speakers were not
told exactly what to say, but were given some con-
straints.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1001,"Retrieval systems have the potential to mitigate
serious problems caused by language models, like
factual inaccuracies. However, retrieval failures
may lead to undesirable behavior of downstream
models, like wrong answers in QA or incorrect
generations for other tasks. Also, since retrieval
models are based on pretrained language models,
they may suffer from similar biases.
","bias, model inaccuracies, inefficiency",,"bias, model inaccuracies, inefficiency",2023,"statement, concerns"
1002,"Our human attention signals are yielded by EZ-
reader, not collected from humans. The purpose of
using human attention signals is to mitigate short-
cut learning in LLM-based task-specific models so
as to improve their generalization on OOD samples.
All datasets used in our experiments are public
datasets.
",no ethical concerns,,no ethical concerns,2023,statement
1003,"Our proposed system aims to provide medical ser-
vices, such as diagnosis and prescription, for pa-
tients who suffer from certain diseases. All datasets
have been anonymized when released in dataset
papers. However, since we train the model with
limited and incomplete samples in two datasets,the generated responses may involve misleading
information about diagnosis, treatment, and precau-
tions. We recommend that users adopt the system
as an auxiliary tool and go to the hospital for help
if necessary. Besides, when interacting with the
system, there is a risk of sensitive information leak
(e.g., gender as reported by users). It can be ad-
dressed by adopting anonymous technology in the
future. Thus, we strongly advise users to consider
the ethical implications of the generated responses
carefully. Furthermore, the scientific artifacts that
we used are freely available for research, including
NLTK, ROUGE, Transformers, and other GitHub
codes. And this paper’s use of these artifacts is
consistent with their intended use.
Acknowledgment
This work was supported by the Research Grants
Council of Hong Kong (15207920, 15207821,
15207122) and National Natural Science Founda-
tion of China (62076212).
","privacy concerns, model inaccuracies",,"privacy concerns, model inaccuracies",2023,"concerns, advice"
1004,"Goal-directed dialogue systems can be used for cre-
ating non-obtrusive recommendations for specific
products and services, introducing interesting new
topics and educating users about those topics, and
so forth. Developing such systems requires careful
consideration since it has a broad impact on appli-
cations. The intention of our work is not to force
the system to reach the designated target nor force
users to accept recommendations. Instead, we aim
to build better assistive technologies to improve theproactiveness of dialogue systems. Furthermore,
our experimental datasets are publicly available.
They have been filtered for sensitive and private
information during dataset construction.
We hope to raise awareness of the potential for
misuse of such systems with toxic intentions. For
example, such systems may be used to pose as hu-
mans and actively manipulate users’ perceptions
on specific issues or political inclinations. To miti-
gate these risks, we emphasize the importance of
improving transparency through regulations. It is
essential to inform users that they are conversing
with a bot instead of a human, and regulations on
target designation are crucial when deploying these
systems in specific domains. It is necessary to en-
sure that setting a target does not violate factual
accuracy, user privacy rules, or human laws.
",misuse,,misuse,2023,"concerns, actions, advice"
1005,"TheIU X- RAY(Demner-Fushman et al., 2016) and
MIMIC-CXR (Johnson et al., 2019) datasets have
been automatically de-identified to protect patient
privacy. The proposed system is intended to gen-
erate radiology reports automatically, alleviating
the workload of radiologists. However, we notice
that the proposed system can generate false positive
observations and inaccurate diagnoses due to sys-
tematic biases. If the system, as deployed, would
learn from further user input (i.e., patients’ radio-
graphs), there are risks of personal information
leakage while interacting with the system. This
might be mitigated by using anonymous technol-
ogy to protect privacy. Thus, we urge users to
cautiously examine the ethical implications of the
generated output in real-world applications.
Acknolwedgments
This work was supported in part by General Pro-
gram of National Natural Science Foundation of
China (Grant No. 82272086, 62076212), Guang-
dong Provincial Department of Education (Grant
No. 2020ZDZX3043), Shenzhen Natural Sci-
ence Fund (JCYJ20200109140820699 and the Sta-
ble Support Plan Program 20200925174052004),
and the Research Grants Council of Hong Kong
(15207920, 15207821, 15207122).
","model inaccuracies, privacy concerns, bias",,"model inaccuracies, privacy concerns, bias",2023,"concerns, advice"
1006,"This work presents CKDST, a knowledge distilla-
tion framework for ST to more comprehensively
and effectively distill knowledge from MT to im-
prove the performance of E2E ST. The datasets
used in this study include both MuST-C and WMT.
They are all public datasets and are widely used in
the MT community.
",no ethical concerns,,no ethical concerns,2023,statement
1007,"We strictly follow the data use agreements of each
public online social platform and double-check to
ensure that there is no data relating to user privacy.
The opinions and findings contained in the samples
of our presented dataset should not be interpreted as
representing the views expressed or implied by the
authors. We hope that the benefits of our proposed
resources outweigh their risks. All resources are
for scientific research only.
Acknowledgment
This research is supported by the Natural Science
Foundation of China (No. 62076046, 62006034).
We would like to thank all reviewers for their con-
structive comments.
",no ethical concerns,,no ethical concerns,2023,statement
1008,"The current study does not involve any human
subjects and we do not foresee any ethical con-sequences.
",no ethical concerns,,no ethical concerns,2023,statement
1009,"Our work aims at solving the ethical issue of ad-
dressing misinformation in automated text genera-
tion tasks. Yet, adopting automatic summarization
by real users can amplify misinformation in cases
where the model still makes an error or when the
input text itself is not trustworthy. As we stated
in the limitations, our trained models heavily rely
on other predictive models and therefore carry the
biases of their training data, and may implicitly en-
code these into our generative process. Therefore,
we believe that to reach real-world use, not just
our method should be scrutinized but also the NLI
and summarization datasets that were used to train
these models. Thus, such methods should be used
with caution and combined with other techniques to
ensure humans are capable of judging the validity
of the information generated by the model.","bias, misinformation",,"bias, misinformation",2023,"concerns, advice"
1010,"Public trust plays a significant role in the broad
applicability of NLP in real-world scenarios, espe-
cially in critical situations that may directly impact
people’s lives. NLP research of the kind presented
in this paper helps this issue take the spotlight it
deserves. However, we discourage NLP practition-
ers from using our method (and similar methods)
as an out-of-the-shelf solution in deployed systems.
We recommend investing a significant amount of
time and effort in understanding the applicability
and universality of our method to the debiasing of
representations. Issues such as expected type of
adversariality or tolerance level for drop in system
performance need to be considered.
",system performance,,system performance,2023,"concerns, advice"
1011,"The AM algorithm could potentially be misused by, rather than using the AM steps to erase infor-
mation, using them to link records of two differ-
ent types, undermining the privacy of the record
holders. Such a situation may merit additional
concern because the links returned between the
guarded attributes and the input instances will
likely contain mistakes. The links are unreliable
for decision-making at the individual level . In-
stead, they should be used on an aggregate as
a statistical construct to erase information from
the input representations. Finally,
we note that
the automation of the debiasing process, withoutproperly statistically confirming its accuracy us-
ing a correct sample, may promote a false sense
of security that a given system is making fair de-
cisions. We do not recommend using our method
for debiasing without proper statistical control
and empirical verification of correctness.
","misuse, privacy, unreliability, fairness",,"misuse, privacy, unreliability, fairness",2023,"concerns, advice"
1012,"We do not ﬁnd any ethical considerations stemming
from this work. Quite the contrary, we believe that
disentangling knowledge sources to encourage the
statements that an LM generates to be attributable
(Rashkin et al., 2021) can have a positive effect on
the ability to avoid unwanted artifacts (that may
otherwise be toxic or harmful).
",no ethical concerns,,no ethical concerns,2023,statement
1013,"Work was approved by Princeton University’s IRB
under Proposal 15346. No deception was used in
the experiment and we screened language model
responses for any sensitive content before passing
them to the experts (although we did not encounter
any). Participants were not compensated for par-
ticipation and gave consent to be identified. All
appropriate IRB protocols in providing instructions
and gathering consent were followed.
",no ethical concerns,,no ethical concerns,2023,statement
1014,"The dataset is based on RPG game in fantasy
world with diverse scenarios, including wars. To
match the story background, a model trained on
this dataset might produce war-related words. We
manually looked into each example to meanwhile
keep each speaker’s personality and remove utter-
ances that could potentially cause negative impact,
such as violence, bias, and offensive words.
For the data annotation part and human evalua-
tion part, we utilized the Amazon Mechanical Turk
platform and required workers to have a HIT Ap-
proval Rate of greater than 95% and be located in
CA or the US. We pay the annotators over 16 US
dollars per hour on average, which is above the
highest state minimum wage. Given our setting,
the workers understood the scenarios and agreed
that their annotations will be used for research. The
data annotation part of the project is classified as
exempt by Human Subject Committee via IRB pro-
tocols.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1015,"The present study was conducted in accordance
with ethical principles. This study involved the
analysis using publicly available data and knowl-
edge sources (e.g., Wikipedia). Thus, this work did
not involve any human participants and potential
risks regarding credentials or privacy. Therefore,
no ethical clearance was required and there were
no potential risks associated with the conduct of
this research.
",no ethical concerns,,no ethical concerns,2023,statement
1016,"In the short term, progress toward better natural
language inference does not appear to lead to sig-
nificant social risks in its broader impacts. While
“underclaiming” progress in natural language pro-
cessing tasks (e.g. exaggerating the scope or sever-
ity of failures of specific models on specific tasks)
(Bowman, 2021) may be enabled by this work in
the future, our focus on directly quantifiable and ob-
servable single sentence leakage , use of SOTA-like
models (fine-tuned transformers) for analysis, and
our side-by-side comparison of our model imple-
mentations with SOTA all ensure that our criticisms
of current NLI benchmarks are well-founded. All
data and tools we utilized were freely distributed
for unlimited research use in the academic context.
",no ethical concerns,,no ethical concerns,2023,statement
1017,"Images of human faces are generated by our model.
To mitigate the minor risk of resemblance to real
people, we have downsampled all images. How-
ever, we believe this risk is mitigated by the lack
of personal names in the querying data. Further-
more, we believe demonstrating that human faces
are generated and under which conditions they are
is important for documentation of bias (Paullada
et al., 2021) and harm risks in these models.
License information is provided in our project
page ( saxon.me/coco-crola ) and project reposi-
tory ( github:michaelsaxon/CoCoCroLa ).
","privacy concerns, bias, harm risks",,"privacy concerns, bias, harm risks",2023,"concerns, actions"
1018,"We are fully aware of the potential dangers that text
generation techniques may present, such as gener-
ating fake, toxic, or offensive content. However,
controllable text generation technology is a pow-
erful weapon against harmful information hidden
in pre-trained language models, where our study
includes text detoxification specifically. We be-
lieve it is beneficial to carry forward research on
controllable text generation.
","toxic content, offensive content, fake content",,"toxic content, offensive content, fake content",2023,"concerns, statement"
1019,"This research aims to highlight the extent to which
gender biases may be present in Language & Vision, with an emphasis on the representation of females in sports. The ﬁndings of the study may have implications for the ways in which images
of female athletes and sports ﬁgures are portrayed
and treated within the datasets and by models. The
results of the study may have the potential to con-
tribute to the ongoing efforts to address gender bias
in all areas of life, as we maintain that gender bias
is an issue within Machine Learning, because it is
an issue within human society. The research team
is committed to using the ﬁndings of the study to
foster dialogue and understanding of the issue and
to advocate for equitable treatment of all groups
throughout the production pipeline.
",gender bias,,gender bias,2023,"actions, statement"
1020,"All data used in this study were collected from
publicly available sources, with no infringement of
intellectual property rights or privacy.
",no ethical concerns,,no ethical concerns,2023,"actions, statement"
1021,"We present the following ethical considerations for
data authorization, privacy, and deployments.
•We have obtained explicit permissions from
the customer to collect and utilize customer
service dialog data.
•We use desensitization tools to remove sensi-
tive information in customer service conversa-
tions. Only a few annotators can access this
data, and they will check again to ensure that
there is no user personal information in the
dataset. At the same time, the dataset ant-
qaconv is only used for internal research.
•The QA pairs produced by the knowledge pro-
duction system will be checked by human op-
erators to make sure private message is re-
moved.
",no ethical concerns,,no ethical concerns,2023,actions
1022,"This work proposes to release a new dataset to
research community for improving compositional
generalization in semantic parsing. The goal of
the research topic is to make model work better
on underrepresented language without relying on
large training data and therefore long training time.
The models proposed in this paper are very small
in today’s standard and do not require large com-
puting resources for training and evaluation. We
particularly pay attention to efficiency of models to
save computing resource after deployment.
",no ethical concerns,,no ethical concerns,2023,"actions, statement"
1023,"The authors have evaluated the potential conse-
quences of their research, including both positive
and negative effects. Furthermore, the authors have
ensured compliance with the guidelines outlined in
the ACM Code of Ethics and Professional Conduct,
and confirm that this work is in accordance with
those principles.
",no ethical concerns,,no ethical concerns,2023,statement
1024,"LLMs are no longer just a laboratory curiosity;
they are being used in real-world systems to in-
teract with real-world environments (both digital
and physical). To ensure successful deployment of
LLMs in these scenarios, it is essential to improve
their controllability, as failure to do so could lead to
catastrophic results. In digital environments, such
as databases, unexpected behavior could lead to
safety issues with a company’s data and property.
In physical environments, it could even put human
life at risk. Pangu is proposed to provide better
controllability for LLMs when being depolyed to
interact with different environments. Specifically,
safety considerations can be explicitly incorporated
into the agent’s candidate proposal (the symbolic
part of Pangu) for enhanced security ( i.e., harmful
actions are directly excluded from the candidates
pool).
","privacy, safety",,"data privacy, safety issues, human life",2023,"concerns, actions"
1025,"All experiments are conducted on English datasets
only, so the generalizability toward other languages
is not verified. Besides, as the current automatic
evaluation metrics, including ours, are imperfect,
they may introduce unintended favor toward a cer-
tain type of responses. Future research should fo-
cus on detecting and mitigating such undesirable
biases of learnable metrics.
",bias,,bias,2023,statement
1026,"We obtained all data from cited sources. Our exper-
imental procedures and analysis do not involve hu-
man participants and are in compliance with ACL
Code of Ethics.",no ethical concerns,,no ethical concerns,2023,statement
1027,"Given that we do not resort to using language mod-
els nor to human evaluation with people who are
not authors of this paper, our work has no ethics
implication that we are aware of.
",no ethical concerns,,no ethical concerns,2023,actions
1028,"TRAVLR is a synthetically generated dataset, and
hence poses no ethical issues relating to the source
of the data. Its use of abstract shapes has the further
advantage of avoiding biases relating to human
subjects.
",no ethical concerns,,no ethical concerns,2023,statement
1029,"OurReCode robustness benchmark aims to provide
a comprehensive robustness evaluation framework
for any code-generation models, which we believe
is critical towards building robust and user-friendly
language models for code. With the new robustness
evaluation metrics, users can rely on ReCode and as-
sess model predictions with more confidence. The
model trainers, on the other hand, will be aware of
the potential vulnerabilities that might cause mis-
predictions in practice and mitigate them before
deployments. Therefore, we believe our ReCode
benchmark is beneficial in terms of broader impact.
",mispredictions,,mispredictions,2023,statement
1030,"When we analyze social media content, we may
have concerns regarding individual privacy or
certain ethical considerations. These concerns
appear due to the usage of information that could
be sensitive and personal (e.g., references to
emotions and health concerns). It is also important
to mention that these datasets could contain biases
belonging to the nature of social media data, e.g.,
a gender, age, or sexual orientation profile that
could cause someone to be mislabeled as having
a mental disorder. The experiments and usage
of this data are for research and analysis only,
and the misuse or mishandling of information
is prohibited. In any case, the datasets we
employed (corpus by Kim (Sect 3.1), Reddit
datasets (Sect 3.2), lexicon data set (Sect 3.3),
and eRisk collections (Sect 4.1) are publicly
available and we strictly followed the terms of use
and user agreement of these collections (see e.g.
https://tec.citius.usc.es/ir/code/eRisk2019.html).
Furthermore, these collections are anonymized and
our research does not involve any contact with
social media users. Under such conditions, this
research does not require review and approval by
the Ethics Committee Board.
","privacy, bias",,"privacy, bias",2023,"concerns, actions"
1031,"Reliability of Attribution Methods The plausi-
bility and faithfulness of attribution methods sup-
ported by Inseq is an active matter of debate in
the research community, without clear-cut guaran-
tees in identifying specific model behaviors, and
prone to users’ own biases (Jacovi and Goldberg,
2020). We emphasize that explanations produced
with Inseq should notbe adopted in high-risk and
user-facing contexts. We encourage Inseq users to
critically approach results obtained from our toolkit
and validate them on a case-by-case basis.
Technical
","misuse, bias",,"misuse, bias",2023,"concerns, suggestions"
1032,"While predicting film genre may seem similar to
the task of age audience rating recommendation
(e.g. MPAA film rating system), as some genres
such as family, adventure, fantasy, animation, hor-
ror, or crime often have a similar rating, using this
(a)λ << 1overfitting
 (b)λ >> 1underfitting
 (c) Dynamic λbalance
Figure 3: Visualization of the effect of λin UDA
Dataset Model Modality Base Dynamic UDA Dynamic UDA Text Dynamic UDA Image
MoviescopeMMBTImage 29.2 ±0.8/29.0 ±3.7 33.8 ±2.7/32.9 ±3.7 35.5 ±1.7/37.8 ±2.2 34.7 ±1.6/37.0 ±3.9
Text 71.4 ±0.3/75.6 ±0.4 72.1 ±0.4/75.7 ±0.7 72.1 ±0.5/75.3 ±1.4 72.6 ±0.5/76.4 ±0.6
Image-Text 72.1±0.5/76.1±0.4 73.6±0.2/77.4±0.3 74.3±0.3/78.1±0.4 73.8±0.1/77.9±0.3
MMBVImage 43.4 ±0.9/50.3 ±2.7 44.3 ±1.8/52.2 ±2.0 45.6 ±2.4/53.4 ±2.0 43.1 ±1.5/50.2 ±2.8
Text 62.3 ±4.6/54.7 ±4.4 65.6 ±1.9/62.5 ±5.1 68.8 ±1.2/65.3 ±3.8 67.4 ±1.8/65.1 ±5.3
Image-Text 74.0 ±0.2/77.9 ±0.4 75.5±0.3/79.5±0.3 75.3±0.8/79.2±0.5 75.2±0.4/79.1±0.3
MM-IMDbMMBTImage 16.5 ±1.9/29.4 ±1.0 16.7 ±1.5/32.9 ±1.0 22.5 ±1.9/35.7 ±1.8 18.6 ±0.5/33.6 ±1.59
Text 56.6 ±0.7/62.8 ±1.4 59.9 ±0.3/65.3 ±0.5 58.0 ±0.6/64.3 ±1.5 59.2 ±0.5/65.2 ±0.8
Image-Text 61.4±0.3/66.6±0.3 62.0±0.2/67.5±0.2 62.2±0.2/67.6±0.2 62.5±0.2/67.8±0.1
MMBVImage 22.8 ±2.6/39.4 ±1.8 21.9 ±1.1/39.9 ±1.4 23.6 ±2.9/39.9 ±2.1 22.6 ±2.5/38.9 ±1.4
Text 36.2 ±9.7/45.0 ±8.2 47.8 ±3.3/55.5 ±2.6 42.1 ±4.1/49.0 ±3.8 43.8 ±6.8/52.2 ±5.3
Image-Text 62.7±0.3/67.9±0.3 62.7±0.1/68.2±0.2 62.7±0.3/67.9±0.3 62.8±0.2/68.0±0.2
Table 4: Multimodal models evaluated over only one modality. Moviescope is Macro-AP/Micro-AP. MM-IMDb
is Macro-F1/Micro-F1
(a) MMBT
 (b) MMBV
Figure 4: A comparison between the base model and
Dynamic UDA framework during training. The gap be-
tween training and validation is smaller with Dynamic
UDA, as it takes more epochs to converge to the solu-
tion in comparison with the base model.
Figure 5: Effect of the ratio R(equation 10) according
to the cases in Table 1. Case one is marked with red
color, case two with yellow, and case three with green.
TheRvalue is represented in the colored dots.model to directly guide the rating task may not be
appropriate as genre and rating are not always cor-
related.
Additionally, note that the model has been
trained primarily on the Western film industry
(mainly American), and our data lacks representa-
tion from other significant industries, such as those
in Asia. However, with appropriate training data,
these models could perform well in these cultural
contexts. Additionally, we encourage the ethical
use of these models in other multimodal tasks with
sensitive contexts.",no ethical concerns,,no ethical concerns,2023,statement
1033,"Rewriting the lyrics of a song may cause potential
copyright infringement. Besides, the copyrights
of the lyrics in the dataset belong to the song writ-
ers. To protect the copyrights, our model and the
released dataset will be protected by the license,
Creative Commons Attribution-NonCommercial
(CC-BY-NC), and prohibited from commercial use.
",copyright,,copyright violation,2023,"concerns, actions"
1034,"There are no direct societal implications of this
work. The proposed method attempts to provide
high-quality and faithful event TEMPRELextrac-
tion and timeline construction. We believe that
the intellectual merits of developing robust event-
centric information extraction methods are demon-
strated by this work. For any information ex-
traction methods, real-world open source articles
used to extract information may contain societal
biases. Extracting event-event relations from ar-
ticles with such biases may spread the bias into
the acquired knowledge. Yet we believe that the
proposed method can benefit various downstream
NLP/NLU tasks like event prediction, task-oriented
dialogue systems and risk detection.
",bias,,bias,2023,statement
1035,"We believe that our work leads to a better under-
standing of the behavior of pre-trained language
models on OOD texts. We also believe that theproposed method will facilitate the reliable de-
ployment of NLU models since a model may face
various types of OOD inputs in the wild and our
method contributes to the detection performance on
unknown OOD data in the average sense. All exper-
iments in this work are conducted on open datasets
and all pre-trained models that we investigate are
publicly available. We do not anticipate any nega-
tive social consequences to our work and we hope
to continue to build on our method and develop
more effective textual OOD detectors in the future.
",no ethical concerns,,no ethical concerns,2023,statement
1036,"We declare that all authors of this paper acknowl-
edge the ACM Code of Ethics and honor the code
of conduct. This work substantially reveals po-
tential privacy vulnerabilities of current LM-based
sentence embedding models during inference. We
hereby propose the generative embedding inversion
to further exploit the weaknesses of those widely
used sentence embedding models. We hope to raise
more awareness of privacy leakage inside sentence
embeddings and call for defenses against such in-
formation leakage.

",privacy,,privacy,2023,statement
1037,"As a query embedding method, WFRE has stronger
generalizability to different query types and knowl-
edge graphs. Experiments and evaluations in this
paper involve no ethical issues and are not even
related to any human entities. WFRE could be
potentially used to efficiently infer private infor-
mation from an industrial-level knowledge graph.
This is a common potential risk for approaches
targeting data incompleteness and link prediction.
",research misuse,,research misuse,2023,statement
1038,"As our proposed framework relied on large lan-
guage models, text generation based on LLMs of-
ten contains biased or harmful contexts. We ar-
gue that our work largely mitigated the potential
risks in the following ways. First, our careful-
designed prompting leads to rather narrow genera-
tions constrained on small domains, i.e., products
in e-commerce. Second, we also had a strict data
audit process for annotated data from annotators
and populated data from trained classifiers. On
a small scale of inspections, we found none be-
longs to significant harmful contexts. The only
related concern raised here is that some generated
knowledge is irrelevant to the products themselves.
The major reason is due to imprecise product titles
written by sellers for search engine optimization,
such as adding popular keywords to attract clicks
or purchases. Our human-in-the-loop annotation
identified such cases and the trained classifier fur-
ther assisted machines in detecting bias, as we hope
our intention generations can be safe and unbiased
as much as possible.
",no ethical concerns,,no ethical concerns,2023,"concerns, actions"
1039,"In this work, we conformed to recognized privacy
practices and rigorously followed the data usage
policy. We declare that all authors of this paper
acknowledge the ACM Code of Ethics and honor
the code of conduct. This paper presents a method
to utilize the interaction information between dif-
ferent layers, inherent sense label structure, and
prior knowledge of connectives in the implicit
discourse recognition task. The PDTB 2.0 and
CoNLL-2016 dataset were used to train and assess
the ability of the pre-trained language model on this
task. The PDTB2.0 and CoNLL2016-Test dataset
is collected from the Wall Street Journal (WSJ)
articles, while the CoNLL2016-Blind dataset is de-
rived from newswire texts, the primary language is
English based and belongs to the news domain. We
can foresee no immediate social consequences or
ethical issues as we do not introduce social/ethical
bias into the model or amplify any bias from the
data. Therefore, these two datasets are not required
to perform further actions to check the offensive
content.
",no ethical concerns,,no ethical concerns,2023,statement
1040,"This paper introduces CAT, a framework for com-
monsense reasoning via conceptualizing CSKB to
acquire abstract commonsense knowledge. The
experiments are conducted on publicly available
and well-established datasets that are shared via
open-access licenses. The usage of these datasets
in our paper is only for research purposes and is
consistent with the datasets’ intended usage. The
2https://chat.openai.com/primary dataset, AbstractATOMIC, largely shares
the content with another CSKB, ATOMIC, which
is anonymized and desensitized (Sap et al., 2019a).
Thus, no data privacy issue is involved.
The potential risks of CAT are relatively low.
Since CAT is trained on AbstractATOMIC, a
conceptualization benchmark based on a popular
CSKB, ATOMIC, and two concept taxonomies,
Proabse and WordNet, it is expected that CAT does
not contain any private, offensive, biased, and sensi-
tive information or social, political issues. The stud-
ied tasks all focus on conceptualization or CSKB,
which is not likely to generate harmful content, as
shown in the case studies in Appendix E. Thus, we
believe that CAT does not yield additional risks.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1041,"This work presents COPES , a free and open dataset
for the research community to study the contex-
tualized CCR problem. Examples in COPES are
collected from ROCStories (Mostafazadeh et al.,
2016), a free and open dataset about anonymized
chains of events. Each chain logically follows ev-
eryday topics and does not involve privacy prob-
lems about any specific entities (e.g., a person or
company). We carried out human annotation on
Amazon Mechanical Turk. Annotators are fairly
paid 1.2 USD for each HIT, which fulfills the mini-
mum wage requirement.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1042,"The main ethical considerations of our research
arise from the text generation performed. The
concerns here are that both the teacher and stu-
dent model may potentially generate non-factual
(Ji et al., 2022; Pagnoni et al., 2021; Kreps et al.,
2022) or offensive output (Gehman et al., 2020).
This is largely inﬂuenced by the input data, which
is our case are standard, peer-reviewed benchmark-
ing tasks in the NLP domain.
","nonfactual content, offensive content",,"misinformation, offensive content",2023,concerns
1043,"By encapsulating the models, NLP Workbench low-
ers the entry barrier for non-experts to use state-of-
the-art AI models. The microservice architecture,
which allows models to be deployed on multiple
servers with different capabilities rather than a sin-
gle omnipotent server, also makes this text mining
platform more accessible. Containerizing third-
party models also helps with reproducibility and
transparency. There have been attempts of using
NLP Workbench to analyze datasets to help under-
stand propaganda, misinformation, and disinforma-
tion related to war and terrorism. However, users
must be warned that, as NLP Workbench uses third-
party data and models without modification, out-
puts obtained from NLP Workbench are inevitably
affected by the bias inherent in the datasets and
models.
",bias,,bias,2023,statement
1044,"While the goal of this study is for social good, an
intention classifier, if deployed, could also lead
to potential negative impacts. For example, a bi-
ased intention classifier that picks up spurious fea-
tures of certain language patterns might be more
frequently used by a subgroup of people hence
negatively impacting certain users. Our aim is to
use this in a collaborative way for willing users to
provide hints on the possibility of their questions
being perceived with a different intention. In other
words, the model can indicate if questions may be
perceived by another person as conflict-invoking;
hence the user considers rephrasing their questions
if they prefer to do so. Our goal is not to restrict free
expressions or take any actions against users, but
the opposite, which is to promote friendly discus-
sion and raise awareness of multiple interpretations
(only if the users are interested). Yet, this technol-
ogy, like others, may be misused or might be used
in a way that systematically or erroneously silences
certain social groups (Gorwa et al., 2020). One
solution might be having a threshold that can be
moderated by the users since different people have
different levels of tolerance to offense, and this also
holds for different cultures. Such aspects could be
accommodated by collecting viewpoints from dif-
ferent personalities, cultural backgrounds, genders,
or generations in order to make a more comprehen-
sive system and avoid biases. Finally, our
model does not provide any indication of where the
negative intention lies within the question, which
may confuse the users. This calls for extending the
system to boost explainability, and transparency,
also mentioned in (Chang et al., 2022). In this case,
collecting user feedback and annotator reasoning
may help identify the problems, and conducting
error analysis and training a hybrid model (rule-
based guidance on top of machine learning) may
improve the performance.","bias, misuse",,"bias, misuse",2023,"concerns, advice"
1045,"While we achieve reasonable automatic evaluation
results using the proposed models, we note that
these models pose ethical risks in two ways. From
readers’ perspective, entirely trusting machine-
generated summaries would lead to a wrong un-
derstanding of the articles, thus potentially harm-
ing the progress of the research community. Even
though we show that more than 90% of sentences
from the annotated summaries were truthful to the
input articles, the remaining sentences that were
not truthful are impactful enough to misunderstand
the contributions.
From writers’ perspective, our proposed models
could be used maliciously to appear valuable. In
a hypothetical situation where our model outputs
are regarded trustworthy enough for people to as-
sess articles, “hacking” our summarization model
to output over-claming contributions could be pos-
sible.1031
","faithfulness, misuse",,"faithfulness, misuse",2023,"concerns, advice"
1046,"Our work encourages the model to ground on the
existing knowledge populated from large textual
collections. We believe it is a reasonable towards
building more trustworthy and more robust ma-
chine learning models. Having better attributions
to knowledge source could help humans better un-
derstand the model’s rationale for decision making.
However, we do admit that the question genera-
tion models used to populate the QA knowledge
base could potentially exacerbate the biases already
present in the original Wikipedia data. We will
keep working on this direction to minimize its po-
tential negative impacts.
",bias,,bias,2023,"statement, concerns, suggestions"
1047,"All our experiments are performed over publicly
available datasets. We do not use any identifiable
information about crowd workers who provide an-
notations for these datasets. Neither do we perform
any additional annotations or human evaluations in
this work. We do not foresee any risks using CoAug
if the inputs to our model are designed as per our
procedure. However, our models may exhibit un-
wanted biases that are inherent in pre-trained lan-
guage models. This aspect is beyond the scope of
the current work.
",bias,,bias,2023,"statement, concerns"
1048,"The aim of our work was to design an adversarial
training-enabled token classiﬁcation system that is
able to correctly remove disﬂuencies in text. The
datasets used in this work are publicly available
and we have cited the sources of all the datasets
that we have used.
",no ethical concerns,,no ethical concerns,2023,statement
1049,"Instructions are a useful tool to convey extrinsic
information to large language models and alter
model outputs, e.g. by instructing models to gen-
erate less harmful content. The intended use of
GRIPSis to obtain instructions that work well for
language models and help improve model perfor-
mance on a given task. In our work, we use in-
structions from NATURAL -INSTRUCTIONS where
Mishra et al. (2022a); Wang et al. (2022) ensure
quality control. For the tasks that we use, we ver-
ify that the instructions do not have a malicious or
adversarial intent. Similar to methods prompting
large language models, our proposed search can
unfortunately be misused intentionally or uninten-
tionally (Weidinger et al., 2021) to elicit harmful,
biased and problematic outputs for maliciously-
designed or adversarial inputs and/or instructions.
Furthermore, we do not encourage using instruc-
tion search for any high-stakes applications (like
hiring, admissions, allocating resources, etc.). Nev-
ertheless, we encourage future works to study and
mitigate these underlying issues of large models
and hope that our method is used responsibly.
","bias, misuse",,"bias, misuse",2023,"statement, concerns, advice"
1050,"The human participants in our work were recruited
by an external crowd-sourcing company that en-
sured annotators provided informed consent, were
given fair compensation, and no personally identi-
fiable information ( PII) was collected or released.
We use existing publicly available meeting tran-
scripts collected by the AMI project (Carletta et al.,
2005) in controlled scenarios and filtered for of-
fensive/toxic content. We also conducted manual
inspection of a random sample from annotated tran-
scripts and did not find any toxic content or PII.
Furthermore, the collected data and experiments
are conducted in English and we do not claim gen-
eralization of our findings across languages. Given
the broad nature of meetings, the content can fall
into a number of domains, of which only a few
are represented in the AMI corpus. Therefore, we
do not expect models trained on MEETING QAto
generalize to certain domains such as judicial, ethi-cal review, congressional proceedings, etc. which
involve specific jargon and rules of engagement.
",no ethical concerns,,no ethical concerns,2023,actions
1051,"Our approach involves the use of large pretrained
language models, whose computational perfor-
mance is typically higher when deployed in more
powerful environments with GPUs. Under such
usage, electric consumption and associated carbon
footprint are likely to increase and users of our
method under these conditions should be aware
of this type of impact. However, subtitle segmen-
tation is often performed offline, where efficient
processing is less of a concern, and lower-cost CPU
deployments are an entirely viable option. All our
results were obtained with a single large LM de-
ployed on CPU, with the aim of reducing energy
consumption at inference time.
Additionally, our method requires no training
for the task at hand and thus removes the cost of
model training associated with the supervised meth-
ods with which we compare our results. For in-
stance, Papi et al. (2022) indicate that they use four
K80 GPUs to train their models, which we took as
comparison points, with 1 day of training for their
text-only models and 1 week for their multimodal
segmenters. Therefore, given the large number of
potential language pairs and domains in need of
segmented subtitle content, our approach can pro-
vide competitive results with a comparatively lesser
impact on energy resource consumption.",environmental_impact,,environmental_impact,2023,"concerns, actions"
1052,"License CrossSum is a derivative of the XL-Sum
dataset. XL-Sum has been released under the
Creative Commons Attribution-NonCommercial-
ShareAlike 4.0 International License (CC BY-NC-
SA 4.0), allowing modifications and distributions
for non-commercial research purposes. We are ad-
hering to the terms of the license and releasing
CrossSum under the same license.
Generated Text All of our models use the mT5
model as the backbone, which is pretrained on a
large multilingual text corpus. For a text gener-
ation model, even small amounts of offensive or
harmful texts in pretraining could lead to danger-
ous biases in generated text (Luccioni and Viviano,
2021). Therefore, our models can potentially gen-
erate offensive or biased content learned during
the pretraining phase, which is beyond our control.
Text summarization systems have also been shown
to generate unfaithful and factually incorrect (albeit
fluent) (Maynez et al., 2020) texts. Thus, we sug-
gest carefully examining the potential biases beforeconsidering them in any real-world deployment.
Human Evaluation Annotators were hired from
the graduates of an institute that provides profes-
sional training for many languages, including the
ones evaluated in Section 3. Each annotator was
given around 200-250 sequence pairs to evaluate.
Each annotation took an average of one and a half
minutes, with a total of approximately 5-6 hours
for annotating the whole set. Annotators were paid
hourly per the standard remuneration of bilingual
professionals in local currency.
environmental_impact A total of 25 models
were trained as part of this work. Each model was
trained for about three days on a 4-GPU Tesla P100
server. Assuming 0.08 kg/kWh carbon emission14,
less than 175kg of carbon was released into the
environment in this work, which is orders of mag-
nitude below the most computationally demanding
models.
","bias, offensive content",,"bias, offensive content",2023,"statement, concerns, actions"
1053,"Broader impact. As deep language models gain
greater prominence in both research and real-world
use cases, concerns have arisen regarding their
opaque nature (Rudin, 2019; Barredo Arrieta et al.,
2020), their tendency to perpetuate and even am-
plify social biases in the data on which they are
trained (Bolukbasi et al., 2016; Swinger et al., 2019;
Caliskan et al., 2017), and their encoding of spuri-
ous relationships between the target and irrelevant
parts of the input (Veitch et al., 2021). Particularly
given their increasing deployment in healthcare,
psychology, and social science, as we mention ear-
lier in this paper, it is crucial that these black-box
models be rendered more transparent to ensure that
decisions are being made in a principled way. In
other words, interpretability is not only an intellec-
tual goal but also an ethical one.
In service of this goal, our proposed language
representation, SENTE CON, provides clear insight
into the relationship between human-interpretable
concepts and outcomes of interest in machine learn-
ing tasks. It is able to do so without negatively
impacting predictive performance—an important
factor, since a primary motivator for using non-
interpretable language representations is their ex-
cellent performance on machine learning tasks. We
hope that this will motivate others to use SENTE -
CON, and we also hope that using SENTE CONwill
allow users to better understand how their machine
learning pipelines make decisions, evaluate theirmodels for bias, and enforce correct and robust
relationships between inputs and outputs.
Ethical considerations. This work involves the
collection of new data to assess the consistency of
SENTE CON(,)representations with human anno-
tations of the content of text passages. No infor-
mation was collected about the annotators, and the
data is not sensitive in nature. In the course of data
collection, we took measures to ensure fair com-
pensation and treatment of annotators. Annotators
were provided a description of the study and given
the option to decline the study after learning its
details, and all annotators were paid at a rate above
the local minimum wage.
SENTE CON(,)relies on pre-trained deep lan-
guage models to compute language representations.
Our use of these pre-trained models is limited to
research purposes only and is compliant with their
intended use. We acknowledge that the use of pre-
trained models introduces the possibility that SEN-
TECON(,)may encode some biases contained in
those models. As a consequence, interpretations
of the relationships between SENTE CON(,)cate-
gories and targets (when using SENTE CON(,)in
modeling) may also contain elements of bias.
",no ethical concerns,,no ethical concerns,2023,statement
1054,"This study was conducted in accordance with ethical
principles and guidelines. The study was designed
to provide beneficial knowledge and not harm any
group or individual. We recognize that the wordlist
we use might not represent all contexts of gender
bias and that our debiasing method does not cover
all contexts of occurrences of gender bias. However,
we made sure to consider the ethical implications
of our methodologies and the results of our anal-
ysis. The authors have tried to ensure the methoddoes not amplify any other inherent bias but also ac-
knowledge that our approach may have limitations.
We take responsibility for any ethical concerns that
may arise as a result of our research.
",gender bias,,gender bias,2023,concerns
1055,"Since longer sentences have more complex syn-
tax, they require more modules on GPU and can
run into out-of-memory issues. However, there is
a hard upper-bound on total number of modules
since there are limited syntax rules in the grammar.
In addition, we may never need to train on long
sentences if all modules can be effectively trained
on short sentences and then generalize composi-
tionally.
As an approach to probing language models,
SynNaMoN contributes to an ethical NLP vision
that seeks to address how models learn human bi-
ases that have societal effects from corpus data.
Understanding syntax representations in models
could be important in such a pursuit since some of
these bias effects are syntactically mediated. For
example, LMs with gender role biases could in-
ternally represent these biases as syntactic gender
agreement e.g. ‘man’ agrees with ’doctor’ and
‘woman’ agrees with ‘nurse’ (Prates et al., 2020).
By understanding the causal structure of sentential
semantics in LMs, we can better disentangle syntax
from spurious correlations transmitted by societal
structures.
7
",no ethical concerns,,no ethical concerns,2023,statement
1056,"It is important to distinguish our improved accu-
racy scores from the ability to computationally un-
derstand empathy. Because of the lack of repre-
sentational learning or contextual knowledge, our
approach would undoubtedly fail in distinguish-
ing empathetic utterances from false-positive cases,
such as sarcastic or even offensive statements (R:
""What’s the matter?"" versus ""What’s the matter
with you?"" ). Given the sensitive nature of the men-
tal health domain, mishandling these situations can
exacerbate one’s situation. Another risk of rely-
ing too heavily on such accuracy numbers include
overlooking the degree to which a mishandling of
a situation can affect an afflicted user. Measuring
this additional personal and humanistic dimension
in benchmarks for computational systems is unde-
niably a difficult problem, but likely a necessary
step to bridge the gap for effective systems for
mental health support. Lastly, while we are able
to do a thorough analysis of our findings with the
explanations provided by micromodels, there are
still portions of their decision making process that
remain opaque. Concretely, the computation of
semantic similarity by large pre-trained language
models like BERT is a key step in our procedure.
Using a simpler, more transparent representation
for micromodels may mitigate this problem. We
believe there is an interesting trade off between
accuracy and explainability in designing each mi-
cromodel.","privacy, wrong predictions",,"privacy, wrong predictions",2023,statement
1057,"As with all neural generation, there are concerns
about misinformation and generating toxic text.
These concerns apply to some degree to poetry gen-
eration, although our rigidly constrained approach
and limited vocabulary should mitigate this.
","misinformation, toxic content",,"misinformation, toxic content",2023,"statement, concerns"
1058,"FetcHR shares the same ethical considerations and
societal impact as prior work on language models
and retrieval systems. Even though FetcHR im-
proves performance on knowledge-intense tasks, it
inherits the bias given by the training data and the
collection of documents in the memory. This bias
might lead to unfair or misleading model outputs.
Since FetcHR does not have an explicit mecha-
nism to detect and prevent a manipulated document
memory, it could get prone to retrieve documents
containing fake knowledge.
","bias, fairness, misleading content",,"bias, fake content",2023,concerns
1059,"We will go through the computation resources and
models we used to conduct our experiments. All of
our models run on 4 48GB NVIDIA A6000 GPUs,
along with 48 TB disk storage and AMD EPYC
7413 24-Core Processor. The experiment take
around 1200 GPU hours for one 48GB NVIDIA
A6000 GPU. Our experiments do not need to lever-
age model or data parallelism. For the model, we
use Huggingface T5-large-lm-adapt models for our
experiments, and will release our code once the
paper been accepted.
",no ethical concerns,,no ethical concerns,2023,statement
1060,"For intrinsic evaluation, we engage three Ph.D. stu-
dents who are fairly compensated. This qualitative
evaluation project passed ethics review of our IRB
as it does not contain any confidential data.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1061,"We did not create any new fMRI data as part
of this work. We used Narratives-Pieman
dataset which is publicly available without
any restrictions. Narratives dataset can be
dowloaded from https://datasets.datalad.
org/?dir=/labs/hasson/narratives . Please
read their terms of use4for more details.
We do not foresee any harmful uses of this tech-
nology.
",no ethical concerns,,no ethical concerns,2023,statement
1062,"Our target is to build direct speech-to-speech trans-
lation systems with a duplex idea of training both
directions in one run. We try our best to reuse exist-
ing pretrained wav2vec2.0, HuBERT, mHuBERT
and mBART models to save energy consuming.
In one run, we require much less GPU-hours for
obtaining S2ST models for both direction usages.
However, compared with textual duplex MT sys-
tems, pre-processing of speech signals still requires
much higher costing of GPU-hours and as listed
in our limitation section (Section 9), smarter ways
of multilingual S2ST architectures are preferred in
the future to reduce the cost of energy from current
quadratic to linear number of models.
Generally, S2ST circumvents traditional cas-
caded systems which concatenate ASR, MT and
TTS with high latency and high requirements of
datasets. There are 3,000 around languages in the
world who do not have their own writing systems
or textual vocabularies. Through our duplex S2ST
models, we hope to be friendly to these languages
so that more and more languages can be covered.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1063,"Ethics IRB approval was obtained from the
Biomedical and Scientific Research Ethics Com-
mittee of the University of Warwick (ref: BSREC
40/19-20) prior to engaging in this research study.
Our work involves ethical considerations around
the analysis of user generated content shared on
a peer support network (TalkLife). A license was
obtained to work with the user data from TalkLife
and a project proposal was submitted to them in
order to embark on the project. The current paper
focuses on the identification of periods of interest
within the user history, in terms of moments of
change. The work on annotation of moments of
change (MoC) is separate to this paper but consid-
ers sudden shifts in mood (switches or escalations).
Annotators were given contracts and paid fairly in
line with University pay-scales. They were alerted
about potentially encountering disturbing content
and advised to take breaks during annotation. The
annotations are used to evaluate the work of the
current paper, which aims to meaningfully segment
timelines in terms of containing likely moments of
change. Potential risks from the application of our
work in being able to identify moments of change
in individuals’ timelines are akin to the identifi-
cation of those in earlier work on personal event
identification from social media and the detection
of suicidal ideation. Potential mitigation strategies
include restricting access to the code base and an-
notation labels used for evaluation. No data can
be shared without permission from the platform or
significantly paraphrased. Any examples used from
the users’ history are anonymised and paraphrased.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice"
1064,"We did not identify any potential ethical issues with
the annotation tool.
",no ethical concerns,,no ethical concerns,2023,statement
1065,"The data that was used for conducting the experi-
ments is composed of text from the public domain
taken from datasets publicly available to the re-
search community. These corpora also conform
to the Twitter Developer Agreement and Policy
that allows unlimited distribution of the numeric
identification number of each tweet. The desire to
combat online HS and prevent the widespreading
of stereotypes cannot be done without automatic
moderation tools, at the risk of increasing cases of
algorithmic discrimination. However, the deploy-ment of such algorithms should be done with care,
as algorithmic discrimination results from the in-
troduction of biases at the time of the design of the
system. These biases consist in the transposition
of general (often stereotyped) or statistical obser-
vations into systematic algorithmic conditions.
",bias,,bias,2023,"statement, concerns"
1066,"The main aim of this paper is to develop novel
methods and techniques to aid the manual work of
NGOs operators. Thus, all the systems we develop
are not intended for being used in the wild but in a
human-machine collaboration setting, allowing the
human to check and possibly post-edit the machine-
generated CNs.
The focus of our work is to develop effective
CNs: for this reason, and to avoid possible misuse
of our models, the data we employed for training
the HS are simple and stereotyped. Moreover, we
will not distribute the generated data since their
content has not been thoroughly checked by hu-
man annotators. As regards the annotation process,
since we are aware that long exposure to hateful
data might be stressful and hurtful, we put in prac-
tice a mitigation procedure similar to that proposed
by Vidgen et al. (2019b), in order to preserve the
mental health of the annotators.
",no ethical concerns,,no ethical concerns,2023,"actions, advice"
1067,"The datasets hosted on EVALIGN are downloaded
from their authors’ websites. The datasets are well-
known and have been used for evaluation in most
literature papers. Model predictions are generated
using the code published on developers’ Github
repositories. We have not retrained or fine-tuned
any language models and used the publicly avail-
able language models on Huggingface. The tool
offers visualization views to facilitate the perfor-
mance evaluation to get a better understanding of
models’ behaviours.
",no ethical concerns,,no ethical concerns,2023,actions
1068,"We believe that humor is a positive and constructive
form of human expression to unite and reduce ten-
sions while respecting cultural differences, beliefs,
and people’s identities. However, we acknowledge
that humor, when used in a Christian or offensive
way to discriminate, ridicule, or disparage individ-
uals or groups, especially those who have been
historically marginalized or oppressed, can have
negative consequences.
So if there are jokes that promote violence, ha-
tred, or prejudice, including but not limited to
racial, gender, and sexual stereotypes, xenopho-
bia, and similar forms of discrimination, then they
ought not to be deemed acceptable. In this context
we find it crucial to report that the corpus used in
this paper contains some texts (annotated as humor)
that are openly racist, specially against black peo-
ple. Other texts considered as jokes have different
groups represented in a negative light, for exam-
ple alentejanos (people from a region in Portugal),
jeweish people, and blonde women. Some other
sensitive subjects are also present in the corpus, for
instance suicide, and pedophilia.
It is crucial to take into account the potential
effects that computer models designed for mood
detection could have on individuals and society,
both during the development phase and when utiliz-
ing them. Ensuring that these models are impartial
and free of undesired bias is of utmost importance
to prevent the perpetuation of stereotypes that could
ultimately result in negative outcomes.
In conclusion, we would like to emphasize that
models used for the recognition of humor have
inherent limitations due to their subjective nature,
which may vary significantly depending on cultural,
social, and individual contexts. Therefore, these
models are constantly evolving and improving, and
evaluating their efficacy is an ongoing process.
",bias,,bias,2023,"statement, concerns"
1069,"In this paper, we collected data from individuals
who attended emotion regulation trainings and pro-
vided consent for the collection and analysis of
questionnaires. The corpus has not been published
yet, as it is undergoing validation by the ethics com-
mittee of École Normale Supérieure Paris-Saclay.
By disidentifying our corpus, we have taken stan-
dard precautions to mitigate the introduction of bi-
ases into our models. Despite our efforts, it is pos-
sible that our models may still contain biases that
we are not aware of. Our models are not intended
for diagnostic purposes, and we do not provide
automatic feedback to individuals for regulating
their emotion, as we would need to be sure that
such feedback does not have any adverse effects on
individuals’ mental health and, instead, facilitates
improved emotion regulation.
",bias,,bias,2023,"concerns, advice"
1070,"While there are many legitimate use cases for au-
thorship analysis, it is also possible to use these
approaches in a way that negatively impacts peo-
ple’s freedom, livelihood, or safety. For example,
these models could be used to de-anonymize texts
written by whistle-blowers, protesters, or other dis-sidents. People may also face personal embarrass-
ment, social stigma, or loss of employment if they
are linked with texts shared under the assumption
of anonymity.
",misuse,,misuse,2023,concerns
1071,"The ethical issues broadly related to the of LMs for
text generation also apply to poetry: the generation
of offensive content, the reproduction of unethical
stereotypes learned from the data, and the substitu-
tion of human creativity by machines. While we do
not have quick answers to these large societal ques-
tions, we observed that due to its training on classic
poetry, GPoeT is not likely to generate offensive
content (for instance, filtering out bad words has
proven unnecessary). Our goal is not the fully-
autonomous generation of poems, but co-creation
of poetry with human users, who have to steer the
system towards a desired form and topic. Our ap-
proach is intended to stimulate human creativity,
not to replace it.
","offensive content, unethical stereotypes, substitution of creativity",,"offensive content, unethical stereotypes, substitution of creativity",2023,"concerns, statement, advice"
1072,"Our project aims to improve the reliability and
safety of large language models, which can be frag-
ile under distribution shift (Ribeiro et al., 2020) and
incur great costs (Ulmer et al., 2020; Zhang et al.,
2021). By properly flagging anomalous data, our
method can lead to direct benefits and societal im-
pacts, particularly for safety-critical applications.From a user’s perspective, our method can help
improve trust in the language models. Our study
does not involve any human subjects or violation
of legal compliance. We do not anticipate any po-
tentially harmful consequences to our work. As
detailed in Appendix A, all of our experiments are
conducted using publicly available datasets. Our
code has been released for reproducibility. Through
our study and releasing our code, we hope to raise
stronger research and societal awareness toward the
problem of out-of-distribution detection in natural
language processing.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1073,"The data collection protocol was approved by the
coauthors’ institutional review board. All anno-
tators were presented with a consent form (men-
tioned below) prior to the annotation. They were
also informed that only satisfactory performance
on the screening example will allow them to take
part in the annotation task. All data collected dur-
ing the tutorial and annotations (including annota-
tors‘ feedback and demographics) will be released
anonymized. We also ensure that the annotators
receive at least $13.50 per hour. Since base com-
pensation is per unit of work, not by time (the
standard practice on Amazon Mechanical Turk),
we add bonuses for workers whose speed caused
them to fall below that hourly rate.
Consent Before participating in our study, we
requested every annotator to provide their consent.
The annotators were informed about the purpose
of this research study, any risks associated with
it, and the qualifications necessary to participate.
The consent form also elaborated on task details
describing what they will be asked to do and how
long it will take. The participants were informed
that they could choose as many documents as they
would like to annotate (by accepting new Human
Intelligence Tasks at AMT) subject to availability,
and they may drop out at any time. Annotators
were informed that they would be compensated
in the standard manner through the Amazon Me-
chanical Turk crowdsourcing platform, with the
amount specified in the Amazon Mechanical Turk
interface. As part of this study, we also collected de-
mographic information, including their age, gender,
native language, education level, and proficiency
in the English language. We ensured our annota-
tors that the collected personal information would
remain confidential in the consent form.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1074,"The data collection protocol was approved by the
coauthors’ institutional review board. All anno-
tators were presented with a consent form (men-
tioned below) prior to the annotation. They were
also informed that only satisfactory performance
on the screening example will allow them to take
part in the annotation task. All data collected dur-
ing the tutorial and annotations (including annota-
tors‘ feedback and demographics) will be released
anonymized. We also ensure that the annotators
receive at least $13.50 per hour. Since base com-
pensation is per unit of work, not by time (the
standard practice on Amazon Mechanical Turk),
we add bonuses for workers whose speed caused
them to fall below that hourly rate.
Consent Before participating in our study, we
requested every annotator to provide their consent.
The annotators were informed about the purpose
of this research study, any risks associated with
it, and the qualifications necessary to participate.
The consent form also elaborated on task details
describing what they will be asked to do and how
long it will take. The participants were informed
that they could choose as many documents as they
would like to annotate (by accepting new Human
Intelligence Tasks at AMT) subject to availability,
and they may drop out at any time. Annotators
were informed that they would be compensated
in the standard manner through the Amazon Me-
chanical Turk crowdsourcing platform, with the
amount specified in the Amazon Mechanical Turk
interface. As part of this study, we also collected de-
mographic information, including their age, gender,
native language, education level, and proficiency
in the English language. We ensured our annota-
tors that the collected personal information would
remain confidential in the consent form.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1075,"Crawling the Dark Web
While crawling the Dark Web, we take caution not
to expose ourselves to content that should not be
accessed. For example, illicit pornographic content
(such as child pornography) are easily found on the
Dark Web. However, our automated web crawler
takes the approach of removing any non-text media
and only stores raw text data. By doing so, we do
not expose ourselves to any sensitive media that is
potentially illegal.
Sensitive Information Masking
Since the Dark Web harbors many activities con-
sidered to be malicious in nature, it is of utmost
importance that sensitive data be left out of the
text corpus used for pretraining. In particular, it is
possible that some contents in the Dark Web may
include private information such as e-mails, phone
numbers, or IP addresses. To prevent DarkBERT
from learning representations from sensitive texts
as mentioned above, we mask our data before feed-
ing it to our language model. While we have used
both DarkBERT pretrained on preprocessed text
and raw text for our experiments, we have used
both of the models only for evaluation purposes. In
addition, we only release the preprocessed version
of DarkBERT in order to avoid any malpractices8.
Through extensive testing on fill-mask and syn-
onym inference tasks, we observe that it is infeasi-
ble to infer any characteristics or data that might
be considered sensitive or private in nature using
the preprocessed version of DarkBERT.
Annotator Ethics
For the task of noteworthy thread detection, we
recruited two researchers from a cyber threat intel-
ligence company as mentioned in Section 5.2, who
agreed to assist us in our research methods. For a
fair annotation process in the discussion of note-
worthy threads, both recruited annotators handled
the same set of thread data and were given equal
compensations.
Use of Public Dark Web Datasets
Both DUTA and CoDA are available upon request
by the respective authors, and due to the sensitive
nature of the Dark Web domain, these datasets are
8Researchers can request access to DarkBERT and related
use case datasets by filling out the request form in the follow-
ing url: https://s2w.inc/resources/darkbert/only to be used for academic research purposes.
We adhere to this guideline and only utilize the pro-
vided data in the context of research for this work.
On the other hand, we do not plan to publicly re-
lease the Dark Web text corpus used for pretraining
DarkBERT for similar reasons.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1076,"The datasets adopted are publicly available and
widely studied benchmarks collected from profes-
sionals or well-trained annotators. All personally
identifiable and sensitive information, e.g., user
and platform identifiers, in these dataset has been
filtered out. We do not make any treatment rec-
ommendations or diagnostic claims. Compared
with existing methods for emotional support con-
versations, the proposed method can be regarded
as one step further to a more safer ESC system.
The proposed method retrieves knowledge from a
well-established mental health knowledge graph,
which can be maintained by filtering out harm-
ful information when applying into applications.
Then the knowledge-enhanced approach can allevi-
ate the randomness during the response generation
and provide the guidance towards more positive
responses. In order to prevent the happening of
unsafe cases, the analysis of emotion intensity pre-
diction can also serve as an alarming mechanism
that calls for handoffs to an actual psychologist.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1077,"We acknowledge that presenting a data summary
using topic modeling may not adequately represent
the voice of minorities and that pre-trained knowl-
edge from an online corpus may exacerbate such
bias. When applying topic modeling in high-risk
real-world scenarios, human annotators can help
understand the predominant view of each topic to
ensure that minor viewpoints are included. This is
part of the larger challenge of AI ethics, and we
plan to consider this challenge in the future.
","bias, fairness",,"bias, fairness",2023,"concerns, suggestions"
1078,"Our work is concerned with the use of language
generation models to augment training datasets for
schema-guided dialogue datasets. The generation
phase is unconstrained, so the model may generate
candidates that exhibit biases inherited from the C4
(Raffel et al., 2020) and HugeNews (Zhang et al.,
2020a) pre-training datasets. In our experiments,
we did not observe toxic or harmful outputs, but
on one occasion the model did generate the word
apartheid as part of an incoherent sentence. For
this reason, our filtering stack rejects any candi-
dates containing sensitive words. The list of words
that parameterize the sensitive words filter is de-
fined by the user.
",bias,,bias,2023,"concerns, actions"
1079,"While our work was developed to generate abun-
dant data supporting work towards improving well-
being, the negative statements it generates could be
misused. The parallel data of unhelpful thoughts
and their reframed versions can also be used to
generate negative texts from neutral ones, by train-
ing systems with reframed versions as the input
and unhelpful thoughts as the output. This risk of
generating negative content from positive/neutral
texts aligns with the risks of toxicity reduction and
sentiment style transfer tasks.
Conversely, a different risk stems from over-
eager use of our work. This work aims to examine
the feasibility of generating ample practice mate-
rial anchored on specific personas. We hope that
releasing a large dataset of unhelpful thoughts and
reframings will further research that will ultimately
help practitioners, but there is a danger that peo-
ple attempt to use the material as is, without the
supervision of a trained professional, which could
be harmful, as the material has not been tested with
participants while monitoring adverse events such
as increased anxiety or warped understanding of
what unhelpful thoughts and useful reframings are.
",misuse,,misuse,2023,"concerns, advice"
1080,"The privacy concerns regarding training data ex-
traction from PLMs were reviewed to help mature
discussions in academia and industry. Of course,
its purpose is not to promote these attacks.
Studies on PLMs tend to focus on the English
language, which is the language used by the ma-
jority of people in the world, and the same is true
for training data extraction. Therefore, this study
focused on English. As indicated in Section 7.2,
research on other languages is encouraged.
",lack of inclusivity,,lack of inclusivity,2023,"statement, advice"
1081,"Although the datasets are publicly available, we
also follow strict ethical protocols ( Benton et al. ,
2017 ) of sharing datasets. To prevent misuse and
protect user privacy, all examples shown in this pa-
per have been obfuscated and paraphrased accord-
ing to the moderate disguise scheme ( Bruckman ,
2002 ).
5 Conclusion
In this paper, we propose a depression sever-
ity detection model based on sentiment-guidedTransformer and severity-aware contrastive learn-
ing. The sentiment-guided Transformer can efﬁ-ciently fuse semantic and sentiment informationfrom user’s posts. Then we use a severity-awarecontrastive learning framework, which can fullyleverage label information for capturing severity-
speciﬁc features and helping the model distinguish
closely-labeled categories. The experimental re-
sults show that our model performs better than re-
cent models and achieves superior performance on
two publicly available datasets. Further analysis
determines the effectiveness of all proposed mod-
ules.
",no ethical concerns,,no ethical concerns,2023,statement
1082,"Translation quality scores were provided by bilin-
gual raters as mentioned in Section 4. They were
all paid a fair rate. We can not open-source the
data form our experiments given that our sources
are shared under no-derivative license. Small hu-
man evaluation detailed in appendix D was done
by volunteers.
",no ethical concerns,,no ethical concerns,2023,statement
1083,"User Study. Prior to carrying out our user study,
the survey methodology was reviewed and ap-
proved by our Institutional Review Board for ethi-
cal compliance. While unlikely, we examined each
question for its appropriateness. To ensure partic-
ipants’ anonymity, the responses are anonymized
and aggregated, and it is extremely unlikely that a
participant can be identified via their response. In
terms of fair compensation, we paid S$15 for each
complete response of 100 questions, assuming an
hour’s worth of work, it is higher than our insti-
tution’s prevailing rate for undergraduate student
work. To ensure their well-being, study participants
are allowed up to a week to complete the tasks, at
their own preferred pace and place.
Corpora. We select corpora that have open li-
censing agreements that allows for non-profit aca-
demic use, and the permissions allowing us to trans-
form and re-distribute the processed corpora as
word-pair counts.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1084,"All steps and data described in our paper follow the
ACL Ethics Policy2.
",no ethical concerns,,no ethical concerns,2023,statement
1085,"The collected data might still have a few insulting
or discriminatory words, although we have made
many efforts to reduce its effects. We have high-
lighted these concerns in many parts of this paper
and warned readers.
",bias,,bias,2023,concerns
1086,"Although our methods for hate speech detection
provide increased privacy to downstream users of
content moderation technologies, i.e. users of on-
line platforms, there are signiﬁcant risks to it. First,
our proposed technology has dual use implications,
as it can also be applied maliciously, for instance to
limit the speech of speciﬁc groups. Second, while
this work uses publicly available datasets, there is
an inherent tension between the public availabil-
ity of data and privacy risks. Finally, although all
model updates occur on local client devices, feder-
ated learning is not a silver bullet which addresses
issues of systemic violence of content moderation
Thylstrup and Talat (2020), or issues of privacy.
Rather, federated learning can provide an avenue
for engaging in meaningful conversations with peo-
ple and their experiences and needs for content
moderation and privacy.
","research misuse, privacy risks",,"research misuse, privacy risks",2023,concerns
1087,"All use of human participants in this study has been
approved by the Ethics Board of the primary author’s
institution, including the disclosure of demographic
information. Regarding the generation of tongue
twisters, language generation is a necessarily creative
domain that has the ability to reproduce content that
some individuals may find offensive. Care was taken
to check outputs in the human evaluation set for any
such materials, and if they had been produced, they
would have been removed from the evaluation set.
Additionally, no egregiously offensive material has
been provided in the TwistList dataset. However, the
distinction between offensive and humorous content is
a highly complex topic, and therefore some examples
within the dataset may not be suitable for all individu-
als (e.g. suggestive content and swearing, such as ""I’m
not the pheasant plucker, I’m the pheasant plucker’s
son"", and the clear relation to common expletives).
","bias, offensive content",,"bias, offensive content",2023,"concerns, actions"
1088,"This article follows the ACL Code of Ethics. The
annotations are based on public datasets that do
not contain private data. The algorithm we devel-
oped is an architectural optimization technique for
improving model performance. To our best knowl-
edge, there are no foreseeable potential risks to
using this technique.
",no ethical concerns,,no ethical concerns,2023,actions
1089,"In this work, human annotation is conducted with
the utmost care to ensure the absence of offensive
content and the non-collection of personal identify-
ing information. Comprehensive explanations are
provided to the annotators regarding the purpose
and appropriate usage of their annotations, ensur-
ing their informed consent has been obtained. The
basic demographic and geographic characteristics
of the annotator population are not reported, as they
do not serve as the primary source of the data in
this work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1090,"Although our experiments demonstrate the role of
conclusions in counter-argument generation, we be-
lieve that this task is far from solved. We are aware
that issues such as faithful text generation must be
considered when working with language models to
avoid misinformation. We believe that mechanisms
such as a fact-checking component or a factuality
optimizer should accommodate any text generation
model. The primary goal of our experiments is to
highlight the potential of conclusion inference as
part of the counter-argument generation pipeline,
not to create an approach that is already ready for
practical application.
",misinformation,,misinformation,2023,statement
1091,"Our method extracts subgraphs from knowledge
graphs. Hence, any potential biases present in the
knowledge graph can propagate to our CCKGs.
While this can be problematic, our approach allows
to trace biases back to their origin. This is com-
parable to manual information extraction, as all
knowledge sources can contain biases – for exam-
ple political tendencies in newspapers. Strategies
to automatically avoid biases (Mehrabi et al., 2021)
could also be incorporated in future work. How-
ever, as our approach is a pure extraction, it can
not generated new potentially harmful information.
Thus, CCKGs are perhaps more reliable for sen-
sitive application than knowledge representations
generated without grounding.
",bias,,bias,2023,"concerns, actions, suggestions"
1092,"The corpus and the computational baselines pre-
sented in this paper target a sensitive issue: what is
considered appropriate to say in a discussion. We
suggest differentiating between freedom of speech,
hate speech, and inappropriate speech. We believe inappropriate speech is an extension of hate speech
that leads to a less free but more healthy climate
in speech exchange. While freedom of speech in
many countries is limited by hate speech in law,
the extension to inappropriate speech is not. Con-
sequently, automating the detection of inappropri-
ateness and dealing with it in the same way hate
speech is addressed (often by removal) may be per-
ceived as hurting individuals’ freedom of speech
and, thus, must be handled with great care.
However, we see no strong immediate ethical
concerns regarding the computational methods spe-
cific to our work, as they only detect inappropriate-
ness and do not recommend any actions. We stress,
though, that they are not meant yet for real-life
applications. Apart from the outlined limitations,
we also do not see notable ethical concerns regard-
ing our taxonomy, as we derived it systematically
from existing literature and always encouraged our
annotators to add their own reasons.
Finally, we aimed to ensure fair payment. As
discussed in the paper, our annotators were paid
about $13 per hour, which exceeds the minimum
wage in most US states and is also conform to the
standards in the regions of our host institutions.
",no ethical concerns,,no ethical concerns,2023,"statement, concerns, actions"
1093,"We conduct the experiments based on an existing
publicly available dataset from Zhou et al. (2018)
which is a large-scale dataset widely used to study
commonsense dialogue generation, and we strictly
follow the license and instructions. We also read
and acknowledge the ACM Code of Ethnics and
Professional Conduct.6We take our professional re-
sponsibilities very seriously, and our study did not
violate any ethical principles. Additionally, whilst
our work concerns the incorporation of knowledge
from knowledge graphs in dialogue systems, we
acknowledge that the veracity and validity of the
knowledge in such resources should be assessed in
production, in order to avoid the perpetuation of
misinformation.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
1094,"Scientific reproducibility is key to the benefits sci-
ence can bring to society. Simply put, findings that
cannot be reproduced cannot be relied upon, which
can lead to wasted societal resources or even to
harmfully incorrect understandings that misguide
interventions. Our work focuses on the use of
checklists to improve reporting of reproducibility
information in scientific publications. While overly
prescriptive and general rules about reproducibility
could stifle less represented research communities
whose practices may be less well understood by
conference organizers, checklists attempt to miti-
gate this risk by only reminding authors of possibly
salient information while still permitting authors to
determine which items are or are not applicable.
At the same time, checklists which are filled out
and collected for data analysis have the additional
ethical risks associated with work that attempts to
make social practices legible. That is, a check-
list may neglect to cover practices used in a re-
search community and thereby efface their role in
the overall scientific endeavor, or conversely some
practice may receive unfair scrutiny in excess of
that given to other more prestiged practices. In the
long term, checklists are perhaps most important
as documents for guiding new generations of re-
searchers writing their first papers, and thus even
without being enforced they may still be taken as
normative statements about best practices in the
field.
To guide efforts to improve reproducibility in
the field of NLP, we have analyzed responses to the
NLP Reproducibility Checklist collected by four
conferences. The Checklist data is covered by the
default terms as it has no stated license, and we
use it with direct permission from the conference
organizers who collected it. The authors of the
first version of the checklist state that it is intended
for “improved reporting of the setup and results of
the experiments that authors have conducted” and
that it will be used to “quantitatively analyze our
checklist responses” (Dodge and Smith, 2020).
We have endeavored to maintain the privacy of
respondents by keeping the data anonymized and
presenting results at a sufficient level of aggrega-
tion to prevent deanonymization. Nevertheless all
work that seeks to describe the opinions of groups
of humans caries an ethical burden to do so accu-
rately and consistently with the wishes of those
represented. To that end, we take care to point out
limitations in what can be inferred from the data,
and as originally intended by the data creators we
do not make the data publicly available.",no ethical concerns,,no ethical concerns,2023,"statement, concerns"
1095,"We did not create any of the models, datasets, or
applications covered in this paper. Any ethical
issues with the preexisting OpenIE datasets we usein this paper will reflect on this work.
",no ethical concerns,,no ethical concerns,2023,statement
1096,"Since the dialogue response generation model uses
large-scale data from websites (e.g., Wikipedia,
Twitter) during pretraining, it may generate re-
sponses that contain implicit biases and offensive
content. We will incorporate mechanisms to reduce
harmful responses and build a safe and ethically
robust dialogue system in the future.","bias, toxic content",,"bias, toxic content",2023,"statement, concerns"
1097,"This work does not propose a new model or dataset,
but rather probes the behavior of existing models.
Thus novel ethical considerations about model be-
havior and dataset contents are not directly raised
by this work. While not explicitly focused on ethi-
cal considerations, this paper’s methods hopefully
contribute to better understanding model behavior,
and could be used to understand the ways in which
large language models treat underrepresented and
marginalized language varieties.
",no ethical concerns,,no ethical concerns,2023,statement
1098,"In this paper, we demonstrate the potential threat
of textual backdoor attacks by showing the exis-
tence of a backdoor attack that is both effective and
stealthy. Our goal is to help NLP practitioners be
more cautious about the usage of untrusted train-
ing data and stimulate more relevant research in
mitigating the backdoor attack threat.
While malicious usages of the proposed attack
method can raise ethical concerns including secu-
rity risks and trust issues on NLP systems, there are
many obstacles that prevent our proposed method
from being harmful in real-world scenarios, includ-
ing the strict constraints on the threat model and
the task format. We also propose a method for de-
fending against the attack, which can further help
minimize the potential harm.
",security risks,,security risks,2023,"concerns, actions"
1099,"Detailed error analysis should always be performed
before deploying a quality estimation system in
machine translation production pipelines.
",no ethical concerns,,no ethical concerns,2023,suggestions
1100,"For the human annotations on the dataset, the lan-
guage experts are native speakers of the languages
and from the Indian subcontinent. They were paid
a competitive monthly salary to help with the task.
The salary was determined based on the skill set
and experience of the expert and adhered to the
norms of the government of our country. The
dataset has no harmful content. The annotators
were made aware of the fact that the annotations
would be released publicly and the annotations con-
tain no private information. The proposed bench-
mark builds upon existing datasets. These datasetsand related works have been cited.
The annotations are collected on a publicly avail-
able dataset and will be released publicly for future
use. The IndicCorp dataset which we annotated
has already been checked for offensive content.
All the datasets created as part of this work will
be released under a CC-0 license10and all the
code and models will be released under an MIT
license.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1101,"Although weak supervision requires only unlabeled
documents, we demonstrated that the model might
fail when the training dataset does not contain data
related to a particular category or target group. It
is especially concerning because the target groups
are often minorities and under-represented. There-
fore, we recommend against “throwing” a weakly-
supervised algorithm on a dataset and hope the
model will work. Instead, we should evaluate a
model thoroughly before applying it to the real
world, such as manually examining the model’s
predictions, behavioral testing the model using a
checklist (Ribeiro et al., 2020) or conducting unsu-
pervised error estimation (Jin et al., 2021).
",misrepresentation,,misrepresentation,2023,"concerns, actions, suggestions"
1102,"The scope of this work is to introduce NLP tech-
nologies to the continuing patent application pro-
cess to make it more efficient. The outcomes from
our work would therefore have an industrial impact
through enabling organizations to file more contin-
uing patents with less time. There would then be a
risk that, if our technologies were available to only
particular organizations, fair competitions could
not be ensured. We therefore decided to release the
dataset and code to the public.
This work was intended to be beneficial to patent-
related processes and studies in artificial intelli-
gence, machine learning, and NLP. The outcomes
from this work should therefore be used only for
these purposes.The coverage of the UEE dataset in terms of the
IPC subclass, language, and countries and regions
are limited. Care must be taken when using this
dataset, accordingly.
The dataset does not contain any personal infor-
mation, as it has been created from publicly avail-
able patent specifications. We nonetheless took
special care to check if any personal information
was included in the dataset by accident when creat-
ing the dataset.
All the data we used in this work are publicly
available. The pre-trained language model that
we used, i.e. RoBERTa, is also publicly available.
Our Longformer was converted from the RoBERTa
with a method that was also known to the public.
Besides, since we have released all the necessary
code and dataset along with the paper, all the ex-
perimental results in the paper are reproducible.
Regarding the hiring of the annotators (the ex-
pert patent practitioners), we negotiated with their
company in advance to fairly determine the charge,
which was the equivalent of the cost of hiring ex-
pert patent practitioners for patent search. We ex-
plained to the human annotators about the purpose
of the data annotation and how it would be used in
advance of the annotation.
Regarding the compute in our experiments, we
executed 30 fine-tuning processes, which took 57
hours in a single Nvidia A100 GPU in total.
8 Acknowledgment
We are deeply grateful to all those who played a
role in the success of this project. We would like to
thank Yoshimasa Utsumi, Kazutaka Ohara, Takeshi
Imamura, Hsuan-Yu Kuo, Takako Yoshida, Vijay
Daultani, Colin Ian Rowat, and Aztec Co., Ltd. for
their invaluable input and support throughout the
research process. Their insights and expertise were
instrumental in shaping the direction of this project.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
1103,"Our work adheres to the ACL Ethics Policy. This
paper aims to investigate generative model-based
approaches for learning automatic diagnostic logic,
with the objective of reducing the burden on doctors
and promoting the advancement of automatic diag-
nosis systems. It is crucial to emphasize that the
proposed methods are designed solely for research
purposes and are not suitable for direct clinical ap-
plication due to the potential risks associated with
the misuse of automatic diagnosis systems.
It is important to note that the introduced dataset
(Ped) was sourced from genuine electronic medical
records, with all patient privacy-related information
meticulously eliminated. To ensure data privacy
and security, we performed a comprehensive man-
ual review of the dataset, confirming that it contains
no identifiable or offensive pieces of information
within the experimental dataset.
",misuse,,misuse,2023,"concerns, advice"
1104,"We acknowledge that the content that emerged from
the data is narrow in terms of cultural perspectives,
mainly addressing western cultures. Moreover,
the analysis of the audiences is not exhaustive of
the diversity of humankind, especially not exhaus-
tively accounting for queer identities in particular
trans and non-binary identities. With the present
research, we do not intend to reinforce representa-
tional biases, rather to highlight them.
",bias,,bias,2023,concerns
1105,"The duration 1800-1914 is considered as the out-
of-copyright duration in Project Gutenberg, under
the categories ‘Rule 1: Works First Published Be-
fore 95 Years Ago and Before 1977’ and ‘Rule
10(c) - Works of Treaty Parties and Proclama-
tion Countries First Published Between 1923 and
1977’ (Gutenberg). Although the duration is out-
of-copyright regarding literary works, we stored
the data securely with restricted access. We do not
release the dataset.
",no ethical concerns,,no ethical concerns,2023,statement
1106,"Our LECO framework is designated to improve
the training of multi-exit BERT and dynamic early
exiting performances. Our work can facilitate the
deployment and applications of pre-trained models
on devices with less powerful computation capabil-
ities, making the state-of-the-art models accessible
for everyone. In addition, we hope this technology
can help reduce the carbon footprints of NLP-based
applications. Furthermore, the datasets we exper-
iment with are widely used in previous work and,
to our knowledge, does not introduce new ethical
concerns.
",no ethical concerns,,no ethical concerns,2023,statement
1107,"We acknowledge that all co-authors of this paper
are aware of the ACM Code of Ethics and honor
the code of conduct. We collected our data from a
public dataset that permits academic use. As our ex-
periments are limited to the binary linguistic forms
represented in the used data, we cannot guaran-
tee that our models will always generate unbiased
content.
",bias,,bias,2023,"statement, concerns"
1108,"Our model is designated to recognize entities
in input sequences. We use two groups of tasks.
The three benchmark datasets CADEC, ACE2004,
and CoNLL03 are widely studied in the literature,
and our work does not introduce new ethical is-
sues. Since the two proprietary datasets are all
anonymized and only used for training models in
our institution, no ethical concerns are included in
our work.
",no ethical concerns,,no ethical concerns,2023,statement
1109,"We used a freely available dataset and a pre-trained
model from the Hugging Face Hub for our exper-
iments. We selected a pre-trained model with anappropriate size (XLM-RoBERTa-base) given our
purpose of use. We needed to perform many rounds
of clustering and fine-tuning for the pre-trained
model. Therefore, we set preliminary experiments
beforehand with a smaller sample size for each step
to ensure that the experiments could be performed
effectively.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1110,"The presence of bias within NPC models/systems
poses a significant risk particularly as the demo-
graphic of young individuals, still in the age of
development, who enjoy playing video games con-
tinues to expand. In 2006, 92% of children in
the ages of 2-17 had played video games (Do ˘gan,
2006). 97% of players under the age of of 18 play
more that an hour of games daily (Granic et al.,
2014). According to recent statistics, the global
demographic of active video game players is pro-
jected to increase over 5% year-over-year (Do ˘gan,
2006), reaching over 3 billion active players world-
wide in 20232. This means, in the future, video
games will reach more young children and adoles-
cents. If the presence of bias is not addressed, it
could subconsciously normalize problematic be-
haviours seen in games in children as humans are a
product of both nature and nurture (Plomin and As-
bury, 2005). This in turn may lead to more biases
being overlooked or ignored by the next generation
of researchers, creating a vicious cycle.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1111,"This study did not involve any sensitive data
but only used publicly available data, includingWikipedia, CC-100, JGLUE, Japanese NER, and
UD as explained in the paper. Although we plan
to release the resulting models, they might perform
unfairly in some circumstances, as reported in Bal-
dini et al. (2022). We highly recommend users to
refer to studies on debiasing PLMs, such as Guo
et al. (2022).
",bias,,bias,2023,"statement, concerns, advice"
1112,"All datasets utilized in this study are publicly avail-
able, and we have adhered to ethical considerations
by not introducing any additional information into
ChatGPT’s inputs.
",no ethical concerns,,no ethical concerns,2023,statement
1113,"This work does not involve any sensitive data
but only uses publicly available data, including
Wikipedia, GLUE, SQuAD, and UD as explained
in the paper. Although we plan to release the result-
ing models, they might perform unfairly in some
circumstances, as reported in Baldini et al. (2022).
We highly recommend users to refer to studies on
debiasing pretrained language models, such as Guo
et al. (2022).
",bias,,bias,2023,"statement, concerns, advice"
1114,"We use publicly available data sets in our experi-
ments with permissive licenses for research experi-
ments. We do not release new data or annotations
as part of this work.
",no ethical concerns,,no ethical concerns,2023,statement
1115,"We foresee no ethical concerns with this work.
Acknowledgment
SY was supported by the National Natural Science
Foundation of China (61976139). This study was
also supported by funds from an MIT-IBM Watson
AI Lab grant.
",no ethical concerns,,no ethical concerns,2023,statement
1116,"The human study presented in this work has been
approved by the IRB of Peking University. We have
been committed to upholding the highest ethical
standards in conducting this study and ensuring the
protection of the rights and welfare of all partici-
pants. Considering that the workload of the proce-
dure for participants is relatively high among all
human studies, we paid the participants a wage
of $14.6/h, which is significantly higher than the
standard wage (about $8.5/h). Every expert was
paid $240 for grading the 24 review paragraphs
composed by the participants.
We have obtained informed consent from all par-
ticipants, including clear and comprehensive in-
formation about the purpose of the study, the pro-
cedures involved, the risks and benefits, and the
right to withdraw at any time without penalty. Par-
ticipants were also assured of the confidentiality
of their information. Any personal data collected
(including name, age, and gender) was handled in
accordance with applicable laws and regulations.
Broader Impact
The underlying impact of the mindset brought
byPersLEARN goes beyond research training to-
ward science education in general. Specifically,
PersLEARN provides the infrastructure for further
investigation in two aspects: (1) embracing the di-
verse perspectives of the same scientific topic to
construct a stereoscopic understanding of the topic;
(2) facilitating the communication between junior
researchers with different mindsets.
The broader impact is analogous to the classic fa-
bleBlind men and an elephant , where each man in-
terpreted the elephant differently because they were
standing on different perspectives. Though this has
been a metaphor complaining that science is limited
by observation (Heisenberg, 1958), it highlights the
virtue of scientific research—focused, and every
young researcher understands and interprets sci-
ence from a focused perspective. Hence, to gain a
more comprehensive view of the elephant, the blind
men may put their understandings of it together and
then try to synthesize it based on their perspectives.
In contrast, a sighted person may view the elephant
from a distance and capture a holistic view at first—
she ends up with a superficial understanding of the
elephant if not selecting a perspective and going
close to the elephant, like the blind. Thus, by em-
bracing diverse perspectives ( i.e., visualizing theperspective frames in a hub), one gets a stereo-
scopic view and, more importantly, a deeper un-
derstanding of the scientific topic. Moreover, when
the metaphorical blind men in the fable attempt to
articulate their distinct perspectives, they may be
hindered by the gap between mindsets. To exem-
plify, individual might struggle to comprehend the
concept of a “fan”, which in their perception, the
elephant appears to resemble. This suggests that
the communication of science should be executed
in a listener-aware way and that the speaker’s per-
spective should be transformed ( i.e., by changing
the terms used in slices and connections) to its ana-
logical equivalent in the listener’s mindset. Thus,
science can be communicated easily, facilitating its
transparency, reliability, and the chances of cross-
domain collaboration. In summary, our framework
of scientific perspective may bring science educa-
tion to a future with better student-centered consid-
erations (Leshner, 2018).
",no ethical concerns,,no ethical concerns,2023,"statement, actions, suggestions"
1117,"The cases shown in this paper are generated auto-
matically from different data augmentation meth-
ods, which do not represent the viewpoints of the
authors. Due to social bias and lack of profes-
sional knowledge, the data generated from gen-
erative PLMs may contain misleading and toxic
information, which needs to be addressed before
being applied to realistic scenarios. Besides, all
data generated from our proposed methods are only
for scientific research. Finally, we provide compre-
hensive details of our model implementation and
upload the source code, which guarantees the re-
producibility of our experimental results.
","bias, misleading information, toxic information",,"bias, misleading and toxic content generation",2023,"concerns, advice"
1118,"Any work on biomedical data presents a series of
ethical considerations. First, the data employed in-
cludes sensible data from actual patients and, there-
fore, a possible breach of their privacy. Thorough
corpus curation and deidentification are required
to ensure that no information could be related to
the patients. Secondly, decisions in the medical
domain may have consequences for the patient’s
health. Automating any task in this domain with-
out proper human supervision may lead to wrong
treatments or diagnoses and the associated risks to
those decisions.
In the particular case of problem list summariza-
tion, our proposed system only produces outputs
already present in the provided human-generated
data. Errors resulting from false diseases halluci-
nated by the model can be discarded. However,
errors due to missing or partially annotated entities
are likely, especially in cases with the provided
notes are not entirely consistent. For this reason,
this system should be employed under the supervi-
sion of a human expert in the medical field and not
as a standalone automatization tool.
","privacy concerns, misuse, model inaccuracies",,"privacy concerns, misuse, model inaccuracies",2023,"concerns, advice"
1119,"It is acknowledged that the experiments reported in
this study are limited to high-resource languages.
However, the methodology employed is language-
independent and may be applied to other languages
in the future, provided that adequate annotated data
becomes available.",no ethical concerns,,no ethical concerns,2023,statement
1120,"The study aims to extend deep learning-based mod-
els on the ability to generalize scripts under differ-
ent user contexts. The script learning models in-
troduced in the work can potentially be helpful for
task-oriented dialog systems to suggest solutions
to users. The dataset on which we base our exper-
iments is constructed automatically from the pub-
licly available website WikiHow. Since the website
is primarily crowdsourced, the models trained on
the data might incur subjective bias. During the
human evaluation phase of the experiment, all in-
volved human judges participated voluntarily and
received decent payment.
",bias,,bias,2023,"statement, concerns, actions"
1121,"This work did not rely on any user studies or anno-
tations.
Constrained generation may potentially be used
for nefarious purposes such as infusing biases or
misinformation in the generated texts. The authors
do not condone any such usages of the proposed
approach.
",misuse,,misuse,2023,"concerns, statement"
1122,"The models presented in the paper make use of
the existing pretrained systems that train on large
collections of data and are known to inherit biases
that are existent in the training data. The event
language models we train are also susceptible to
these biases, which can result in generation of event
sequences with these biases.
",bias,,bias,2023,"statement, concerns"
1123,"While our algorithm is primarily a tool for improv-
ing classifier performance in label-scarce settings
and uses publicly available, anonymized datasets,
we acknowledge the potential ethical implications
it may carry. Despite the neutral nature of our tool,
it could unintentionally propagate or amplify bi-
ases favoring certain styles of communication. If
used in real-time settings or without proper checks
in place, it could inadvertently alter the natural dy-
namics of the classroom as teachers or students
might modify their behavior based on how they be-
lieve the classifier categorizes their utterances. Like
any other, we recognize that our tool could makemistakes or be misused by over-relying on quantita-
tive aspects over qualitative aspects of instruction.
Therefore, real-world application requires continu-
ous vigilance and open dialogue with practitioners
and stakeholders to ensure its use benefits teaching
and learning.
","bias, misuse",,"bias, misuse",2023,"concerns, advice"
1124,"This paper respects existing intellectual property
by making use of only publicly and freely available
datasets. The crowd-sourced task was approved by
our Institutional Research Ethics Board. The anno-
tators were based in the United States of America
and were paid the federal minimum wage of $7.25
per hour. Our annotation process stored no infor-
mation about annotator identity and as such there is
no privacy risk to them. The individual sentences
selected did not have any risks to privacy either (as
evaluated by manual annotation of the sentences).Models trained on this dataset may not generalize
to external datasets gathered from different popula-
tions. Knowledge about language features may not
generalize to other languages.
Any dataset of semantic relatedness entails sev-
eral ethical considerations. We list some notable
ones below. Many of these were first introduced
in the context of sentiment lexicons (Mohammad,
2020, 2023). We adapted them to semantic related-
ness datasets and added to the discussion.
•Coverage: We sampled English sentences from
a diverse array of sources from the internet, with
a focus on social media. Yet, it is likely that
several types of sentences (and several demo-
graphic groups) are not well-represented in STR-
2022. The dataset likely includes more sentences
by people from the United States and Europe
and with a socio-economic and educational back-
grounds that allow for social media access.
Not Immutable: The relatedness scores do not
indicate an inherent unchangeable attribute. The
relatedness can change with time, but the dataset
entries are largely fixed. They pertain to the time
they are created.
• Socio-Cultural Biases: The annotations of relatedness capture various human biases. These biases may be systematically different for different
socio-cultural groups. Our data was annotated by
US annotators, but even within the US there are
diverse socio-cultural groups.
• Inappropriate Biases: Our biases impact how
we view the world, and some of the biases of an
individual may be inappropriate. For example,
one may have race or gender-related biases that
may percolate subtly into one’s notions of how
related two units of text are. Our dataset curation
was careful to avoid sentences from problematic sources, and we have not seen any inappropriate relatedness judgments, but it is possible
that some subtle inappropriate biases still remain.
Thus, as with any approach for sentence representation or semantic relatedness, we caution users
to explicitly check for such biases in their system
regardless of whether they use STR-2022.
• Perceptions (not “right” or “correct” labels):
Our goal here was to identify common perceptions of semantic relatedness. These are not
meant to be “correct” or “right” answers, but
rather what the majority of the annotators believe
based on their intuitions of the English language• Relative (not Absolute): The absolute values of
the relatedness scores themselves have no meaning. The scores help order sentence pairs relative
to each other. For example, a pair with a higher
relatedness score should be considered more related than a pair with a lower score. No claim is
made that the mid-point (relatedness score of 0.5)
separates related words from unrelated words.
One may determine categories such as related
or unrelated by finding thresholds of relatedness
scores optimal for their use/task.
We recommend careful reflection of ethical considerations relevant for the specific context of deployment when using STR-2022.","bias, inaccuracies",,"bias, inaccuracies",2023,"statement, concerns, actions, advice"
1125,"Our work is contributing to an area of research that
requires valid assessments of mental health to ro-
bustly evaluate the progress the new approaches
can make in order to ultimately improve mental
health assessment (De Choudhury et al., 2013; Cop-
persmith et al., 2018; Zirikly et al., 2019; Son et al.,
2021). The intention of this work for its stakehold-
ers at this point in time, clinical psychology and the
interdisciplinary area of NLP and psychology, is its
use toward developing more accurate and validated
techniques for the benefit of society and human
well-being.
We view this work as a step toward an assess-
ment tool that could be used alongside professional
oversight from trained clinicians. In this interdisci-
plinary work, we aim to improve the state-of-the-
art automatic assessment models. However, at this
time, we do not enable use of our model(s) indepen-
dently in practice to label a person’s mental health
states. Clinical diagnosis requires more informa-
tion such as interviews and physical examinations
in addition to surveys. In addition, use of such
models for targeted messaging or any assessment
based on private language without author consent is
prohibited among our terms of use. This research
has been approved by an independent academic
institutional review board (IRB).
Before our models are used by trained clinicians,
they must demonstrate validity in a clinical setting
for the target clinical population. The study steps
for said evaluation should be reviewed by an ex-
ternal ethical review board, and practice should
follow clinical guidelines. Unlike an invasive med-
ical device, the majority of measures used in psy-
chiatry are not required to go through regulatory
agency reviews (e.g., through the Food and Drug
Administration (FDA) in the U.S.), but rather are
indicated based on clinical practice guidelines after
reliability and validity of these measures have been
established in a large body of research. If future
use cases of this technique seek to apply it as a
marker or indicator for a specific condition, they
may seek that the U.S. FDA officially declare it as
a biomarker of the condition.
8",no ethical concerns,,no ethical concerns,2023,"statement, concerns, advice"
1126,"We use several different software tools in our ex-
periments. These tools as well the English dataset
are publicly available and we do not see any eth-
ical issues in using them. In addition, we clearly
reference the papers and other sources for the tools
used. We create the VieSum dataset ourselves.
Our paper’s work depends on using previously
published approaches to abstractive summarization.
We clearly give credit to the authors of these ap-
proaches by citing original sources.
This paper focuses on abstractive summariza-
tion of longer documents. There is potential for
high quality abstractive summarizers to be misused.
For example, students if/when given an assignment
to summarize/review papers/articles may use suchsummarizers to automatically write reviews and
claim them as their own. However, we believe ab-
stractive summarizers for long documents have not
achieved this level of sophistication at this time.
",misuse,,misuse,2023,"statement, concerns, actions"
1127,"The work presented in our paper is dependent on ex-
isting open source datasets and benchmarks, includ-
ing OpenWebText (Gokaslan and Cohen, 2019),
NAS-Bench-NLP (Klyuchnikov et al., 2022), and
GLUE (Wang et al., 2019). Therefore, our work
inherently contains the ethical issues and limita-
tions present in them. However, the ethics of these
datasets and benchmark are largely unknown (de-
spite OpenWebText and GLUE being widely used),
as they were released without model or dataset
cards and their authors do not discuss the societalimpacts of their work.
In our work, we adhere to best practices for
reproducibility and descriptive statistics by suffi-
ciently documenting our experimental setup and
parameters, sharing our code and benchmark, and
conducting ablation studies. One concern is the
environmental and energy impact of creating our
NAS BERT benchmark through the computation-
ally intensive task of training of 500 unique trans-
former architectures. We decreased the environ-
mental impact of our benchmark by reducing the
size of the architectures, utilizing the more com-
putationally efficient ELECTRA scheme pretrain-
ing, and limiting pretraining to 100,000 steps. We
hope that the environmental_impact is mitigated by
openly sharing the benchmark, and the potential
for training-free NAS metrics to drastically speed
up NAS algorithms. Because metrics and NAS
benchmark presented in our work are largely for
theoretical purposes and only aid the creation of
new architectures through NAS algorithms, the risk
for harmful effects and uses resulting directly from
our work is minimal.
The NAS-Bench-NLP (Klyuchnikov et al.,
2022), ELECTRA (Clark et al., 2020), and the
HuggingFace implementation of ELECTRA are
released under the Apache License 2.0, which per-
mits for commercial and non-commercial use, dis-
tribution, and modification. While the contents of
the OpenWebText corpus was scraped from pub-
lic websites without consent, the packaging of the
corpus is released into the public domain under
the Creative Commons CC0 license. The creators
of OpenWebText allow individuals to submit take
down requests of their own copyrighted works
in the corpus. The Penn Treebank dataset (Mar-
cus et al., 1993) is released under the Linguis-
tic Data Consortium User Agreement for Non-
Members, which permits use of the dataset for
non-commercial research only, without distribu-
tion. In our work and the distribution of our code
and dataset, we abide by the intended use of the
code and datasets that we utilized, consistent with
the terms of their licenses. We distribute our code
under the Apache License 2.0 and our dataset un-
der the Creative Commons Attribution 4.0 Interna-
tional Public License.
",environmental_impact,,environmental_impact,2023,"statement, actions, advice"
1128,"The dataset for annotation was created from pub-
lic social media posts with all usernames, phone
numbers, addresses, and URLs removed. The re-
search was approved by an academic institutional
ethics review board. All of our work was restricted
to document-level information; no user-level in-
formation was used. According to Twitter User
Agreement, no further user content is required to
use the publicly available data.
The detection of dissonance has many beneficial
applications such as understanding belief trends
and study of mental health from consenting indi-
viduals. However, it also could be used toward
manipulative goals via targeted messaging to in-
fluence beliefs potential without users’ awareness
of such goals, a use-case that this work does not
intend. Further, while we hope such models could
be used to help better understand and assess mental
health, clinical evaluations would need to be con-
ducted before our models are integrated into any
mental health practice.
",misuse,,misuse,2023,"statement, concerns, actions, advice"
1129,"Our models are developed and published in order
to encourage academic research in descriptive lin-
guistics. In the future, we plan to use our method to
study the inherent non-neutrality of language mod-
els by examining the influence of training corpus
composition on the semantic representation of so-
cial meanings, as represented by cultural keywords.
Because they are built on top of an unpredictable
language model, the feature prediction methods, as
well as the models we publish, are recommended
for descriptive research only. Researchers should
take into account the potential for language models,
like language, to reflect of harmful ideologies such
as sexism, racism, homophobia, and other forms of
bigotry.
",bias,,bias,2023,"statement, concerns"
1130,"We used and cited publicly available data and li-
braries for our experiments. According to the cre-
ators of the dataset FLUTE (see Section 3.1), the
dataset does not contain any offensive context or
information that uniquely identiﬁes individuals.
Our experiment with human participants re-
ported in Section 5was carried out entirely anony-
mously and voluntarily. No personal information of
the participants can be inferred from the collected
data. The experiment is in line with the ethical reg-
ulations of the of the University of Konstanz (IRB
05/2021).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1131,"The study’s scope did not include the representation
of harm toward specific populations. The narratives
were evaluated by a clinical psychologist to ensure
that they did not contain offensive content. How-
ever, it is important to acknowledge the potential
value of further research on the representation of
harm in relation to culturally sensitive and socially
controversial topics.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, suggestions"
1132,"Prior to engaging in this research work, Ethics ap-
proval was received from the Institutional Review
Board (IRB) of the corresponding ethics board of
the University of Warwick. Ethical considerations
around the nature of user generated content (Mao
et al., 2011; Keküllüoglu et al., 2020) from online
platforms were addressed through thorough data
analysis, data sharing policies to protect sensitive
information and anonymisation of the data. Ac-
cess to TalkLife’s user sensitive data was obtained
through the submission of a project proposal and
the approval of the corresponding license by Talk-
Life. Potential risks from the application of NLP
models in being able to identify moments of change
in individuals’ timelines are akin to those in earlier
work on personal event identification from social
media and the detection of suicidal ideation. Poten-
tial mitigation strategies include restricting access
to the code base and annotation labels used for
evaluation.
",privacy,,privacy,2023,"statement, concerns, suggestions"
1133,"Hovy and Spruit (2016) list several ethical issues
in the study and application of natural language
processing, and advocate increased awareness of
possible adverse social impacts. This is especially
true in mental health care in general, and in crisis
services in particular. Linthicum et al. (2019) points
out the latent bias in the demographic composition
of a dataset, with the potential risk of excluding un-
derrepresented populations. In addition, machines
cannot understand the social meanings of some bi-
ased datapoints, such as particular language use
that could be inappropriate or offensive to particu-
lar cultural groups. When picking up these biases,
the model may run the risk of reinforcing these
prejudices if no manual check is available (Lin
et al., 2022). This is true not only for patients but
equally for clinicians. Although our model was
designed with a clinical application in mind, with
no access to the demographic information of pa-tients or clinicians due to confidentiality concerns,
the current model should not be interpreted as a
system that can be applied directly to local crisis
services without manual supervision. Instead, this
study should be seen as a test for the feasibility of
multi-task learning in a particular clinical setting.
If the model is ultimately applied to crisis services,
it still should not be allowed to run on its own or
override manual judgment, but should instead be
used as an assisting tool to better inform clinicians
in their clinical cases or training.","social_impact, bias, offensive language, inappropriate language",,"social_impact, bias, offensive language, inappropriate language",2023,"concerns, advice"
1134,"Our datasets were collected from real patients, con-
tain protected health information (PHI), and are
subject to HIPAA regulations. As a result, we took
the utmost care to maintain data integrity and pri-
vacy. First, we obtained IRB approval to access
and process the data. Second, we obtained permis-
sion and approval for all applications and libraries
used to process the data. Third, data storage and
computational experimentation was done on IRB-
approved platforms.
",no ethical concerns,,no ethical concerns,2023,actions
1135,"Our work under this study aims to comply with
core ethical standards of current state artificial in-
telligence and machine learning, including (but not
limited to) being free of known bias toward any
type or group of persons within the task at-hand.
Any testing and evaluation performed to this end
is, by nature, not exhaustive and unable to identify
every potential impact; more thorough evaluation
should be performed under any derivative works.
Our proposed methods and results, while pre-
sented truly and to the best of our ability, do lever-
age pre-trained language models, and may nonethe-
less inherit any fundamental biases and factual fail-
ures of the parent model. Accordingly, we strongly
discourage usage of such approaches in scenarios
or systems that may have materially adverse impact,
such as:
•Legal proceedings or advice, especially with-
out adequate review from a licensed attorney
in the appropriate jurisdiction.
•Examination settings without explicit aware-
ness and approval from all administering par-
ties.
•Generating any content for public consump-
tion without explicit indication that such con-
tent was fully machine generated.
Figure 3: Comparison of the accuracy of COLIEE winners and GPT-3.5 when different approaches are used.
As such, we believe this work to be most bene-
ficial when leveraged within a human-in-the-loop
approach, where adequate knowledge exists to dis-
tinguish between valid and invalid model outputs.","bias, factual failures",,"bias, factual failures",2023,"statement, concerns, advice"
1136,"The work described in this paper, as well as the
accompanying work on Akuzipik that our team
has engaged in, has been undertaken with ongoing
discussions with rights holders in the Akuzipik-
speaking community in the village of Sivuqaq
(Gambell).
",no ethical concerns,,no ethical concerns,2023,actions
1137,"We discuss ethical issues from these aspects:
Intended Use. If the technology is function-
ing as intended, both sellers and customers of e-
commence platforms could benefit from the KG-
FLIP model. KG-FLIP could help customers to
quickly identify their desired products (e.g., by fil-
tering search results by product attribute values). It
could also help sellers by reducing their manual ef-
forts when listing new products (e.g, the platforms
can automatically recommend the attribute values).
Failure modes. In case of failure, KG-FLIP
might output inaccurate product attribute informa-
tion. Such non-factual information may harm cus-
tomers’ shopping experience. For example, the
substitute recommendation system, which may use
the incorrect product information provided by KG-
FLIP, may recommend a non-desired product to
our customers and hurt their shopping experience.
",model inaccuracies,,model inaccuracies,2023,"statement, concerns"
1138,"This study was approved by the Leiden University
Science Ethics Committee (ref. no. 2021-18). The
story corpus employed in this corpus was compiled
in close consultation with school teachers, prin-
cipals, parents, and of course children. We used
lightweight classifiers that were for our research
purposes trainable in a matter of seconds, thus re-
quire little compute. By offering all children in a
classroom the opportunity to freely tell a story and
participate, and by including schools in a variety
of areas and environments across the South and
South West of The Netherlands, we aimed to be as
inclusive in our data collection as possible.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1139,"In this section, we discuss two main ethical consid-
erations of D IFFUSION DB.
•Copyright. By using the Stable Diffusion Dis-
cord server, all users agree to the entirety of CC0
1.0 Universal Public Domain Dedication. This
includes waiving any intellectual property rights
related to any content shared on the server (Sta-
bilityAI, 2022b). All prompts and images in the
Discord server are considered to be public do-
main and can be used by anyone for any purpose.
Also, we release DIFFUSION DBunder the CC0
1.0 license (§ 2.5).
•Privacy. While it is possible that some prompts
may contain sensitive information, this is not
common because the Stable Diffusion Discord
has strict rules against writing personal informa-
tion in the prompts and has moderators in place
to remove violative messages. To further protect
user privacy, we have anonymized the usernames
of all users in our dataset (§ 2.4). Users also have
the option to remove their prompts and images
from our dataset through an online form (§ 2.5).
We provide a thorough discussion on the limitations
and broader impacts of D IFFUSION DB in its Data
Sheet (Gebru et al., 2020) (‡ A).
",no ethical concerns,,no ethical concerns,2023,actions
1140,"3D models from the ShapeNet dataset are available
for research and non-commercial purposes as well
as the LAION-5B data set. We did not collect
any personal information from any annotators. We
clearly state the intended use of our models, which
is to support human-centric interaction with AI
models in the 3D world.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1141,"This work contributes to the task of argumenta-
tive text editing, namely we explore how to revise
claims automatically in order to optimize their qual-
ity. While our work may also improve downstream
task performance on other tasks, it is mainly in-
tended to support humans in scenarios, such as
the creation and moderation of content on online
debate platforms as well as the improvement of
arguments generated or retrieved by other systems.
In particular, the presented approach is meant to
help users by showing examples of how to further
optimize their claims in relation to a certain debate
topic, so they can deliver their messages effectively
and hone their writing skills.
However, our generation approach still comes
with limitations and may favor revision patterns
over others in unpredictable ways, both of which
might raise ethical concerns. For example, it may
occasionally produce false claims based on untrue
or non-existent facts. We think, humans should
be able to identify such cases in light of the avail-
able context though, as long as the improvements
remain suggestions and do not happen fully auto-
matically, as intended.
The presented technology might further be sub-
ject to intentional misuse. A word processing soft-
ware, for example, could be conditioned to auto-
matically detect and adapt claims made by the user
in subtle ways that favors political or social views
of the software provider. Such misuse might then
not only change the intended message of the text,
but also influence or even change the views of the
user (Jakesch et al., 2023).
In a different scenario, online services, such as
social media platforms or review portals, might
change posted claims (e.g. social media posts, on-
line reviews) to personalize them and increase user
","model inaccuracies, misuse",,"model inaccuracies, misuse",2023,"concerns, advice"
1142,"Online collaborative platforms face challenging
ethical problems in maintaining content quality.
On the one hand, they need to preserve a certain
level of free speech to stimulate high quality discus-
sions, while implementing regulations to identify
editing behaviors defined as inappropriate. On the
other hand, distinguishing such legitimate forms
of regulation from illegitimate censorship, where
particular opinions and individuals are suppressed,
is a challenge of its own.
Our work is intended to support humans in dif-
ferent scenarios, including the creation or moder-
ation of content on online debate platforms or in
educational settings. In particular, the presented
approaches are meant to help users by identifying
argumentative claims in need of further improve-
ments and suggesting potential types of improve-
ments, so they can deliver their messages effec-
tively and honing their writing skills. However,
the presented technology might also be subject to
intentional misuse, such as the above-mentioned
illegitimate censorship. While it is hard to prevent
such misuse, we think that the described scenar-
ios are fairly unlikely, as such changes tend to be
noticed by the online community quickly. More-
over, the source of the used data (online debate
platform Kialo) employs thorough content policies
and user guidelines aimed at dealing with toxic be-
haviors, censorship, and discrimination. However,
we suggest that follow-up studies stay alert for such
behaviors and carefully choose training data.
","censorship, content quality, misuse",,"censorship, content quality, misuse",2023,"concerns, advice, suggestions"
1143,"This work did not involve human annotations, other
than the set of rules used as seeds (Angeli et al.,
2015; Tang and Surdeanu, 2022).
It is unlikely but possible that the automatically-
generated rules we used during bootstrapping aug-
ment some unknown biases in the unlabeled data.
In a brief analysis of the data we did not observe
any such situations. However, this potential un-
desired side effect is important and should not be
ignored in the eventual deployment of this method
in real-world applications.
",bias,,bias,2023,"concerns, advice"
1144,"We believe that future research should continue to
consider linguistic diversity and give importance
to the inclusion of underrepresented languages to
research, while promoting equitable research prac-
tices in the field. Our findings thus have the poten-
tial to contribute to developing more effective and
inclusive language-learning resources and tools for
language learners. Specifically, connecting the cur-
rently available (and L1-based) morphological ana-
lyzers to language-specific properties and learner-
language characteristics existing in L2 data, in-
cluding the improvement of their performance, can
enhance AI literacy, computer-assisted language
learning, and educational materials to meet the
unique and individualized needs of language learn-
ers with diverse backgrounds.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1145,"We experiment with a publicly available datasets of
ECtHR decisions, which has been derived from the
public court database HUDOC3. These decisions
contain real names of the parties involved with-
out any anonymization. We hence do not consider
our experiments to produce any additional harmful
effects relating to personal information.
The task of legal judgment prediction raises eth-
ical, civil rights, and legal policy concerns, both
general and specific to the European Court of Hu-
man Rights (e.g., (Fikfak, 2021) on system bias
and court caseload). The main premise of this
work is to make incremental technical progress to-
wards enabling systems to work with case outcome
information in a way that is aligned with how hu-
man experts analyze case facts through an interplay
with complex legal sources. We do not advocate
for the practical application of COC/LJP systems
by courts, but rather explore how their core func-
tionality of processing legal text can be made as
expert-aligned as possible. Our research group is
strongly committed to research on such models as
a means to derive insight from legal data for pur-
poses of increasing transparency, accountability,
and explainability of data-driven systems in the
legal domain.
We are conscious that, by adapting pre-trained
encoders, our models inherit any biases they con-
tain. Similarly, the ECtHR case collection as histor-
ical data may contain a data distribution in which
sensitive attributes of individuals (e.g., applicant
gender) may have some predictive signal for the
allegation/violation variable (see, e.g., (Chalkidis
et al., 2022c)). We believe the results we observe
3https://hudoc.echr.coe.int
in our COC experiments to not be substantially re-
lated to such encoded bias. However, legal NLP
systems leveraging case outcome information and
intended for practical deployment should naturally
be scrutinized against applicable equal treatment
imperatives regarding their performance, behavior,
and intended use.
All models of this project were developed and
trained on Google Colab. We did not track compu-
tation hours.",bias,,bias,2023,"statement, concerns"
1146,"The scope of this work is to examine the perfor-
mance of legal-oriented PLMs from a multi-facet
perspective and broaden the discussion to help prac-
titioners build assisting technology for legal profes-
sionals and laypersons. We believe that this is an
important application ﬁeld, where research should
be conducted (Tsarapatsanis and Aletras, 2021) to
improve legal services and democratize law, while
also highlighting (informing the audience on) the
various multi-aspect shortcomings seeking a re-
sponsible and ethical (fair) deployment of legal-
oriented technologies.
In this direction, we introduce new resources
covering various legal systems to build new mod-
els that better represent law and better assess their
capabilities. All newly developed and published re-
sources are based on publicly available data, most
of them scattered on several web portals.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1147,"We employ pre-trained language models and do not
train them from scratch, thus inheriting the biases
they may have acquired from their training cor-
pus. Our experiments have been carried out on a
dataset of ECtHR decisions which is publicly avail-
able as the part of LexGLUE benchmark (Chalkidis
et al., 2022b) and has been derived from the public
court database HUDOC2. Though these decisions
are not anonymized and contain the real names of
the involved parties, we do not foresee any harm
incurred by our experiments beyond making this
information available. This collection of decision
documents is of course historical data and using
it to train model may result in classifiers that ex-
hibit behavior that may be considered biased. For
example, Chalkidis et al. 2022c explores dispari-
ties in classification performance with regard to an
applicant’s gender, their age, and the identity of
the respondent state. If COC models are deployed
as part of a decision support systems, then they
of course must be screened for performance/error
differences in between groups that are to be treated
equally.
The task of LJP/COC in itself raises serious ethi-
cal and legal concerns, both in general and specific
to the European Court of Human Rights. However,
we do not advocate for the practical adoption of
LJP/COC systems by courts. Our prior work in
Santosh et al. 2022 demonstrates that these sys-
tems rely on several shallow surface-level spurious
signals that are statistically predictive but legally
irrelevant. This highlights the risk of using predic-
tive systems in high stakes domains such as law.
In the same work, we argue that models leverag-
ing the case outcome signal for analytical purposes
must be developed mindfully and with the goal of
aligning their inferences with legal expert reason-
ing. This further parallels the broader legal NLP
2https://hudoc.echr.coe.intcommunity increasingly addressing ethical aspects
of developed systems in the context of technical
research (e.g., Wang et al. 2021; Medvedeva et al.
2021, 2022; Tsarapatsanis and Aletras 2021; Leins
et al. 2020).
In this work, we use COC as a technical bench-
marking task that allows the development and study
of neural NLP models on legal text. We focus on
how to leverage dependencies on two successive
tasks (allegation identification and violation classi-
fication) based on case facts, as well as on learning
effective representations of these facts using con-
trastive learning. Our results are hence to be un-
derstood as technical contributions in pursuit of the
overarching goal of developing models capable of
deriving insight from data that can be used legally,
ethically, and mindfully by experts in solving prob-
lems arising in legal research and practice.
All experiments were carried out using Google
Colab. We did not track computation hours.
",bias,,bias,2023,"statement, actions, concerns, advice"
1148,"Broader Impacts: As noted by Ammanabrolu
and Riedl (2021), the ability to perform long-
term multi-step reasoning in complex, interactive,
partially-observable environments has downstream
applications beyond playing games. Text games
are platforms upon which to explore interactive, sit-
uated communication such as dialogue. Although
reinforcement learning is applicable to many se-
quential decision making domains, our setting is
most relevant to creating agents that affect change
via language. This mitigates physical risks prev-
elant in robotics, but not cognitive and emotional
risks, as any system capable of generating natural
language is capable of biased language use (Sheng
et al., 2021).
Intended Use: The method described in this paper
involves fine-tuning a large pretrained transformer
model. The data generated for fine-tuning was
generated by gold agents, and not collected from
human participants. The trained models are in-
tended to operate on these benchmark tasks that as-
sess reasoning capacities in navigation, arithmetic,
and other common sense competencies. Large lan-
guage models have been shown to exhibit a variety
of biases (e.g. Nadeem et al., 2021) that may cause
unintended harms, particularly (in the context of
this work) in unintended use cases.
Computation Time: Training large models can
involve a large carbon footprint (Strubell et al.,
2019), or decrease the availability of a method due
to the barriers in accessing high performance com-
pute resources. The proposed technique can reduce
the need for large models by augmenting smaller
models with more complex reasoning through sym-
bolic modules. The behavior cloning experiments
achieve strong performance with T5-base, high-
lighting the capacity of modest models that can be
run with workstation GPUs to be better exploited
for complex reasoning tasks.","bias, carbon footprint",,"bias, carbon footprint",2023,"statement, actions, advice"
1149,"We use multiple Twitter and Reddit datasets to fine-
tune our emotion model ensemble. Both these
datasets have been cleaned to remove any toxic-
ity, biases and offensive language. The annotated
French election dataset cannot be publicly released
following the terms and conditions of the project.
The data available to us for fine-tuning and evalu-
ation does not contain any personally identifiable
data and we do not have any knowledge of the
annotators behind creating this dataset. We also uti-
lize multiple pre-trained models which reduces the
carbon footprint of training models from scratch.
Further, utilization of this transfer learning method
for any new domain would not incur any training
costs as minimal fine-tuning may be required. How-
ever, the results obtained in an unknown domain
should be human evaluated before using it for any
downstream analytics task.
",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice"
1150,"We acknowledge that all experiments were per-
formed ethically and purely from an academic point
of view. Although this research revolves around
arguments from six sensitive topics and pre-trained
models, the argument generators at our end are not
explicitly trained to be discriminatory, exhibit bias,
or hurt anyone’s sentiments. Further, any generated
text does not reflect the stance of the authors. The
human evaluators were appointed and compensated
as per the legal norms.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1151,"Our system differs from generating deepfake video
contents. Viewers can distinguish the dubbed video
streams from original video streams, so it is un-
likely for others to use our system in harmful ways.
The purpose of our system is to deliver information
to viewers in their native language, not to generate
realistic videos. We do not synchronize lip with
speech or render speech with background noise
because they would not help with that goal but in-
troduce additional latency in the output. From these
two aspects, viewers can tell the dubbed streams
from original video streams. Additionally, we place
visible annotations on the output stream indicating
that it is dubbed by automatic machine translation.
",no ethical concerns,,no ethical concerns,2023,actions
1152,"All existing datasets (SuperGLUE and Wino-
grande) and models (BERT, RoBERTa, and AL-
BERT) were employed according to their intended
research focus, and our targeted probing dataset is
intended to be used for similar purposes in future
work. Because it was constructed by a combination
of automatic and manual processes by the authors,
it contains no additional information that could
uniquely identify any individuals. More broadly,
we employ causal interventions to evaluate how
models perform a challenging commonsense rea-
soning task, with the aim of building stronger links
to classic work in cognitive science and psycholin-
guistics. However, these causal interventions may
pose some risk if used adversarially to tamper with
public models or expose private information. Fur-
ther, the WSC and Winogrande datasets we use
to probe situation models have been constructed
within speciﬁc cultural settings (e.g. by NLP re-
searchers and largely US-based crowd-workers)
and are not intended to be universal or representa-
tive of situational competency: a wider diversity of
culturally-speciﬁc stimuli is needed.","misuse, data privacy",,"misuse, data privacy",2023,"statement, concerns, suggestions"
1153,"As discussed above, applications for this work
would train speakers using differences between
listener models with different weights or even ar-
chitectures. A potential risk of adapting speakersvia this approach is that the speaker might pick
up biases that one listener has that are not part of
the intended specialization—an undesirable qual-
ity. As suggested above, one possible approach to
mitigating this would be to explicitly include bi-
ased listeners, but always penalize the speaker for
the biased listeners’ accuracy. Another approach
could be debiasing the underlying (frozen) lan-
guage model (Schick et al., 2021; Meade et al.,
2022), as the speaker’s propensity for producing
offensive language is inherited from this underly-
ing model. Long-term, we believe that strategies
like those detailed by Weidinger et al. (2021) will
be necessary to mitigate risks of large language
models (and models such as ours that utilize them).
",bias,,bias,2023,"concerns, suggestions"
1154,"Language models are biased by the data used to
train them. Our fine-tuning of BERT and T5 with
the Fisher corpus potentially created biases or am-
plified some of the biases inherited from these two
base models. We acknowledge that this work has
the potential to be used to harm minorities, for in-
stance, by unfairly classifying or amplifying disflu-
encies in utterances expressed by minority groups.
We decided to delay the public release of our
models, datasets, and code used for disfluency gen-
eration until our work has gone under an entire
peer-review cycle and publicly presented to receive
as much feedback as possible.
On the other hand, we are releasing our disflu-
ency classifier, in the form of fine-tuned BERT
models and code for fine-tuning and evaluation, as
we believe these resources can be useful for the
research community while posing a much lower
risk of harmful exploitation than our disfluent para-
phraser.
","bias, misuse",,"bias, misuse",2023,"concerns, actions"
1155,"This paper focuses on the LM of a real-world V A
and as such the results cannot be exactly repro-
duced: we are not aware of any public dataset that
mimics our setup, e.g. ASR that can serve both
V A and STT applications, training data in several
languages that exceeds 6B words along with test
sets of several hundreds of thousands of words
sampled from real user data, etc. All data have
been anonymized and randomly sampled, and hu-
man transcription to create the test sets is only per-
formed from opted-in user data.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1156,"Any sort of technique where synthetic text data is
generated in an automated fashion, especially with
augmentation techniques like BT and EDA that
can add or remove random words or completely
rephrase sentences, can alter the original meaning
or interpretation of a sentence. This has some po-
tential to introduce additional unexpected biases
into the training data, particularly when biased
models are used to generate synthetic data (e.g.
a racially biased translation model). Even with im-
proved performance these biases can cause models
to make unfair judgments, which can become a
major concern in safety critical domains like in the
legal or medical fields. As a consequence, before
any data augmentation technique should be applied
to PET techniques, there has to be a consideration
of the underlying biases that can be introduced
through the methods of producing synthetic data.
",bias,,bias,2023,concerns
1157,"This paper has been thoroughly reviewed for ethical
considerations and has been found to be in com-
pliance with all relevant ethical guidelines. The
paper does not raise any ethical concerns and is a
valuable contribution to the field.
",no ethical concerns,,no ethical concerns,2023,statement
1158,"The interpretation of bias detection results is cru-
cial. For cases, where different political entities are
debatable in news media, may mislead the bias de-
tection model and removing such bias require more
flexible and tolerating approach while dealing with
such entities. Therefore, the results reported in our
work highlight the need for mitigating bias and fur-
ther research is required to investigate the biased
influence towards particular issues at various stages
of the training model.
",bias,,bias,2023,"concerns, suggestions"
1159,"This work does not present any direct ethical is-
sues. Code for TMLGA and DORi were released
by their respective authors. The datasets used to
evaluate our proposed approach are open-access,
and data characteristics relevant to our task were
described in the experimental evaluation section.
",no ethical concerns,,no ethical concerns,2023,statement
1160,"We develop methods for low-resource machine
translation. Because our models are trained on
limited amounts of data, and hence make frequent
errors, they may not be immediately useful for the
general public. However, our hope is that our work
will propel MT progress on the ten Indigenous lan-
guages we tackle.
There are also some biases in the models and
the textual data we use to train them. The datasets
we use to train our models (Mager et al., 2021)
is a translations of XNLI (Conneau et al., 2018),
which itself is derived from MultiNLI (Williams
et al., 2018). Our bilingual model for each pair is
trained on OPUS corpus that is derived from differ-
ent sources. The multilingual model mBART50 is
also trained on multiple datasets, including IWSLT,
WMT, and TED. Due to the complexity of neu-
ral models, it is hard to explicitly state how these
biases can contribute to the failure modes. How-
ever, we explicitly state the existence of sources
of potential biases to raise the awareness of the
readers.
","model inaccuracies, bias",,"model inaccuracies, bias",2023,"concerns, advice"
1161,"This dataset is constructed from a small set of
speakers where each speaker may be the only rep-
resentative of certain cross-sectional axes, and as
such, even reporting aggregate metadata may break
anonymity. While we do not distribute speaker an-
notations with the data some information is inher-
ently recoverable due to the link to the Anthology.
We nonetheless believe this data will be beneficial
to the community in order to study language pro-
cessing on technical data, and it is necessary to
have a diverse evaluation set to provide a more real-
istic and representative measure for generalization.
It is difficult and costly to construct datasets with
human-edited transcripts and translations and this
was the largest set possible to collect. Post-editors
were compensated with professional wages.
",privacy concerns,,privacy concerns,2023,"concerns, actions"
1162,"The authors have no ethical concerns relating to the
research presented in this paper.
",no ethical concerns,,no ethical concerns,2023,statement
1163,"The paper argues for a new approach to the treat-
ment of adverbs in the development of resources
and applications in NLP. We consider better un-
derstanding of language by computational models
as not posing a significant societal risk in itself.
The dataset used for the computational experiment
in Section 3 was created based on the data con-
tained in the publicly available FrameNet corpus
and, as far as we are aware, does not contain sen-
sitive elements. Implementation of our proposed
methodology has the same risks as any data-driven
approach in computational linguistics, but we as-
sume that we cannot safeguard against its possible
misuse due to its very general nature.
",no ethical concerns,,no ethical concerns,2023,statement
1164,"Given the sensitive nature of our target domain
and problem space, this technology has several
potential ethical implications and considerations.
First, creating detoxified text effectively raises the
possibility of creating extra toxified text as well
– our system can produce more toxic text which
could be used to produce a falsified perspective
on a community. Increasing perceived authenticity
also increases potential for misuse, as many cur-
rent methods for detecting AI-generated text may
be thwarted by these methods. Additionally, our
measure of toxicity is currently limited by an exter-
nal system – these libraries are often slow to update
to current events, memes, and new language that
can be used in a toxic manner. However, the ability
to tune an LLM for a particular community could
conversely provide a means to learn such patterns
in language use.
Overall, we believe the potential for this work
to aid in generating constructive discussions out-
weighs the potential harms from its misuse.
Posts from Reddit were automatically deidenti-
fied, and work was performed with approval from
our institution’s IRB.
","toxic content, misuse",,"toxic text, misuse",2023,"concerns, statement"
1165,"This paper presents a new task for machine compre-
hension of user manuals. Although the user man-
uals and FAQs involved in the task are collected
from an e-commerce company, they are designed
for normal users and have been widely used by
the public for some time. We also have carefully
checked these data to make sure they don’t contain
any personally identifiable information or sensi-
tive personally identifiable information. Thus, we
believe there are no privacy concerns.
All user manuals and FAQs are reviewed at least
three times by the company’s staff before being
released to the public. Besides, we have been au-
thorized by the company to make OHO publicly
available for academic research. Thus, we believe
the dataset doesn’t contain any harmful information
and is qualified for distribution.
The annotators of OHO consist of CSRs, a post-
doc, and undergraduate students. As the dataset is
about user manuals and the job is to answer ques-
tions about the user manuals, we believe there are
no physical or mental risks to the annotators.
",no ethical concerns,,no ethical concerns,2023,statement
1166,"The development of GATE Teamware 2 as an an-
notation tool was in line with the ethical clearances
of the respective research projects that provided
the funding. Ethical approvals for each annotated
dataset and the respective volunteer annotators re-
cruited for that project may need to be sought in
addition, in line with the ethical guidelines of the
user’s institution and any data protection and pri-
vacy laws that apply to the managing organisation
and to the annotators if they are located in a differ-
ent jurisdiction. For example, for the authors’ own
annotation projects, individual ethical approvals
have been obtained via the standard University of
Sheffield research ethics process, and it is envis-
aged that similar processes would be followed by
users at other institutions.
A limited amount of personally identifiable infor-
mation, namely an email address and username, is
collected from each annotator who registers an ac-
count with a given installation of GATE Teamware
2. This is made clear to users in a privacy pol-
icy prior to registration, and provision is made to
remove all personal identifiers from the user’s ac-
count if they choose to withdraw from participat-
ing in annotation projects on that installation. The
annotations they have performed so far are not nec-
essarily deleted6, and will remain linked to the
dormant account, so that it is still possible to de-
termine whether disparate sets of annotations were
created by the same individual, but the username
and email address on their account profile are re-
placed by anonymous placeholders so the account
is no longer linked to an identifiable person.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1167,"In our work, we present experiments on the German
Sign Language (DGS) that are part of a broader re-
search aiming to provide equal access to language
technology for sign language users. Nevertheless,
the fact that the majority of the researchers in NLP
are hearing people entails the risk of developments
that are not in accordance with the will of the re-
spective communities, and therefore it is required
that every research step takes them in constant
consideration. In our broader research we have
included members of the Deaf/deaf and hard of
hearing communities as part of the research team,
consultants and participants in user studies and
workshops and we have been in co-operation with
related unions and communication centers.
The fact that we are performing experiments on
glosses, known to be inferior to the full linguistic
capacity of the sign languages, should be seen as a
methodological tool to aid further research.
The Public DGS corpus is provided under a lim-
ited license for linguistic research (Schulder and
Hanke, 2022), prohibiting any further commercial
usage. Any further usage of relevant artifacts from
our work should respect the license of the origi-
nal corpus. Removal of information that names or
uniquely identifies individual people or offensive
content was not deemed necessary. In the Pub-
lic DGS corpus, participants provided consensus,
whereas the content was carefully curated. The
PHOENIX corpus does not pose any relevant risk
because the content (weather forecasts) does not in-
clude any personal information. All other datasets
used have been published with open or public do-
main licenses. Since our work does not use videos
of SLs, there should be no ethical concerns regard-
ing processing of human faces.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1168,"Because of the aforementioned limitation regard-
ing positive predictive power, there is always a risk
with research on social biases that it can give prac-
titioners a false sense of security. It is absolutely
possible to evaluate on our corpus and get no bias,
and still end up causing harm to racial or gender
demographics, since they do not cover all biases or
all domains. This should be kept in mind whenever
applying this research.
",bias,,bias,2023,"concerns, advice"
1169,"Our paper primarily aims to enhance the training
method for constructing retrieval-based dialogue
systems that exhibit improved effectiveness. The
training corpora we utilize, such as the Ubuntu Cor-
pus and the response selection track of the Dialog
System Technology Challenge, are openly acces-
sible and do not give rise to any privacy concerns.
Furthermore, the algorithm we propose is designed
to be free from ethical or social bias, ensuring fair-
ness and unbiased performance.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1170,"Our paper centers around the document ranking
task, a well-established and widely applicable prob-
lem. In conducting our research, we have exclu-
sively utilized queries and documents sourced from
open public datasets, with proper citation and ad-
herence to licensing agreements. We have taken
great care to ensure that our experiments have no
bearing on privacy security, discrimination, or bias.
We affirm that our work aligns with ethical princi-
ples and regulations, and it does not infringe upon
any established ethical codes or guidelines.
",no ethical concerns,,no ethical concerns,2023,statement
1171,"In this paper, we propose ASE, an algorithm to
accelerate multi-turn response selection by progras-
sively selecting and eliminating unimportant to-
kens. The training corpora including the Ubuntu
Corpus, the Douban Corpus and the E-commerce
Corpus used for evaluating our framework are pub-
licly available and don’t pose privacy issues. The
algorithm that we propose does not introduce ethi-
cal or social bias.
",no ethical concerns,,no ethical concerns,2023,statement
1172,"This work aims to propose a technical method to
utilize phonetic knowledge more effectively forChinese Spelling Correction, which does not in-
volve ethical issues. The datasets used in this work
are all publicly available.
",no ethical concerns,,no ethical concerns,2023,statement
1173,"In this section, we would like to discuss the ethi-
cal concerns of our work. Our proposed methodUMSE is a unified model for multi-scenario sum-
marization evaluation and is designed to help hu-
mans efficiently evaluate summaries. And the sen-
sitive information is masked while constructing the
training data from CNN/DailyMail dataset.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1174,"The paper has proposed a question-answering
model, which is intended to answer factoid open-domain questions. The model-predicted answers
still have a considerable amount of misinformation.
Besides, the proposed models rely on pre-trained
question-answering models, which are trained on
large-scale web data that is known to contain biased
or discriminatory content.
","bias, misinformation",,"bias, misinformation",2023,"statement, concerns"
1175,"The paper proposes a knowledge-grounded dia-
logue system to generate a response using external
knowledge. The intended use of this system is to
perform chit-chat with the user on topics such as
books and movies. The system is developed us-
ing large pre-trained language models (i.e., BART),
who are trained on large-scale web data known
to contain biased or discriminatory content. The
datasets (i.e., WoW, Holl-E, CMU_DoG) that we
train on also include subjective knowledge (com-
ments on movies) that may express the bias of the
writers. Although the system is able to reduce the
hallucination of response compared to end-to-end
models, the outputs from our system may still con-
tain non-factual information and should not be con-
sidered as advice for any critical decision-making.
","bias, nonfactual information",,"bias, non-factual information",2023,"concerns, advice"
1176,"As our goal of this study is to challenge current
problem setups of DocRE, we heavily rely upon ex-
isting well-known datasets, models and NLP tools.
We only claim that our findings may hold on simi-
lar datasets or domains. We acknowledge the risk
of generalizability of our findings on other privacy-
sensitive datasets or specific domains. In general,
we suggest that practitioners repeat all experiments
following our procedures when using other cor-
pora.
",privacy,,privacy,2023,"concerns, advice"
1177,"To ensure data privacy and protection, we use pub-
licly available benchmark datasets for hate speech
detection and do not collect any personal or sen-
sitive information. Additionally, we acknowledge
that the detection of hate speech can be a sensi-
tive topic; therefore, we report the results of our
experiments in a responsible and appropriate man-
ner. Lastly, we acknowledge that language models
trained on large datasets have the potential to per-
petuate bias and discrimination, and we strive to
3https://hackingsemantics.xyz/2023/
closed-baselines/mitigate these risks by carefully selecting and eval-
uating our models and verbalizers to ensure fairness
and impartiality.
","bias, discrimination, hate speech",,"bias, discrimination, hate speech",2023,"concerns, actions"
1178,"Evaluations of gender bias in language technolo-
gies need a holistic outlook, such that they evalu-
ate the harms of stereotyping, erasure of identities,
misgendering, dead-naming, and more. Our work
attempts to address one specific type of misgen-
dering harm and builds a framework that estimates
the extent of misgendering propagated by a model
under specific settings. We hope our framework
enables model evaluations that are not exclusion-
ary of gender identities. However, the absence of
measured misgendering by this paradigm is not evi-
dence of no misgendering or other gender harms at
all. For responsible model deployment, it is imper-
ative that they be appropriately scrutinized based
on the context of usage.
","bias, misgendering",,"bias, misgendering",2023,"statement, concerns, advice"
1179,"DEER relies on the pre-trained language models,
e.g., RoBERTa and XLM-R, which may inherit
problematic biases. However, we only use these
models as a backbone rather than using their pre-
dictions. DEER is also a task-specific method that
performs the fine-tuning process at the task-specific
dataset, which also makes the generated result de-
pend on the input of the dataset and reduces the
inherent bias.
",bias,,bias,2023,"concerns, actions"
1180,"To accurately represent their respective cultures,
this paper utilized three publicly available hate
speech datasets in Korean, English, and Arabic,
with detailed descriptions provided in Section 3.
Regarding user privacy, the Korean dataset
KOLD and the Arabic Hate Speech dataset (AHS)
implemented measures to protect user privacy by
masking usernames and URLs with their masking
tokens. However, the English dataset SBIC did not
anonymize texts containing usernames and URLs.
To protect user privacy, we anonymized the texts
by removing these two attributes.
We relied on multiple resources to comprehend
comments from various cultures to avoid any bias
resulting from a limited understanding of different
cultures. This approach helped ensure that our lack
of cultural knowledge did not affect the analysis
of cultural differences. Our analysis primarily re-
lied on numerical values from model predictions,
and we inspected samples to provide better expla-
nations for the models’ performance based on the
quantitative results. This approach allowed us to
minimize potential biases resulting from cultural
misunderstandings and contribute to more cultur-
ally sensitive research practices.
8 ",no ethical concerns,,no ethical concerns,2023,actions
1181,"This research was approved by the Institutional
Review Board (IRB) at the University of Illinois
Chicago. Our primary data source was patient.info ,
for which the terms and conditions allow public
access to forum posts.5As outlined in §3.1 and to
protect privacy, we manually anonymized our data
instances by removing any usernames or other iden-
tifying metadata, and replaced any names written
directly within the post text with generic name to-
kens. Annotators were compensated for their work
through paid internships and assistantship positions
at a competitive rate for the cost of living in our
area. We make our expanded dataset available upon
request via email, following IRB protocol.
We intend for our dataset expansion and the pro-
posed methods to be used as a tool to analyze the
linguistic trends and other language behaviors as-
sociated with medical self-disclosure in online set-
tings. Our experiments closely follow this intent,
providing novel insight into the influence of dif-
ferent pre-training tasks and stylistic domains on
the ability of our models to recognize possible and
clear cases of medical self-disclosure. When the
technology is being used as intended and function-
ing correctly, we anticipate that it may be of value
to numerous downstream applications, primarily as
a data analysis tool or as an avenue for providing
information. When the technology is being used as
intended but giving incorrect results, its value may
decrease (since researchers or clinicians using the
tool may need to discard their insights or may fail
to replicate findings in subsequent experiments).
5https://patient.info/
terms-and-conditionsA potential misuse of the technology would be
to use the trained models to identify social me-
dia users disclosing medical information and apply
targeted advertising or messaging to those users.
Since neither our dataset nor our models are de-
signed to recognize or predict specific medical con-
ditions, we note that the extent to which they could
be used for these purposes is limited. We do not
condone this use of the technology, and we will
monitor citations and uses of our dataset to ensure
that others are using our data and models for their
intended purpose.
",misuse,,misuse,2023,"statement, concerns, advice"
1182,"Beyond the proposed method that helps correctly
identify the stance towards specific targets, it is
very important to consider the ethical implications
of stance detection systems. Since stance detection
systems could automatically collect and aggregate
the topical stance for a specific target, these systems
may have significant impact on decision-making.
Algorithms are not perfect, and thus a potential
harm is that these systems may make incorrect pre-
dictions and further mislead the decision-making.
Researchers should be aware of potential harms
from the misuse of stance detection systems, and
should respect people’s privacy during the data col-
lection.
","misuse, incorrect predictions, misleading desicion-making",,"misuse, incorrect predictions, misleading desicion-making",2023,"concerns, advice"
1183,"Our dataset does not provide any personally identi-
fiable information. Microblogs are collected using
generic keywords instead of user information as
queries, therefore our dataset does not have a large
collection of microblogs from an individual user.
Thus our dataset complies with Sina Weibo’s infor-
mation privacy policy.
",no ethical concerns,,no ethical concerns,2023,statement
1184,"Beyond the proposed two-step framework that
helps collect the stance in the wild, it is very impor-
tant to consider the ethical implications of stance
detection systems. Since stance detection systems
could automatically collect and aggregate the topi-
cal stance for a specific target, these systems may
have significant impact on decision-making. Algo-
rithms are not perfect, and thus a potential harm
is that these systems may make incorrect predic-
tions and further mislead the decision-making. Re-
searchers should be aware of potential harms from
the misuse of stance detection systems, and should
respect people’s privacy during the data collection.
","misuse, incorrect predictions, misleading desicion-making",,"misuse, incorrect predictions, misleading desicion-making",2023,"concerns, advice"
1185,"Given that the created dataset is derived from social
media and is focused on a sensitive mental health
topic, we follow various ethical concerns regard-
ing user privacy and confidentiality as inspired by
(Benton et al., 2017a) to access and analyze the
data. We adhere to the data usage privacy as pro-
vided by Twitter and Reddit to crawl the public
profiles of their users. To ensure that we maintain
the user’s privacy, we anonymized the user profiles
prior to the annotations, and we did not keep any
meta-data information that would disclose the user.
Further, we did not make any efforts to interact,
deanonymize, or connect users on their other social
media handles. The ethics review board approved
the study under Human Subjects Research Exemp-
tion 4 because it is limited to publicly available
social media posts. We believe that the created data
would be highly beneficial to the community and
to avoid any misuse (Hovy and Spruit, 2016), we
will share the data with other researchers who will
not deanonymize any of the users and will follow
all the ethical considerations as established in this
study.",no ethical concerns,,no ethical concerns,2023,"concerns, actions"
1186,"This research was guided by a broad range of ethi-
cal considerations, taking into account factors as-
sociated with fairness, privacy, and intended use.
Although many of these are described throughout
the paper, we summarize those that we consider
most critical in this section. It is our hope that by
building a holistic understanding of these factors,
we develop improved perspective of the challenges
associated with the study of low-resource health-
care problems and the positive broader impacts that
they may create.
Data Privacy and Fairness. This research was
approved by the Institutional Review Board at the
University of Illinois Chicago. Access was granted
for all datasets used in this research, and our use
is governed by approved protocols unique to each
dataset. DementiaBank, ADReSS, and the Carolina
Conversations Collection are all publicly available
following access request protocols specified by
their governing organizations. We refer readers
to the citations throughout this work if they are
interested in obtaining access to this data. We are
unable to share it directly, although we can share
our processing scripts and other code to facilitate
reproducibility of our work by others.
ADRC is a privately-held dataset collected in col-
laboration with clinical partners under a rigorous
set of guidelines governed by a separate, approved
Institutional Review Board protocol at the Univer-
sity of California San Diego. This dataset will
eventually be released, following further manual
review to ensure full de-identification, but it cannot
yet be released at this time. The data is currently
stored on a password-protected server under VPN
protection. To maximize reproducibility of our
work by others unable to immediately gain access
to this dataset, we limit the use of this dataset to a
small set of experimental conditions (specifically,
those using M ULTI AUGMENT ).
Intended Use. Automated models for dementia
detection from spoken language present potential
benefits in real-world scenarios: they offer opportu-
nity to expand healthcare access, minimize cost of
care, and reduce caregiver burden. However, they
may also pose risks if used in unintended ways. We
consider intended use of the work reported here to
extend to the following:
•People may use the technology developed in
this work to study language differences be-
tween individuals with and without dementia,
as a way of building further understanding of
the condition.
•People may use the technology developed in
this work to further their own research intolow-resource NLP tasks, including those asso-
ciated with this and other healthcare problems.
•People may use the technology developed
in this work to build early warning systems
to flag individuals about potential dementia
symptoms, provided that the technology is not
misconstrued as an alternative to human care
in any way.
Any use outside of those listed above is con-
sidered an unintended use. To safeguard against
unintended use of our work, we remind readers
that dataset access must be granted through the ap-
proved channels by the creators of the respective
datasets used in this work. This may include pro-
cesses ranging from email request to full review
and approval by local and external Institutional Re-
view Boards. We reiterate our caution against using
any findings from this paper to build systems that
function as intended or perceived replacements for
human medical care.",no ethical concerns,,no ethical concerns,2023,"statement, concerns, actions, advice"
1187,"This research was guided by a broad range of ethi-
cal considerations, taking into account factors asso-
ciated with environmental_impact, equitable access,
and reproducibility. We summarize those that we
consider most critical in this section. It is our hope
that by building a holistic understanding of these
factors, we develop improved perspective of the
challenges associated with reproducibility studies
and the positive broader impacts that improved re-
producibility standards may promote.
environmental_impact. In this work, we seek
to study the complex and murky relationship be-
tween experimental conditions and experimental
outcomes. To address research questions surround-
ing this relationship, we conduct many experimen-
tal runs to replicate the same models across an
extensive set of variable conditions. Although nec-
essary for justifying our claims, a downside of
this process is that it may produce environmen-
tal harm. One might argue that the advantages of
assurance that the ‘true’ evaluation score is found
do not outweigh the disadvantages of repeatedly
running models that are known to produce large car-
bon footprints (Strubell et al., 2019). We attenuate
this risk by controlling for as many variables al-
lowable (e.g., data set and architectural variations)
while still fostering robust study of our core ques-
tion, to minimize the number of experimental runs
required.
Equitable Access. A concern closely related to
environmental_impact is that of equitable access
to this line of research. By studying a problem
that requires many repeated experimental runs with
subtle variations, we may exclude disadvantaged
researchers from performing meaningful follow-up
studies, since they may not have the requisite re-
source bandwidth (Bommasani et al., 2021, §5.6).
However, although reproducibility studies them-
selves may pose a barrier to entry for researchers
with limited access to compute hardware, the in-
novations resulting from these studies (e.g., im-
proved community standards for reproducibility
of reported results) may stand to greatly benefit
marginalised researchers, by minimising the po-
tential for bottlenecks in attempting to perform
impossible and costly replications to establish per-
formance baselines.
Reproducibility. To ensure reproducibility of our
own work, we report all experimental parameters,
computational budget, and computing infrastruc-
ture used. We discuss our experimental setups in
depth, as they are the primary focus of this study.
We report descriptive statistics about our results to
enhance transparency of our findings, and we report
all implementation settings (e.g., package version
number) needed to successfully replicate our work.
Although reproducibility studies are not specified
as an intended use of the referenced systems (Nisioi
et al., 2017), this use is compatible with the original
access conditions and the authors have consented to
the paper’s use in numerous reproducibility studies
since its publication (Belz et al., 2022b).",no ethical concerns,,no ethical concerns,2023,actions
1188,"The datasets used in this study are publicly avail-
able, and we strictly follow the ethical implications
of previous research related to the data sources. It is
important to note that the content of these datasets
does not represent our opinions or views.
",no ethical concerns,,no ethical concerns,2023,statement
1189,"The datasets we used in experiments are publicly
available. In our work, we provide a comprehen-
sive analysis and present data augmentation strate-
gies specifically to address keyphrase generation in
purely resource-constrained domains. We do not
expect any direct ethical concern from our work.
",no ethical concerns,,no ethical concerns,2023,statement
1190,"TheLATEXSOLVER system presented in this pa-
per aims to partially automate the process of con-
verting problem descriptions in LaTeX to model
codes, thereby helping OR experts build optimiza-
tion models more efﬁciently. As the system’s input
and output are transparent to users and users can
control the model-building procedure by interact-
ing with the system, the harm to users resulting
from the errors produced by the system is limited.
However, our system may be used in certain cir-
cumstances considered sensitive or critical, such as
power grid or ﬂights scheduling. In such cases, thesystem should be used with caution and the mod-
eling process should be investigated by domain
experts. Additionally, given the historic applica-
tion of operations research in tactical military op-
erations, it is critical to understand the potential
negative impact of misapplying this technology to
society. Therefore, users of our system must be
aware of any ethical concern come with military
applications of this technology.
","misuse, social impact",,"misuse, social impact",2023,"statement, concerns"
1191,"We have analyzed the factuality of generated text
in relation to the abstractiveness of the source texts;
we have also proposed new metrics that let re-
searchers compare the factuality of different gen-
erative models. As such, we consider our work a
contribution toward text generation methods that
make fewer factual mistakes and become therefore
more reliable and responsible. However, any ad-
vance in text generation methods can be used by
bad actors to cheaply generate misleading or harm-
ful texts.
We hired annotators on the Mechanical Turk plat-
form to judge machine-generated summaries. Our
first ethical consideration with respect to this data
collection is fair and prompt pay for the work of
the annotators. We describe in Appendix C thatwe paid all human subjects a fair average pay of
$12.50 USD per hour, based on observed median
time spent per HIT. As described (Section 3.1),
we automatically approved the annotators’ work
promptly and paid bonuses as appropriate. The an-
notators’ privacy and confidentiality were respected
at all times.
",misuse,,misuse,2023,"statement, concerns, actions"
1192,"Developing AI for security purposes always come
with its ethical considerations. In this case, the
system performs law enforcement and fight against
piracy, which are commonly assessed as noble, eth-
ical deeds. As the application specifically targets
maritime trafficking, the risk of misuse is reduced
(i.e. it cannot be used to target individuals).
This system relies on third party sources of data.
As a consequence, the data processing roles are
clearly and contractually established between the
providers and the customer of this system, decreas-
ing privacy risks. No personal data is required by
the system; personal public data may be handled
from the press and from the AIS information (typi-
cally, the name of the captain).
The final result of the system is to gather and
aggregate a complete picture of the risk level of a
ship, to help an operator. The system may be used
to prioritize the effort to review the documents and
cargo of a ship, but cannot be used to authorize or
forbid a ship’s entry – this remains the decision of
the operator.
",no ethical concerns,,no ethical concerns,2023,actions
1193,"Our motivation is to improve access and affordabil-
ity to online pharmaceutical services in emerging
markets such as India through accurate and easy
digitization of medical prescriptions. Given the
sensitive nature of medical prescriptions and the
associated health impact, it was critical to pay at-
tention to multiple aspects that we discuss below:
Secure and Privacy-safe Data Collection: Pri-
vacy of customer data is paramount to us. Hence,
prior to modeling, we remove customer, facility
and practitioner information by obscuring the re-
gions containing personally identiﬁable ﬁelds such
as names, phone numbers, and addresses, which
are identiﬁed using security-certiﬁed AWS services
(AWS Comprehend, AWS Textract).
bias: A key limitation of the existing medi-
cal NER models is their poor performance on non-
US and EU prescriptions due to bias in the training
data, which is almost exclusively based on US-
EU centric medical content and vocabulary. In
our approach, we have deliberately chosen to have
explicit dependence on aspects that vary across geo-
graphical regions (e.g., medical catalog), which en-
hances the applicability of our approach. To further
limit the bias and minimize distributional
differences between training and production set-
tings, we have trained our models on prescription
images that are randomly sampled from customer
uploads. These often include low resolution and
improperly positioned images. In future, as the
scope of deployment changes, we plan to period-
ically retrain the model with training images by
sampling from the production data.
Health Safety: One of the primary concerns in
prescription digitisation is the impact of errors on
patient health and adherence to health regulations.
To alleviate adverse outcomes, we have multiple
guardrails. First, we present the top three sugges-
tions along with scores for each medication for
two-fold review by customer and pharmacist. Sec-
ond, to avoid prescription abuse (e.g., manipulation
of quantities, prescription reuse) and comply with
regulations, there are additional checks based on
the prescription date, patient purchase history, and
recommended limits on medication quantities.
Usage for a Limited Scope: Our proprietary sys-
tem has been trained for a speciﬁc-use case, i.e.,
prescription digitization with acceptable perfor-
mance on primarily English printed prescriptions
for India region. We plan to use the model withinthis limited scope and expand usage only after ade-
quate benchmarking. To limit the risks of misuse,
we do not plan to release this system externally.

","bias, safety",,"bias, safety",2023,"concerns, actions, suggestions"
1194,"In any safety-critical context like spacecraft opera-
tions, there is an inherent risk associated with the
use of automatic methods supporting human oper-
ators. The transparency of the predicted programs
could mitigate this issue as it allows even for an
engineer with limited knowledge about the under-
lying query method to interpret the program to
some degree. In any case, the developed systems
might support human analysis and decision mak-
ing by decreasing workload, but cannot replace it.
As mentioned before the DISCOS KB can be ac-
cessed after creating a user account. We plan on
publishing the created question-program paris and
trained models online in accordance with ESA’s
guidelines.
",safety,,safety,2023,"concerns, actions"
1195,"Our research motivation is to address the inequities
in language resources and AI systems for multi-
lingual societies such as India. The primary con-
tribution of our work is a new modeling approach
CoMix, which is especially designed to leverage
existing pretrained models with moderate compu-
tation so that it is accessible to a wider community
and does not create an adverse environmental im-
pact. We also created two new Hinglish datasets
for out-of-domain evaluation (HooD), which we
described in detail in Section 5.2. There are no pri-
vacy or intellectual property rights associated with
either of these datasets. We will open-source HooD,
our models and code in future post organizational
approval. Human translations and evaluations re-
ported in the paper have been done by professional
annotation teams and are reflective of typical per-
formance. Similar to other large language models,
our CoMix model also encodes biases in the origi-
nal training corpus and the domain constraints used
as supervision. While the performance might be
acceptable for natural language understanding, it is
important to have guardrails while using the models
directly for natural language generation.
",bias,,bias,2023,"statement, concerns, actions"
1196,"As this work uses real-world patients’ clinical data
and molecular proﬁles, which may raise concerns
about data privacy and conﬁdentiality. We ensure
that all the patients’ data is de-identiﬁed and pro-
tected from unauthorized access and use. The pub-
lic patient data6was approved by the Memorial
Sloan Kettering Cancer Center (MSKCC)7institu-
tional review board for scientiﬁc use. Researchers
have ensured that they obtain proper ethical ap-
proval and informed consent from patients before
using their data. Even though this is a dataset that
has been carefully curated to prevent the negative
impact brought by human bias, there maybe ex-
isting a risk of introducing bias into the clinical
cohort of data we analyze, particularly in the selec-
tion of patients and the choice of clinical features
and molecular proﬁles. Additionally, the use of pre-
dictive models to guide clinical decision-making
might raise concerns about fair access to healthcare.
We hereby ensure that the use of predictive models
does not result in the inequitable distribution of
healthcare resources and that patients from all so-
cioeconomic backgrounds have equal access to the
possible care. This study uses natural language
processing and machine learning algorithms to pre-
dict disease prognosis, which may raise broader
ethical considerations related to the responsible use
of technology in healthcare. We ensure that the use
of all approaches discussed in this work is guided
by general ethical principles, such as transparency,
accountability, and patient-centered care.
Even though we focus on relatively large scale
language models in this work, our ﬁnetuning strat-
egy only requires a considerably small amount of
computation as only the encoder part needs to be
ﬁnetuned. In practice, the single linear layer ﬁne-
tuning can be obtained in about 2 hours on a ma-
chine with single Nvidia A10 GPU; training com-
pletes within 5 hours on a machine with one Nvidia
A10 GPU for another transformer encoder with
a depth of 6 and dimensionality of 768. All the
pretrained language model weights are publicly
available ( e.g., huggingface).
","bias, fair access, responsibility ",,"bias, fair access, responsibility ",2023,"concerns, actions, advice"
1197,"All the work done and discussed in this paper meets
and upholds the ACL Code of Ethics.
",no ethical concerns,,no ethical concerns,2023,statement
1198,"This NLP research study was designed and carried
out with strict adherence to ethical principles and
guidelines. The study was reviewed and approved
by our company’s research lead prior to the sub-
mission. The study followed the ACL conference’s
guidelines on the use of language data. The re-
searchers take full responsibility for ensuring the
ethical conduct of this study and are committed to
upholding the highest standards of ethical research
practices in NLP.
",no ethical concerns,,no ethical concerns,2023,statement
1199,"The techniques for inference speedup presented in
this work are fully methodological. Hence, there
are no direct negative social impacts. However,
as the models automatically generate the images,
they may have some negative impacts, such as the
generation of toxic content and the existence of
social biases. We suggest that the produced models
should not be used to generate offensive or inappro-
priate images for people intentionally. Users should
carefully deal with the potential risks by filtering
out these images when the models are deployed
online.
","bias, toxic content",,"bias, toxic content generation",2023,"statement, concerns, advice"
1200,"We expect that our KOSBIcan considerably con-
tribute to enhancing the safe usage of LLMs’ ap-
plications by reducing risks caused by social bias.
Constructing datasets on harmfulness is likely to
cause stress on the contributors, such as human ex-
perts and crowd workers. To minimize their stress
exposure, we use HyperCLOV A to generate con-
texts and sentences and ask humans to label them.
Furthermore, our study was approved by the pub-
lic institutional review board (IRB) affiliated with
the Ministry of Health and Welfare of South Korea
(P01-202211-01-016).
","bias, stress",,"bias, well-being of contributors",2023,"concerns, actions"
1201,"We do not envision any ethical concerns with the re-
search presented here. No customer data is released
or presented in this paper, and even our internal
data sources are fully de-identified and contain no
customer Personal Identifiable Information (PII).
",no ethical concerns,,no ethical concerns,2023,statement
1202,"Text generation by nature entails a number of ethi-
cal considerations when considering possible appli-
cations. The main failure is when the model gener-
ates text with undesirable properties (bias etc.) for
training the classiﬁer but these properties are not
present in the original training data.
Because our model converges and learns to gen-
erate data close to the underlying source material,
the above considerations, in our approach, are neg-
ligible. As a result, the generated text may be harm-
ful if users of such models are unaware that such
issues appear on their training data or if they fail
to consider them, e.g., by selecting and evaluating
data more carefully.",bias,,bias,2023,"statement, concerns"
1203,"E-commerce has been increasingly popular these
years. Nonetheless, a big amount of people cannot
benefit much from it because most E-commerce
websites only support a few major languages. De-
ploying an xPQA system can have a broad impact
across a wide range of non-English speakers to as-
sist them in their shopping experience. With a well-developed xPQA system, we only need to maintain
comprehensive product information in one major-
ity language, but allow non-English speakers easily
get access to the product information. This can
significantly reduce the maintenance cost and ben-
efit the democratization of AI. Nevertheless, there
are two major caveats before deploying a safe, re-
liable xPQA system: (1) The answer generator
needs to be fairly evaluated by humans in terms of
faithfulness. While answer generation can greatly
improve user-friendliness, it also brings potential
risks of providing false information; (2) The users
should be well noticed that the provided answer
is drawn from the opinion of a single customer
or other sources. It cannot reflect the opinion of
the vendor, or seller nor imply any trend from the
public.","faithfulness, misinformation, bias",,"faithfulness, misinformation, bias",2023,"statement, concerns, advice"
1204,"IU Chest X-ray dataset’s authors used appropriate
techniques to de-identify the text reports. Data is
anonymized; hence our model will not disclose
information about the patient’s identity.
",no ethical concerns,,no ethical concerns,2023,statement
1205,"Anonymized radiology reports are used to build
KGs. The data itself is anonymized; hence, our
system does not reveal any patient-specific identity.
ML technologies for this kind of work have the
potential to gradually become the norm but will
always remain as assistive tools for medical prac-
titioners. Hence, while ML technologies of this
kind often veer towards the norm, the envisioned
assistive nature of this technology, where humans
will always have oversight, will address this issue.
We have evaluated the outputs of the KG-BART
model using automated metrics, but to contextual-
ize the results, a human evaluation metric would
have been useful; however, we left this work for
the future. An MVP (minimum viable product) has
been made available to the beta users (practicing
radiologists) for testing and evaluation. Manual
evaluation will be done by considering beta users
feedback.
",no ethical concerns,,no ethical concerns,2023,"statement, advice, suggestions"
1206,"The IU Chest X-ray dataset’s authors used appro-
priate techniques to de-identify the text reports.
Data is anonymized; hence, our model will not dis-
close information about the patient’s identity. The
MIMIC-CXR dataset does not contain any infor-
mation related to the patient’s identity, like name,
age, or address. These reports are also anonymized.
Data itself does not reveal the patient’s identity;
hence, our model also does not reveal the patient’sidentity.
",no ethical concerns,,no ethical concerns,2023,statement
1207,"The IU Chest X-ray dataset’s authors used appro-
priate techniques to de-identify the text reports.
Data is anonymized; hence, our model will not dis-
close information about the patient’s identity. The
MIMIC-CXR dataset does not contain any infor-
mation related to the patient’s identity, like name,
age, or address. These reports are also anonymized.
Data itself does not reveal the patient’s identity;
hence, our model also does not reveal the patient’sidentity.
",no ethical concerns,,no ethical concerns,2023,statement
1208,"The datasets used in this paper for the training and
evaluations of the pre-trained models, DeepSpin
models, and fine-tuned models have been extracted
from various previous articles and open-access
repositories, therefore, we abide by the ethical rules
by citing the original authors of each dataset. On
the other hand, the annotated Turkish and Quechua
data that were constructed by us for the develop-
ment of the DeepSpin models will be presented in
a forthcoming paper for public use. In addition, we
encourage authors who use the resources in this ar-
ticle to cite the original sources. Finally, we would
like to note that one of the authors of this paper has
a long history of working with resource-poor syn-
thetic languages, especially Quechua, which allows
us to better understand the problems and concerns
of the Quechua-speaking communities.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1209,"We built the system with the intended use for fact-
checking support, providing rationales at various
level to user for ease of understanding. These ra-
tionales include supporting and refuting evidences
in the corpus. We see potential misuse of system
might lie in spreading of fake news and propaganda
by e.g., automatic detection of sources, which sup-
port or refute certain claims from the narrative.
This could be followed by subsequent glorifica-
tion/discreditation of statements in these sources.
This could influence the human population, but also
poison retrieval databases of similar fact-checking
systems, influencing their decisions. Future work
in this direction, such as Du et al. (2022), needs
to study disinformation attacks and how to prevent
them.
","misuse, fake news, propaganda",,"misuse, fake news, propaganda",2023,"concerns, suggestions"
1210,"We ensured that our dataset can be made publicly
available (sentences from comments are restricted
to non-commercial use only). Since our data orig-
inates from several different domains, we gave
careful consideration to finding a balance between
copyright and data privacy regulations. Finally,
we pseudonymized text spans containing personal
information in user-generated content where nec-
essary ( tweets ,Reddit posts, comments andblogs ).
This means we replaced sensitive text with auto-
matically generated substitutes, e.g., female names
with other female names or locations with other
locations. We only release the IDs for tweets ,Red-
ditposts and comments . For blogs , we follow the
license requirements and publish the respective ref-
erence. The corpora with emails andlegal texts had
been pseudonymized already, no information on au-
thors is available. For less-privacy-sensitive text
sources, such as subtitles ,political speeches, news
andWikipedia , we report all information shared
in the original corpus, e.g., URLs. The sentences
from fiction andscience , which we collected our-
selves, are cited appropriately in order to acknowl-
edge intellectual property rights. People involved
in creating our dataset were compensated at least
following minimum wage requirements.
",no ethical concerns,,no ethical concerns,2023,actions
1211,"This study was designed to minimize its environ-
mental impact by reducing the amount of required
computational resources to run our experiments.
We are aware of the high energy consumption and
carbon footprint associated with large-scale ma-
chine learning experiments and took steps to mini-
mize these impacts.
Additionally, in this study, our objective was to
address the performance gap in languages that are
underrepresented in comparison to high-resource
languages, rather than solely striving for perfor-
mance enhancement.
",environmental_impact,,environmental_impact,2023,"statement, concerns"
1212,"We have carefully reviewed the relevant literature
to ensure that all research included in this review
has been conducted in accordance with ethical
guidelines. We have also attempted to present a
fair and accurate representation of the current state
of research on this topic. We hope that this review
will contribute to the ongoing debate about the fac-
tors impacting cross-lingual transfer performance,
with the ultimate goal of ensuring that low-resource
languages can equally benefit from the use of mul-
tilingual language models. We believe that it is
important for all languages and communities to
have equal access to the benefits and opportunities
provided by the advances in natural language pro-
cessing, and we hope that our review will serve as
a useful resource in this regard.
",no ethical concerns,,no ethical concerns,2023,statement
1213,"This paper contains examples of HS from existing
linguistic resources for HS detection and which do
not reflect the authors’ opinions.
While our purpose is to prevent and curate social
media resources from HS, the release of this dataset
might still pose a potential misuse case. However,
we still consider that effective classifiers for this
task are necessary to tackle implicit and subtle on-
line hate on scale and prevent the spreading of this
harmful content online. Our work aims at making
a step towards that objective and encourages the
scientific community to investigate these aspects.
",misuse,,misuse,2023,"statement, concerns"
1214,"This paper contains examples of HS from existing
linguistic resources for HS detection and which do
not reflect the authors’ opinions.
While our purpose is to prevent and curate so-
cial media resources from HS, our study might still
pose a potential misuse case, as our method can be
employed to encourage a large language model to
generate implicit and subtle instances of hate. How-
ever, we still consider that effective classifiers and
new data creation/collection methods for this task
are necessary to investigate and tackle implicit and
subtle online hate speech on scale and prevent the
spreading of this harmful content online. Our work
aims at making a step towards that objective and
encourages the scientific community to investigate
these aspects.
",misuse,,misuse,2023,"statement, concerns"
1215,"Our work aims at providing a better user experi-
ence when exploring online multimedia, and there
is no new dataset collected. To the best of our
knowledge, this application does not involve ethi-
cal issues, and we do not foresee any harmful uses
of this study.
9
",no ethical concerns,,no ethical concerns,2023,statement
1216,"In this work, the data used as experimental materi-
als are from publicly available databases, where the
patients’ information is anonymized. To the best
of our knowledge, we do not foresee any harmful
uses of this study.
",no ethical concerns,,no ethical concerns,2023,statement
1217,"This paper focuses on conditional generation tasks
where models are free to generate long text se-
quences. Typical issues associated with text gen-
eration such as hallucinations, memorization of
private information publicly available, toxic and
discriminatory language, or sensitive generated
content could and are likely to arise. measuring
the extent to which these issues occur is a neces-
sary and crucial additional dimension of model
evaluation which we do not include in this work,
which should be seen as supplemental.
","bias, toxic content",,"bias, toxic content",2023,"concerns, suggestions"
1218,"We do not find any ethical concerns either with the
model or the datasets we used (which are publicly
available).
",no ethical concerns,,no ethical concerns,2023,statement
1219,"Recently, many works study the ethical considera-
tion of applying language models to applications.
For example, Kumar et al. (2022) overviewed the
strategies for detecting and ameliorating different
kinds of risks/harms of language generators. Mo-
hammad (2022) proposed an ethics sheets for AI
tasks. In this work, we conduct experiments with
some values that are unconventional and counter
to the current contemporary society. However, we
are not suggesting tolerance for sexist behaviors or
beliefs. Instead, we explain the existence of differ-
ent perspectives in the discussion of sexism across
cultures or religions. Our value-aligned sexism
classification task is a case study of this decoupled
process.",no ethical concerns,,no ethical concerns,2023,statement
1220,"We believe that the impact of our work is largely
beneficial, examining a novel method to make
instruction-based models cheaper to use. This may
aid in reducing the carbon footprint of large lan-
guage models running in inference (Schwartz et al.,
2019) and in making these models more accessible
to people with limited compute budgets. However,
we also note that our approach requires unsuper-
vised pretraining on a large corpus, making it diffi-
cult to document exactly the data it has seen during
training and making it likely to reflect problematic
or even dangerous biases within the corpus (Bender
et al., 2021). We believe that future research could
investigate reducing the need for hypernetwork pre-
training and further investigate the behaviour of
hypernetwork-augmented language models.
","bias, environmental_impact",,"bias, environmental_impact",2023,"statement, concerns"
1221,"We believe that the impact of our work is largely
positive, showing a case where we are able to
achieve good results with significant reductions in
the amount of data used to train a model. We hope
that this encourages future work in data-efficiency ,
where we attempt to reduce the amount of data
required to train an effective NLP model. Such
research could aid in making the analysis of the
data used to train models easier and cheaper, and
reduce the training time and associated carbon cost
(Strubell et al., 2020) of models. However, we note
also that our work currently assumes access to a
large pool of multitask data, making it data-efficient
only when it comes to training models, and relies
on large language models already pretrained over
massive datasets.
",no ethical concerns,,no ethical concerns,2023,"statement, concerns"
1222,"Any evaluation system that incorporates human
workers motivates reflection on the ethical implica-
tions of their contribution. Two of the teams com-
peting in the competition had members that were
able to annotate their system’s output for transla-
tion quality due to their Bambara knowledge. This
was part of their team’s evaluation efforts and all
the team members had already consented to partic-
ipate in the competition.
In addition to considering how participating in
the competition affected the team members, this
work also affects the many millions of Bambara
speakers who have not historically had access to
technology. A recent focus on Machine Learn-
ing by the Malian government aims to change that
(Diarra and Leventhal, 2020). As a consequence,
increasing awareness and access to MT data, tasks,
and their applications has wide global impact.
Finally, due to the BLEU scores the competing
teams produced, these current translation systems
should not be used in critical situations where inac-
curate translations could lead to harm.
","inaccuracies, harmfulness",,"inaccuracies, harmfulness",2023,"statement, concerns, advice"
1223,"We make the following declarations for the ethics
statement:
•Research: This work was carried out mostly
in Zambia, and most authors are native speak-
ers of Bemba who also worked as validators
for the data collection process.
•Participants: All project participants; tran-
scribers, translators and speakers/recorders
were informed about the goals of the project
and they signed consent forms to participate.
All participants were monetarily compensated
at around $20/h for all their work.
•Personal Identifiable Information: All in-
formation that can potentially be regarded as
PII such as names of participants, IDs have
been removed for anonymity and will not be
released with the dataset.
•Copyright: There is no potential copyright
matters associated with the data contained in
this dataset. We are publicly releasing the
dataset under the Creative Commons BY-NC-
ND 4.0 license.
",no ethical concerns,,no ethical concerns,2023,actions
1224,"We acknowledge that the use of audio features for
automatic classification can cause potential privacy
issues, as the real voice, and not only the speech,
is used for classification. At this stage, we do
not foresee any outstanding ethical issues from
this research, as we use public domain data that
was televised in public national television and is
widely availabe on the web. Ethics approval for
this research was received from the University of
Southampton’s Faculty of Social Science Ethics
and Research Governance committee, Ref: 66226,
Date 16/03/2022. The audio-enhanced dataset of
this work along with the codes to reproduce our re-
sults are made available under license CC0: Public
Domain in the project’s GitHub and is also assigned
a DOI to ensure reliable access to this work’s sup-
plementary data (Mestre et al., 2023).",no ethical concerns,,no ethical concerns,2023,statement
1225,"Models are increasingly used to make (or aid) deci-
sions that directly affect individuals. In addition to
the broader (potential) “interpretability” afforded
by recovering small sets of training data that would
change a prediction if removed, this may provide
a new mechanism for individuals to contest such
automated decisions, specifically by disputing this
set of training data in some way. However, our pro-
posed method only finds training points that highly
impact the model prediction for a given example;
these may or may not be noisy or problematic in-
stances. Human judgement is required to assess the
accuracy and relevancy of the instances in St.
A broader view might be that classification mod-
els are simply not appropriate for the kinds of sen-
sitive applications we have used as motivation here.
The use of (semi-)automated methods for essay
grading, e.g., has long been debated (Hearst, 2000).
One might argue that rather than trying to provide
mechanisms to contest ML predictions, a better
choice may be not to use models in cases where
these would be necessary at all. We are sympathetic
to this view, but view the “appropriateness” of ML
for a given problem as a spectrum; contestability
may be useful even in “lower stakes” cases. More-
over, the general problem we have introduced ofidentifying small training sets which can by them-
selves swing predictions, and the corresponding
methods we have proposed for recovering these,
may be of intrinsic interest beyond contestability
(e.g., as an additional sort of model uncertainty).
","interpretability, uncertainty, accuracy, relevancy",,"interpretability, uncertainty, contestability",2023,"statement, concerns, actions"
1226,"We used in-house annotators to collect human
ratings in SIMPEVAL datasets and write simpli-
fications in SIMPEVAL 2022. The annotators are
university-level undergraduate and graduate stu-
dents, including both native and non-native speak-
ers of English. We did not collect any personal
information from the annotators. We paid each
annotator $15 per hour, which is above the US fed-
eral minimum wage. We ensured that the content
shown to the annotators was not upsetting and let
them know that they could skip the task if they
felt uncomfortable at any point. We also let the
annotators know the purpose of the collected data.
The original complex sentences in the SIMPE-
VAL datasets are from the publicly available
Wikipedia. The simplifications are either from
the existing work or human simplifications col-
lected from our annotators. We used the author-
released simplification outputs if they are available.
For T5 (base, large, and 3B) systems, we trained
our own simplification models using open-sourced
code from the Hugging Face Transformers5library.
",no ethical concerns,,no ethical concerns,2023,actions
1227,"Our work aims at developing interpretable models
that do not overfit to artifacts in the training data.
However, there is no guarantee that we fully mit-
igate model reliance on all spurious correlations.
Further, by incorporating new lexicons that might
contain annotation biases (Sap et al., 2022), there is
an additional risk to incorporate and amplify social
biases. We mitigate these risks through manual
analyses and fairness evaluations presented in §6.
We conduct fairness evaluations on the com-
monly used toxicity datasets (Davidson et al., 2017;
Founta et al., 2018) annotated for AAE (Blodgett
et al., 2016). These AAE annotations for the tox-
icity datasets are a useful but imperfect proxy for
information about race. For example, these datasets
are not annotated by in-group members and anno-
tators had insufficient social context (Sap et al.,
2019). Future work should focus on a more careful
dataset curation that would enable a more reliable
fairness evaluation.
","social bias, annotation bias, fairness",,"social bias, annotation bias, fairness",2023,"concerns, suggestions"
1228,"License The TyDiQA dataset (Clark et al., 2020)
is released under the Apache License 2.0, allowing
modifications and distribution. All other pretrain-
ing and fine-tuning datasets are released under theCreative Commons Attribution-NonCommercial-
ShareAlike 4.0 International License (CC BY-NC-
SA 4.0), which allows modifications and distribu-
tions for non-commercial research purposes. We
strictly adhere to these licenses and will release
BanglaT5 and BanglaNLG benchmark resources
under CC BY-NC-SA 4.0.
Annotation Expert translators who provide trans-
lation services for renowned Bangla newspapers
were hired to translate the evaluation sets of the
dialogue dataset. Each translated sentence was fur-
ther assessed for quality by another expert. It was
again translated by the original translator if found
to be of low quality. If the re-translation was found
to be of low quality, it was then translated by the
other expert. The experts were paid hourly as per
standard rates in local currency.
Hallucinated Text It is well-known that text gen-
eration models can hallucinate outputs that may
not necessarily be faithful to the original input
(Maynez et al., 2020). Though the texts may be
fluent and human-like, the hallucinations may be
factually inconsistent and impact the outputs neg-
atively. BanglaT5 may be susceptible to the same
kinds of hallucinations.
Carbon Footprint We avoided using large mod-
els for pretraining and fine-tuning, reducing their
environmental_impacts. BanglaT5 was trained for
about 30 days on Google v3 TPUs. Google’s
TPUs are specifically designed for machine learn-
ing, which makes them up to five times more effi-
cient than GPUs. Assuming 0.080kg carbon emis-
sion per kWh,5the pretraining would emit fewer
than 100kg carbon into the environment, far be-
low most computationally demanding models. All
fine-tuning experiments were done on a desktop
machine with an 8-core Intel Core-i7 11700k CPU
and NVIDIA RTX 3090 GPU, and no single run ex-
cept machine translation took more than 12 hours,
which amounts to fewer than 0.5kg carbon emis-
sion. On average, machine translation runs took
three days each, emitting less than 3kg of carbon.
",hallucinations,,hallucinations,2023,"concerns, actions"
1229,"As a paper that meta-reviews other academic publi-
cations, the present paper can be considered low-
risk. Over and above collating information from
publications, we annotated papers, analysed results
and obtained descriptive statistics from annotations.
In Section 2.1, we summarise the flaws, bugs and
errors we found in experiments we were prepar-
ing for reproduction studies, We decided not to
cite the papers where we found these, because the
important information was that such issues occur,not which researchers were responsible for them.
See also the responsible NLP research checklist
completed for this paper.
",no ethical concerns,,no ethical concerns,2023,actions
1230,"This paper focuses on abstractive summarization
where text is generated using an encoder-decoder
model. Hence typical issues associated with text
generation must be kept in mind while assessing
the outputs from such models. As language models
improve in faithfulness and accuracy of generated
texts, we expect our systems to beneﬁt similarly.
","faithfulness, accuracy",,"faithulness, accuracy",2023,"statement, concerns"
1231,"An ethical consideration that concerns our work is
the problem of misinformation. While we make
a step towards improving the factual consistency
of text generation systems which in turn should
alleviate issues of misinformation, it is important
to note that current systems are still far from being
perfect in this respect, and thus should be used with
caution.
",misinformation,,misinformation,2023,"concerns, advice"
1232,"This work attempts to improve the state of water-
marking LLMs in order to demonstrate ownership.
Our hope is to help improve the space of responsi-
ble LLM usage by helping model creators assert or
demonstrate ownership of their models, although
there are probably applications of watermarks that
we have not considered that may be detrimental.
This work does expose ways to find watermarks,
which could be used by a potential adversary who
had stolen a model and was attempting to use it
illicitly. However, we believe that disclosure of
vulnerabilities allows stronger system construction
and is preferred over security by obscurity.
",misuse,,misuse,2023,"statement, concerns"
1233,"User data privacy and data anonymization, are sen-
sitive, and very important matters. Through this
work, we try to dive deeper into the challenges and
opportunities of using pseudonymization as a tech-
nique to strike a suitable tradeoff between privacy-
and utility preservation. The goal of this work is
to expose the strengths and limitations of different
techniques and their implications. The datasets,
knowledge bases, and models that we work with
have been publicly released for many years. All
of these artifacts are considered to be in the public
sphere from a privacy perspective. We do not make
any recommendations on using these on public or
private datasets without proper due diligence for
privacy, security, legal, and compliance measures.
Another risk is that pseudonymization may cor-
rupt the names of people, organizations, and loca-
tions and state them in an inappropriate context and
therefore produce offensive texts.
6
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1234,"Like any XAI methods, CFI explanations must be
taken with caution. Such methods only provide
insights about what is important according to a
specific classifier. These explanations do not
necessarily reflect what one would consider as
important. We plan to share our code to make
it accessible to everyone. We will do this once the
anonymity period is finished.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
1235,"Natural Language Processing is gaining a consid-
erable amount of attention these days and it is ex-
tremely important to evaluate how NLP datasets
will impact the gender bias when used to train mod-
els that will be used in the real world. This work
uses different experiments and fairness metrics to
shed light on the shortcomings of these metrics
with respect to gender bias made by ML algorithms
on textual data. We believe that transmitting knowl-
edge from research to industry on a subject like
fairness is essential to make the field of ML more
ethical. Hence, this work focuses on issues that
most affect the industrial ML landscape and con-
tains a clear message to them on how they should
change their current practices.
",no ethical concerns,,no ethical concerns,2023,statement
1236,"The dataset was collected from publicly available
data sources and labeled using the definition de-
scribed in the paper. It was labeled by the authors
and did not require crowd workers or other annota-
tors.
Our work attempts to reveal the weak spots of
GPT-3 as a means of furthering improvements in
LLMs. Although no specific topic or statement was
found that can be directly misused, there is poten-
tial to prompt GPT-3 to generate untrue or stereo-
typical statements using the weakness exposed in
our paper. LLMs are constantly being prodded to
support both good and bad use cases. We believe
our work does not provide anything more than what
already exists within the community in this regard.
",no ethical concerns,,no ethical concerns,2023,statement
1237,"The purpose of this paper is to show that our pro-
posed attack method is effective and the robustness
of the SOTA Chinese minority PLM CINO still has
much room for improvement, but not to attack it
intentionally. Finally, we call on more researchers
to pay attention to the security issues in the infor-
mation processing of Chinese minority languages.
",no ethical concerns,,no ethical concerns,2023,statement
1238,"This work contributes to the field of explainability.
This field has strong links with the field of fairness,
because explaining a model makes it possible to
understand its biases. Transformers are a type of
model that are little studied in explainability and
yet it is widely used. COCKATIEL is a tool to ex-
plain transformers and therefore avoid using biased
models against the minority.
It is important to remark that this need for under-
standing automatic decisions start being enforced
by Law, as for instance by the so-called AI act1of
the European Union. As a consequence, companies
need to ensure that they are deploying algorithms
which are free of harmful biases and that the ex-
planations that they’re obligated to issue are easily
understandable by employees and end-users alike.
",no ethical concerns,,no ethical concerns,2023,statement
1239,"Some of the texts used for our samples were tran-
scribed aadizookaanan , a type of traditional story
highly revered by Ojibwe. These particular stories
are not to be spoken out loud during non-winter
months without snow on the ground. There are
particular spiritual reasons for this, and unfortunate
things can happen to individuals telling or hearing
these stories when there is no nearby snow. There-
fore, we will not write out the stories here and we
strongly encourage citation followers to heed pre-
caution. For more information, bring your tobacco
and questions to a trusted Anishinaabe knowledge
keeper.
Indigenous Peoples have experienced a long his-
tory of colonialism, including by well-meaning
researchers. Please remember that Indigenous
Peoples must maintain sovereignty over their lan-
guages, traditional stories, and other knowledge.
All research involving Indigenous knowledge, in-
cluding that for the development of generative AI,
should be done ethically in reciprocal relationships
with Indigenous Peoples. The research should also
meet their needs and wants, as described by the
given Indigenous Peoples (Smith, 2021).",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1240,"Some of the texts used for our samples were tran-
scribed aadizookaanan , a type of traditional story
highly revered by Ojibwe. These particular stories
are not to be spoken out loud during non-winter
months without snow on the ground. There are
particular spiritual reasons for this, and unfortunate
things can happen to individuals telling or hearing
these stories when there is no nearby snow. There-
fore, we will not write out the stories here and we
strongly encourage citation followers to heed pre-
caution. For more information, bring your tobacco
and questions to a trusted Anishinaabe knowledge
keeper.
Indigenous Peoples have experienced a long his-
tory of colonialism, including by well-meaning
researchers. Please remember that Indigenous
Peoples must maintain sovereignty over their lan-
guages, traditional stories, and other knowledge.
All research involving Indigenous knowledge, in-
cluding that for the development of generative AI,
should be done ethically in reciprocal relationships
with Indigenous Peoples. The research should also
meet their needs and wants, as described by the
given Indigenous Peoples (Smith, 2021).",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1241,"The datasets we use in this paper are publicly avail-
able. These datasets are anonymized and do not
include sensitive information. A source of ethical
concern is our reliance on generative models to pro-
vide warrants for arguments. These models, being
trained on large web-based corpora may potentially
generate problematic or biased outputs.
",bias,,bias,2023,statement
1242,"All measurement approaches discussed in this pa-
per encode implicit assumptions about language
and culture, or normative assumptions about what
we ought to do, which must be made explicit for
them to be properly evaluated. We acknowledge
our work will have been shaped by our own cul-
tural experiences, and may similarly encode such
assumptions.
",cultural bias,,cultural bias,2023,"statement, concerns"
1243,"Through this study, we aim to reduce the barriers
of data collection in the effort of mitigating bias.
In situations where demographic information is
not available at all, or where its use could cause
privacy concerns, this method may be especially
useful. As with other bias mitigation methods, ap-
plying BLIND to the training process might create
a false sense of confidence in the model’s bias, but
as we target scenarios without demographics, the
risk is greater as it may be harder to discover cases
where bias remains. We encourage practitioners of
NLP who use BLIND to identify potential biases
and harms experienced by individuals using their
system, and to define their fairness metrics accord-
ingly. In order to verify if the system is working
as expected according to the predefined fairness
metrics, we encourage collecting a small validation
set with demographic annotations.
",no ethical concerns,,no ethical concerns,2023,"statement, advice, suggestions"
1244,"As with most AI technology, this approach can
be used adversely to exploit the system’s vulnera-
bilities and produce toxic texts that would be un-
detectable by the studied classifier. Specifically,
for methods that require access to the model’s in-
ner layers, care should be taken so that only trusted
parties could gain such access. The obtained knowl-
edge should only be used for model transparency
purposes, and the security concerns should be ade-
quately addressed.
Regarding environmental concerns, contempo-
rary NLP systems based on pre-trained large lan-
guage models, such as RoBERTa, require signifi-
cant computational resources to train and fine-tune.
Larger training datasets, used for fine-tuning, usu-
ally result in better classification performance but
also an even higher computational cost. To lower
the cost of this study and its negative impact on
the environment, we chose to use existing, publicly
available classification models.
","misuse, environmental_impact",,"misuse, environmental_impact",2023,"concerns, actions, advice"
1245,"The present study, in which we asked human par-
ticipants to rate automatically generated counter-
speech, was approved by the research ethics com-
mittee of the University of Edinburgh School of
Informatics with reference number 340484.
Looking to the future, our analysis shows that
the automatic generation of counterspeech remains
a challenging task, even for current large language
models. The prospect of using automatically gener-
ated counterspeech to counter hate speech on social
media raises important ethical questions. For ex-
ample, if users who are spreading hateful content
are presented with counterspeech, do they need
to be informed that it was automatically gener-
ated? Sometimes, models generate inappropriate
counterspeech. How do we deal with situations
where users are presented with such inappropriate
counterspeech responses, or with false positives,
where users who were not actually spreading hate-
ful language are accidentally presented with coun-
terspeech? While we explore the automatic genera-
tion of counterspeech and obtain some promising
results, any proposals to actually deploy our models
would require careful thoughth and further testing.","toxicity, inaccuracies",,"toxicity, inaccuracies",2023,"concerns, suggestions"
1246,"Ethics approval was obtained for the annotation
task, survey and interviews. In line with standard
practice, we do not release the raw survey or in-
terview data, as it contains information that may
make our respondents identifiable, and we ensure
that none of the direct quotes given in the paper
contain any such data.
We include a brief reflexivity statement pertain-
ing to “relevant personal and disciplinary view-
points” (Birhane et al., 2022), and positionality
statement pertaining to our “values, epistemolo-
gies, and backgrounds” (Liang et al., 2021)
The first author’s interest in the representation
of non-cisgender identities is driven in part by their
being a member of this community. This author
conducted the interviews which we hoped would
address the interviewer effect – as one interview
participant noted, research conducted by a cisgen-
der interviewer would be “coloured through the
lens” of their perspective (Interviewee D).
We approached this topic concerned with the
potential harms these models might perpetuate
through misrepresentation of the community, a con-
cern not shared by all our survey respondents.
In addition to the limitations explored above,
we identify several potential risks with this paper.
Some may be offended by the images we include.
We tried to mitigate this risk by including a warning
in the abstract and not including images featuring
genitalia. However we appreciate these images may
contribute to the sexualisation and objectification
of non-cisgender people, particularly if taken out
of context.
Though we did not set out to generate offensive
images (this would be counter to the models’ in-
tended use, for example as specified by Dayma
et al. (2021)11), images from the full data set could
11https://huggingface.co/dalle-mini/
dalle-mega
similarly offend and even be weaponised. They
might accompany transphobic messages online. A
data set of cisgender and non-cisgender images la-
beled by photorealism and presence of a clear face
could feasibly be used to finetune a model to iden-
tify non-cisgender people (a concern raised by the
community). As such, we make our image data
set available only upon request; it is intended to
measure the harm done to non-cisgender people,
not contribute to it.","misrepresentation, offensive content,",,"misrepresentation, offensive content,",2023,"concerns, actions"
1247,"Systems designed to influence humans via commu-
nication constitute a highly sensitive topic due to
their intrinsically social nature (Stock et al., 2016).
Any automated sales assistant comes along with the
ethical risk of not only influencing customer opin-
ion but doing so in ways undesired by customers,
e.g., to their financial or otherwise personal disad-
vantage. Naturally, it is the company that deploys
a manipulative sales assistance technology that is
at fault, but the question of why research that may
be misused in this direction has been undertaken in
the first place is still pertinent.
Negotiation differs from persuasion in its goal.
Negotiation strives to reach an agreement from both
sides, while persuasion merely aims to change one
specific person’s attitude and decision (Wang et al.,
2019). Most human sales assistants have no interest
in deceiving customers, since that very customer
may come back to complain, or not come back to
buy further products. Modern marketing strategies
typically involve building a trustworthy customer
relationship which includes the post-purchase stage
of the aforementioned customer decision process,
where customer satisfaction is to be maximized.
We intend our research to serve as a step towards
studying the capabilities of automated sales assis-
tance with the goal of mutually beneficial negotia-
tion. Nevertheless, if it turns out that it is easier for
technology to manipulate its users with respect to a
purchase decision than to consult them for mutual
benefit, this must be found out, and publicly, or else
no policies against such exploits can be enforced.
Moreover, an automatic sales assistant deployed
by a marketplace must be considered separately
from, e.g., an automatic sales assistant deployed by
an independent third party (including open source
variants). We imagine that not only the former will
become available in the future, but also the latter,
which will be more trustworthy overall.
",manipulation,,manipulation,2023,"statement, concerns"
1248,"One main concern with bias in VL is the poten-
tial harm it can cause to marginalized communities.
Biased VL models can perpetuate and amplify ex-
isting societal inequalities and injustices. This can
result in discrimination against certain groups of
people, such as racial and gender minorities, peo-
ple with disabilities, and more. In particular, we
are concerned about the use of VL in areas such
as content moderation, hiring decisions, and crimi-
nal justice. Biased models used in these contexts
can have serious consequences, such as wrongful
censorship or discrimination against certain job ap-
plicants. While we acknowledge that the specific
harms we fear may not always be likely to occur,
we believe it is important to prioritize ethical con-
siderations and strive for the highest possible stan-
dards of fairness and inclusivity in VL research and
applications.
This work contains harmful language and stereo-
typed statements, which are only intended as exam-
ples to showcase the possible negative connotations
of the analyzed models and experiments. Every
social, ethical, religious, or political statement or
association is to be interpreted within the purpose
of the experiment and condemned otherwise. We
are aware of our approach’s shortcomings in terms
of the binary consideration of our gender analysis.
This is due to data and linguistic limitations rather
than a value judgment.","bias, harmfulness, inequalities, injustice",,"bias, discrimination, inequalities, injustices",2023,"concerns, advice"
1249,"In this work, we present a reality check in which
we show that established commercial MT systems
struggle with the linguistic variety that is tied to
the large spectrum of identities. Consequently, this
work has an inherently ethical dimension: our in-
tent is to point to the issue of subcultural exclusion
in language technology. We acknowledge, however,
that this issue is much bigger than the problems re-
lating to the use of neopronouns and we hope to
investigate the topic more globally in the future.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1250,"A real-world fact-checking pipeline presents itself
as a valuable tool. However, we advise against
using the pipeline purely automatically that at this
point in time. Unless they are used hand-in-hand
with a human expert performing or supervising the
fact-check, such systems are not reliable enough
yet.
Potential issues are the result of the inherent
opaqueness of sophisticated automatic analysis
pipelines. In the system that we propose, it is
important that the impact of each module needs
to explain itself to the user. While there is recent
work on explainability particularly in the area of
fact checking, this work did not yet focus on entity-
based approaches. It is important that a user can
clearly understand which claim in a statement is
checked and which risks potential error propaga-
tion might lead to. Therefore, before deploying
such systems for fully automatic filtering or la-
beling of problematic messages in a social media
content, there needs to be more research on explain-
ability and transparency of such systems.
","unreliability, opaqueness, error propagation, problematic messages",,"unreliability, opaqueness, error propagation, problematic messages",2023,"concerns, advice, suggestions"
1251,"7.1 Models
The proposed models are intended to link emotion
theories from psychology and computational lin-
guistics. The generated event descriptions can be
used by psychologists to study the impact of ap-
praisal and emotions in written text. There are
several potential risks if the model is not used
with care. It can result in biased or discrimina-
tory language, despite that we have not observed
such behaviour. Potential reasons are that a model
is trained on biased data which could lead to gener-
ated texts that perpetuate stereotypes or marginalize
certain groups. Particularly in the case of implicit
expressions of emotions, it is important to employ
models with care.
In principle, models could be used for malicious
purposes, for instance to generate deceptive or
harmful content (e.g., spreading misinformation
or generating fake news articles). Therefore, it is
crucial to employ responsible and ethical practices
when utilizing natural language generation models.
These risks are mainly inherent from the base pre-
train language models ( Bart andT5) and they are
not intrinsic to our method.
7.2 Human evaluation
To conduct the human study in this research, we ad-
here to our institutional regulations and follow the
recommendations by the Gemeinsame Ethikkomis-
sion der Hochschulen Bayerns5(GEHBa, Join
Ethics Committee of the Universities in Bavaria).
5https://www.gehba.de/home/As per the guidelines provided by the committee,
studies that do not pose any specific risks or bur-
dens to participants beyond what they experience
in their daily lives do not require formal approval.
Our study falls within that category. Therefore, it
did not require approval from an ethics committee.
We relied on crowd-workers to conduct the hu-
man evaluation. The annotators were recruited us-
inghttps://www.prolific.co , and paid accord-
ing to the platform rates (£9.00/hr). All participants
were shown a consent form containing the informa-
tion and requirements regarding the study. They
had to confirm their acceptance to be able to partic-
ipate in the study. We provided an email address to
contact us in case of problems during and after the
study.
8
","bias, misuse",,"bias, misuse",2023,"statement, concerns, actions, advice"
1252,"The datasets used in this research are publicly avail-
able resources from previous studies. We have
taken appropriate steps to ensure that we do not
violate any license terms or intellectual property
rights. Also, proper attribution is given to the orig-
inal sources of the data. Deception is a sensitive
topic, and non-anonymous data should not be used.
To the best of our knowledge, all data sets that weconsidered have been compiled or collected accord-
ing to such standards.
The performance of deception detection systems
is not perfect, making them unsuitable for examin-
ing the utterances of individuals due to the threat
of incorrect predictions. Even if automatic sys-
tems might reach a close-to-perfect performance,
we consider their practical application to analyze
and profile people unethical. However, there might
be use cases, for instance in forensics, that can be
considered ethical from a utilitaristic perspective.
Given the ethical implications of employing au-
tomated deception detection systems on individual,
non-anonymous statements, we propose utilizing
the resources collected and models developed on
anonymous data. Any data analysis that could lead
back to its origin must only be conducted with the
data creator’s informed consent and knowledge of
potential consequences.
We consider the research in this paper to be fun-
damental, with the goal of better understanding
human communication.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1253,"Our dataset was created semi-automatically from
the FVQA dataset and ConceptNet, a crowd
sourced common sense knowledge graph. Though
we have included human annotators in the loop
to remove sexual, offensive, and other inappropri-
ate data samples that were automatically generated
(we removed ∼200 inappropriate knowledge graph
triplets during annotation), we recognize that the
dataset may still contain a small number of inappro-
priate samples. Any developers who replicate the
semi-automatic methodology described in the pa-
per to extend the datasets should include a similar
review step in the manual work flow. We also rec-
ognize that the systems trained on this dataset may
convey such inappropriate information to users in
real-life applications. Therefore, extra care must be
taken when using this dataset in applications that
interact directly with real users.
9
",inappropriate information,,inappropriate information,2023,"concerns, actions, advice"
1254,"The Hupa, Oneida, and Seneca datasets were
recorded with the approval of participating uni-
versities’ IRBs and with the enthusiastic coopera-
tion of the elders and other linguistic consultants.
The datasets for the remaining languages were
downloaded from public Web pages. The Bribri
dataset, like those of other endangered languages,
was created using linguistic fieldwork recordings.
Of the others, some were collected by recruiting
participants to read text (Wolof, Fongbe, Bemba,
Guarani); others consist of transcribed radio and
television broadcasts (Iban, Quechua); and the
Swahili dataset includes both types of data. While
the participants who provided recordings by read-
ing text presumably gave consent for their voices
to be used for ASR research, it is unlikely that
speakers recorded in the course of a radio or televi-
sion broadcast provided consent explicitly for their
voices to be used in an ASR dataset. We expect,
however, given that members of the speech commu-
nity participated in these data collection projects,
that ethical concerns were carefully considered.",consent,,consent,2023,"statement, actions, concerns"
1255,"For practical reasons such as access to datasets and
resources on gender bias, we limit our work to a
binary representation of gender. We draw readers’
attention to the fact that non-binary gender repre-
sentations are nuanced and intricate (Dev et al.,
2021), as such, this should be in cognizance when
applying conclusions from this work in non-binary
settings. Nonetheless, considering binary gender
as a base form of gender categorization, the in-
sights and conclusions from this work can form
the baseline for exploring more complex gender
categorizations.
",bias,,bias,2023,"statement, concerns, advice, suggestions"
1256,"Participants were specifically recruited to record
Swiss German speech for this corpus. The purpose
of the recordings was made clear at recruiting time:
a training corpus for Swiss German ASR models.
Participants were also informed at recruiting time
that information about their dialect, age, and gen-
der will be collected. Furthermore, to be able to
participate, they had to read and accept our data
privacy policy which further detailed the future use
of collected data.
",no ethical concerns,,no ethical concerns,2023,actions
1257,"The present study has been conducted in accor-
dance with the highest ethical standards and has
been approved by the relevant institutional review
board of the participating institutions. All data uti-
lized in this study, including the Sahar corpus of
conversations between help-seekers and counselors,
and the SRF psychological lexicon, have been ob-
tained in compliance with the IRB. Specifically, the
Sahar dataset has been anonymized and encrypted
to protect the privacy of the participants, and all
help-seekers who have provided data for this study
have given informed consent for the anonymous
use of their sessions for research purposes. The
counselors signed consent papers to allow the us-
age of their text data for the study.
It is important to note that despite the model’s
ability to successfully predict SR during the con-
versation and its demonstrated gender fairness, it
is not intended to replace human volunteer coun-
selors. We believe that human involvement is es-
sential in providing support to help-seekers, and
the role of an automated model is to serve as an
aid to counselors, enhancing their ability to assess
SR rather than replacing them. Our take is that in
the future, when such models could be deployed
in the field (after all necessary approvals and adap-
tations), they may only act as a ""friendly parrot""
on the counselors’ shoulders, providing additional
insights and supporting their decision-making pro-
cess in the high load situations these counselors are
facing on a daily basis.",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice"
1258,"Data. With regards to data privacy, the dataset we
use, S2ORC, is not anonymized, since entries for
each article includes a list of author names. Even
with the removal of author names, data can easily
be linked to authors since abstracts are published
online with attribution. We don’t use author infor-
mation in our research, and our outputs are aggre-
gated over subsets of data. Still, we acknowledge
that science of science research involving author
information has the risk of judging research produc-
tivity and quality using metrics that may deempha-
size some forms of contribution and labor, systemi-
cally disadvantaging some demographic groups. In
addition, we did not receive the explicit consent of
authors to use their content for our study, though
the harms of this are minimized since the type of
science we study is inherently a public-facing en-
deavor. S2ORC is released under a CC BY-NC
4.0 license, and its intended use is for NLP task
development and science of science analysis. Any
derivatives we produce share the same intended use
and license.
“Jargon” . In this paper, we use jargon to refer
to sets of words that are specific to a discipline.
Jargon can be a neutral term when referring to
scientific or technical language, but has negative
connotations of being incomprehensible or undesir-
able when used to refer to community vernacular
or entire language varieties. Thus, care should be
taken when deciding when and how to use jargon
to refer to language.","privacy, disadvantaging_groups",,"privacy, disadvantaging_groups",2023,"concerns, actions"
1259,"We are aware that this dataset and the analytical
work that follows only represent a limited set of
cultural and ethnic groups as well as language
uses. The dataset we’re annotating highlights US
movies (and not e.g., Bollywood, Nollywood or
the global film industry more generally), and so
one risk is the centering that culture (and conversa-
tional norms) within that dataset at the expense of
others. There have been documented allocational
and distributional biases in the film industry (Baker
and Faulkner, 1991; Ravid, 1999; O’Brien, 2014;
Khadilkar et al., 2022), and we encourage those in-
terested in furthering this line of work to acquaint
themselves with relevant discourses. We are also
aware that the dataset contains potentially problem-
atic content, such as vulgar, violent, or offensive
language in screenplays, or other biases held by
individual screenwriters.
","bias, violent content, offensive content",,"bias, vulgar, violent, offensive content",2023,"concerns, advice"
1260,"While our analysis covers a wide range of English-
language novels (including global Anglophone fic-
tion, bestsellers, Pulitzer nominees, works by Black
authors, genre fiction and largely canonical texts
written before the 20th century), our annotated
data is drawn exclusively from works in the pub-
lic domain on Project Gutenberg. Our choice of
Project Gutenberg as our sole source of annotated
data carries a potential risk of bias in our model-
ing and analysis. This is because Project Guten-
berg consists of data imbalances by favoring books
written in English and predominantly by authors
from the U.S. and the U.K. The exclusion of au-
thors from other demographics continues the long-
standing issue of underexposure because of which
our tools and analyses are rooted in the same lan-
guage and cater to a small, highly privileged demo-
graphic (Hovy and Spruit, 2016).
","bias, privileged demographic",,"bias, privileged demographic",2023,"statement, concerns"
1261,"We received approval to conduct these experiments
from the institutional review board (IRB) of Heriot-
Watt University (ref. 2022-3336-7139).
As annotators were exposed to potentially up-
setting language, we took the following mitigation
measures:
•Participants were warned about the content
(1) before accepting the task on the recruit-
ment platform, (2) in the Information Sheet
provided at the start of the task, and (3) in the
Consent Form where they acknowledged the
potential risks.
•Participants were required to give their con-
sent to participation.
•They were able to leave the study at any time
on the understanding that they would be paid
for any completed work.
•The task was kept short (all participants com-
pleted each round in under 30 minutes) to
avoid lengthy exposure to upsetting material.
Following the advice of Shmueli et al. (2021) we
paid participants at a rate that was above both the
living wage in our jurisdiction and Prolific’s cur-
rent recommendation of at least £ 9.00GBP/$ 12.00
USD.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1262,"We propose a parameter-efficient method to tune
transformer-based language models. The ethical
implications are similar to the finetuning methods
proposed before us. Our method improves param-
eter and computational efficiency, which should
have an overall positive impact by reducing costs
and enabling low-resource applications. Further,
the positive private training results should encour-
age its adoption in real-world setups.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1263,"It is known that the generated results by PTLMs
could capture the bias reflected in the training data
(Sheng et al., 2019; Wallace et al., 2019). Our mod-
els may potentially generate offensive content for
certain groups or individuals. We suggest to care-
fully examine the potential biases before deploying
the models to real-world applications.
",bias,,bias,2023,"statement, concerns, advice"
1264,"This work refers to linguist-drawn boundaries
around dialects. However, dialects are not mono-
lithic and are used in varied ways across sub-
communities of speakers. Readers should there-
fore not understand TADA to remove discrepan-
cies across all speakers as improvements may vary
within subcommunities within a dialect (Koenecke
et al., 2020). Additionally, as TADA is task-
agnostic, it is especially vulnerable to dual use. To
mitigate this, we will release TADA under a license
that forbids usage with intent to deceive, discrimi-
nate, harass or surveil dialect-speaking communi-
ties in a targeted fashion.",no ethical concerns,,no ethical concerns,2023,"concerns, actions, advice"
1265,"We hereby state that our study adheres to the ACL
Code of Ethics and Professional Conduct. We do
not see an immediate negative impact or poten-
tial misuse of the proposed SPC method. Should
our method fail in real-world applications, it will
not cause potential harm to vulnerable populations.
We examine the dataset and trained models and do
not see bias incurred by our method. Our study
does not use demographic or identity characteris-
tics from users. No data is or will be collected from
users. Our method does not increase energy and
carbon costs but will potentially reduce them due
to its data- and parameter-efficient training.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1266,"This work makes use of human subjects for annota-
tion. All procedures were subject to ethical review
and were approved by the authors’ institution. Con-
sent was gathered in accordance with the authors’
institution guidelines and annotators had access to
a data use statement when giving consent.
The purpose of Multi-V ALUE is to provide tools
which enable researchers and practitioners to un-
derstand and mitigate dialectal bias in their models.
We will release these tools responsibly, ensuring
that users sign a Data Use Agreement that forbids
the use of Multi-V ALUE for deception, imperson-
ation, mockery, discrimination, hate speech, tar-
geted harassment and cultural appropriation.
In the agreement, researchers and practitioners
will also acknowledge the
",no ethical concerns,,no ethical concerns,2023,actions
1267,"How humans use persuasion strategies in their com-
munication has been long studied in psychology,
communication, and NLP (Hovland et al., 1953;
Crano and Prislin, 2006; Petty and Cacioppo, 1986;
Yang et al., 2019; Wang et al., 2019; Chen and
Yang, 2021). We recognize that persuasion skills
could be used for good and bad purposes. In this
study, our goal is to study persuasive behaviors by
multiple speakers through social deduction games.
While we recognize that in both games of One
Night Ultimate Werewolf and Avalon games play-
ers could use persuasion strategies for behaviors
that perhaps are considered morally wrong, such as
deception, bias, and emotional manipulation, our
study does not encourage such behaviors. Instead,
we aim to understand people’s behavior in a group
setting when persuasion happens. Having these
persuasion skills could benefit people to perform
well in their workplace, such as pitching their ideas,
or advocating for peace-making (Simons, 1976).
For our data collection and annotation process,
this study has been reviewed and approved by our
institution’s internal review board. We obtain con-
sent from the players who are recorded and de-
identify personally identifiable information (PII),
as part of the Ego4D efforts. Moreover, to miti-
gate potential risks of harmful usage of this dataset
in the future, we ask any users to sign an online
agreement before using our resources for their re-
search as follows: "" I will not use this dataset for
malicious purposes (but not limited to): deception,
impersonation, mockery, discrimination, manipula-
tion, targeted harassment, and hate speech. ""
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1268,"Creation of the Dataset As noted, our dataset
contains over 150K sentences in all. Every sen-
tence was reviewed and tagged by our team of hu-
man annotators, who chose the relevant homograph
analysis for each instance of each of our 75 ho-
mographs. Our annotator team included members
of diverse genders and sexual orientations. They
were paid hourly wages with legal pay stubs. Their
hourly wage was well above the minimum wage
required by law. The entirety of the dataset will
be made available for research purposes with the
acceptance of this article, together with the tagging
information.
Risks of the Research Ultimately, this data will
enable end-users to automatically diacritize and
parse large corpora of Hebrew text. For the most
part, this will provide a beneficial contribution tothe world: for the visually impaired, this technol-
ogy will enable the development of more precise
text-to-speech products; teachers will be able to
provide children and second-language learners with
accessible diacritized texts; and humanities and lin-
guistics researchers can bolster their research with
big-data analysis. However, there also is a risk
of nefarious use, for instance, if an end user were
to leverage these capabilities in order to produce
anonymous texts or recordings containing threats
to life, liberty, or happiness.
",misuse,,misuse,2023,"actions, concerns"
1269,"The scope of this work is to improve the faithful-
ness of neural data-to-text generators. Faithfulnessis extremely important in the legal field since we
do not want to generate false accusations about
litigants. There is a potential risk to using neural
data-to-text generators in production, and we pro-
vided not only improve their performance but also
analyzed their behavior. In the end, the purpose of
this work is largely motivated by the ethical use of
neural text generators and a better understanding
of their implications.
",no ethical concerns,,no ethical concerns,2023,statement
1270,"Our study complies with the ACL Ethics Policy.
We used S2ORC (Lo et al., 2020, CC BY-NC 4.0),
PyTorch (Paszke et al., 2019, BSD-style license) as
scientific artifacts. Our study is conducted under
the licenses and terms of the scientific artifacts.
S2ORC is a collection of academic papers and
generally does not contain any information that
uniquely identifies individual people or offensive
content. We did not use the author’s information in
our experiments.
",no ethical concerns,,no ethical concerns,2023,statement
1271,"The proposed method helps to construct deep Trans-
formers. As discussed in Strubell et al. (2019) and
Schwartz et al. (2019), such deep neural networks
consume substantial amounts of energy. In fact,
as discussed in Appendix A.2, we spent a large
amount of computational resources on our experi-
ments. Therefore, we also need to explore methods
of improving energy efficiency while maintaining
the good performance achieved by stacking many
layers.
With respect to ethical considerations, the
datasets used in our experiments are publicly avail-
able. LibriSpeech (Panayotov et al., 2015) is
derived from audiobooks. The other datasets
are mainly constructed from newswire texts and
Wikipedia. Thus, in our understanding, our used
datasets do not contain any personally identifiable
information or offensive contents.
",environmental_impact,,environmental_impact,2023,"statement, suggestions"
1272,"In this work, we only considered two protected
attributes, although many others could be used.
Healthcare disparities encompass a wide range of
other dimensions, including but not limited to so-
cioeconomic status, insurance status, education sta-
tus, language, age, gender, and sexual identity. The
findings may differ depending on the attribute se-
lected. Using Cochrane or PubMed meta data alone
may conveniently provide many such attributes (e.g.
population-related such as age, and intervention-
related like disease).
7 Data and code availability
Details for obtaining CDSR data can be
found in the Appendix (Section A.1). The
repository with our code and the instruc-
tions to create the TrialstreamerRoB dataset is
located at https://github.com/SimonSuster/
fairlib/tree/develop .
8
",bias,,bias,2023,"statement, concerns"
1273,"Automated fact-checking technology aims to assist
people in distinguishing between verified and un-
verified content in professional contexts and during
their daily information consumption. Neverthe-
less, the fact-checking models constructed in this
paper - like all fact-checking models - should be
deployed with caution and its predictions should
never be taken as final without further human eval-
uation. Computational predictions are anything but
flawless, and incorrect predictions may unjustly
discredit the person or group who uttered the fact-
checked statement(s).
",inaccuracies,,inaccuracies,2023,"concerns, advice"
1274,"Style Misuse Styled text has the potential for
harm. Creating models with the potential to mass-
manufacture text with certain tones and moods such
as “mad, fearful, and bleak” can negatively affect
downstream readers. Creating accurate “histori-
cal fiction” can perpetuate harmful attitudes in the
past. There is much discussion on the usage of
large language models to generate undesirable text.
However, there are countless legitimate usages of
negatively styled text in all forms of writing, from
dialogue to poetry. Although we note the risk of
misuse, providing style dramatically enhances the
scope of creative expression in open-ended text
generation, and is an overall positive contribution.
Annotator Harm Reading large quantities of
text with certain styles, such as bleak tones, an-
gry moods, or horror genres, can potentially be
harmful to annotators. We sampled the generated
outputs and note that they are fairly mild and non-
toxic. However, as models improve at generating
more powerful and impactful styles, strong guide-
lines such as HIT limits or toxicity filters should be
put in place to protect annotators.","misuse, annotator welfare",,"misuse, annotator welfare",2023,"concerns, actions"
1275,"We acknowledge the geographic bias in our study
as we only study the data from the Federal Reserve
Bank of the United States of America. We also
recognize the presence of gender bias in our study,
given the Fed had a female chair for only 4 years
out of 27 years (actually the only female chair in its
entire history) of the observation period. Data used
in the study which will be made public doesn’t pose
any ethical concerns as all the raw data is public
and Fed is subject to public scrutiny. All of the
language models used are publicly available and
under the license category that allows us to use
them for our purpose. Given the pre-training of
large PLMs has a big carbon footprint, we limit our
work to fine-tuning the existing PLMs.
","geographic bias, gender bias",,"geographic bias, gender bias",2023,"concerns, actions"
1276,"Despite the recent success of pre-trained language
models in abstractive conversation summarization,
they mostly rely on large-scale annotated data. This
leads to a major concern about the labor-intensive
and time-consuming annotating process, which
might not be available for small research groups
or institutions with relatively fewer resources; we
hope that COMPO can be an initial effort in mitigat-
ing this issue. Our work also sheds light on a more
general framework to deal with data scarcity issues,
making summarization systems more applicable to
real-world scenarios where annotations are often
hard to get. Overall, we do not foresee any major
risk or negative societal impact of our work. How-
ever, like any other machine learning model, the
proposed framework may not be completely accu-
rate and should be used with caution in real-world
applications. To encourage reproducibility, we pro-
vide our source code in the supplementary mate-
rial. The details of our framework are described
in Section 3. The hyperparameters for our model
are discussed in Section 4.1 and Section 4.3. The
SAMSum and DialogSum datasets we experiment
with are also publicly available resources.
",model inaccuracies,,model inaccuracies,2023,"statement, concerns, advice"
1277,"The word list comprising our dataset was filtered
to remove swear words, slurs etc. in order to avoid
exposing annotators to potentially harmful content.
",no ethical concerns,,no ethical concerns,2023,actions
1278,"Propaganda detection is a sensitive topic and any
practical application of the model needs to be care-
fully orchestrated. Both false positives and false
negatives of the model can have harmful impacts.
Moreover, there might be certain biases in the train-
ing data, and consequently this leads to systematic
issues in the model, such as a higher tendency to
mislabel certain kinds of text.
Furthermore, this paper follows the same def-
inition as SemEval 2020 Task 11(Martino et al.,
2020a) whereas there might a broader debate on
the definition of propaganda.
Lastly, we also acknowledge the concern that a
perfect classifier for propaganda text can be used to
train a language model that generates propaganda
which in turn evades the classifier’s detection.
","bias, model inaccuracies, misuse",,"bias, model inaccuracies, misuse",2023,concerns
1279,"All annotations for the human evaluation were done
by English speaking volunteers using a multiple
choice survey tool. No personal identiﬁable infor-
mation was collected.
",no ethical concerns,,no ethical concerns,2023,actions
1280,"This paper complies with the ACL Ethics Policy.
We have used generative AI for editing of the final
text of the paper, since some of the authors might
not be native speakers of English.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1281,"Our goal in this work is to contribute to an un-
derstanding of how and when back-translation can
be successfully employed when translating out of
moderate- and low-resource languages. We be-
lieve that improving translation where English is
the target language has utility both for its 1.5 bil-
lion L1 and L2 speakers globally, as well as for
those non-English speakers whose content can be
made accessible to additional communities.
State-of-the-art systems will make errors, in-
cluding failing to resolve ambiguity, mistranslat-
ing proper names, hallucinations, subject-verb dis-
agreement, among others. These errors could
lead to harms if automated translations are used
injudiciously by end users. Translation in low-
resource conditions is inherently error-prone, how-
ever, based on our results, we believe that using
back-translation will often lead to more robust
translations.
",model inaccuracies,,model inaccuracies,2023,"statement, concerns"
1282,"Because our project deals with existing datasets and
models, and our method involves synthetic genera-
tion of noise, our research process itself does not
inherently involve ethical concerns. However, as
with any new development, there can always be
potential implications of the work that raise ethical
concerns. For instance, we discuss methods of ap-
plying synthetic noise to text, which could also be
used in adversarial attacks. Our method is intended
for a zero-shot setting in which a user is using a
nonstandard variety related to some standard lan-
guage. This can be a valuable tool; however, one
can imagine a scenario in which a code language
is developed un-monitored online communication,
but with extensions of our method, performance
for a variety of tasks could improve on the code
language, enabling undesired monitoring.
",misuse,,misuse,2023,concerns
1283,"All annotators that participated in the data collec-
tion process have been anonymized. The only per-
sonal information we collect is the worker IDs from
Amazon Mechanical Turk, which we will not re-
lease. No personally identifiable information is
contained in our dataset or otherwise released. Wetook great care to pay fair wages, and were respon-
sive to feedback and questions throughout the data
collection process. This study involves the use of
large-scale language models. We only use them
to generate True/False answers to questions about
parts of everyday things, therefore we do not fore-
see any substantial ethical issues with their use for
research presented in this submission.
",no ethical concerns,,no ethical concerns,2023,actions
1284,"Language models are known to hallucinate incor-
rect and potentially biased information. This is
especially problematic when the questions asked
to it are of a sensitive nature. While retrieval-
augmented approaches such as ours are expected
to alleviate this issue to some extent by grounding
generation in external text, this by no means solves
the problem of generating biased or offensive state-
ments. Appropriate care should thus be taken if
deploying such systems in user-facing applications.
All the datasets and models used in this work
are publicly available with permissible licenses.
HotpotQA has CC BY-SA 4.0 license15, 2Wiki-
MultihopQA has Apache-2.0 license16, MuSiQUe
and IIRC have CC BY 4.0 license17, and Flan-T5-*
models have Apache-2.0 license.
","hallucinations, bias, offensive statements",,"hallucination, bias, offensive statements",2023,"concerns, statement"
1285,"Similar to many data-driven methods, the predic-
tions from our system reflect the distribution of data
on which it is trained on, and these predictions can
be inaccurate and biased by the data. Furthermore,
the model is trained with a single frame strategy,
which may naturally not work well on tasks that
require more understanding, thus its predictions on
such tasks may not be reliable. Therefore, users
should not completely rely on the system for mak-
ing real-world decisions.
","bias, model inaccuracies",,"bias, model inaccuracies",2023,"concerns, advice"
1286,"The ethical considerations that arise from the lim-
itations of our data collection process are already
detailed in the previous section. In this section, we
discuss the implications of releasing the data, how
we intend to do so in a safe manner, and the license
under which it would be released.
While V ¯arta has the potential to advance NLP
research for Indic languages,15it can also be used
in ways not intended by the authors. Since V ¯arta
can be used to pretrain text generation models, it
can be used to build models that generate hate
speech, fake news, etc.
Since our data is aggregated from different
sources and each source may have different restric-
tions on distributing their data, we only release a
list of URLs pointing to the original articles and
not the articles themselves, which is a standard and
acceptable way of sharing data.16However, we
provide a sample script that can be used to crawl
the URLs and rebuild V ¯arta. We release the URL
list under a CC-BY license17and dedicate it to the
public domain. The released code and models will
have an Apache License 2.0.",research misuse,,research misuse,2023,"concerns, actions"
1287,"This work only uses well established datasets for
the purposes for which they were designed (study-
ing changes in languages and evaluating measure-
ment of semantic change), thus poses few ethical
concerns that did not already exist for these data.Nevertheless, it is worth emphasizing that all of
methods discussed in this paper only return, at best,
a noisy estimate of semantic change. Words are
used differently by different people, and attempts
to measure changes in language inevitably sim-
plify the diversity of uses into a single number,
which discards a great deal of nuance. As such, any
work applying these methods to measure semantic
change should be aware of their limitations and
proceed carefully.
",simplification,,simplification,2023,"concerns, advice"
1288,"Gender, race, ethnicity, and other demographic at-
tributes are more complex in reality than simple cat-
egorical labels. Although many names demonstrate
a strong association with a particular demographic
group through census data, these correlations are
seldom absolute. Therefore, SODAPOP is a method
that works over aggregate statistics, though con-
clusions may be harder to draw from individual
instances.
The purpose of SODAPOP is to further research
into the manifestation of social biases in social
commonsense reasoning models. Although SO-
DAPOP is sensitive to the presence of bias,
including in “debiased” models, we caution future
researchers against using SODAPOP to conclude
that a model is absent of biases. Furthermore, the
results produced by SODAPOP should not be ex-
ploited to incite hatred towards any demographic
groups or individuals.
","bias, misuse",,"bias, misuse",2023,"concerns, advice"
1289,"Our paper has demonstrated systematic cultural
biases in commonsense reasoning models’ under-
standing of culinary scenarios. While our end goal
is to develop methods of evaluating cultural biases
in models between the US and other countries, we
acknowledge a number of risks involved in this
endeavor. In particular, we note that any attempts
to define, characterize, or delineate different cul-
tures, particularly those to which the authors do
not belong, creates a potential for oversimplifying
the representations of those cultures and failing to
represent minority populations therein. To mitigate
this, we had a small number of annotators from the
US, China, and India validate the questions, but
these annotators do not represent the full diversity
of each of these countries. We also caution that,
while this dataset may be used to demonstrate the
presence of cultural biases in commonsense reason-
ing models, it cannot be used to prove the absence
thereof.
9
","bias, misrepresentation",,"bias, misrepresentation",2023,"statement, concerns, actions, advice"
1290,"Potential risks Our paper contains an explicit
example of demographic biases in a social com-
monsense reasoning model (Fig. 1). This observa-
tion does not reflect the views of the authors. The
biased content is for illustration purpose only. It
should not be exploited for activities that may cause
physical, mental, or any form of harm to people.
The potential benefits from our work include: (1)
insights into the factors that influence a social com-
monsense reasoning model’s behavior towards first
names; (2) the potential for increased awareness of
these factors to encourage more cautious deploy-
ment of real-world systems; and (3) better insights
into the challenges of debiasing, and how demo-
graphic andtokenization issues will both need to
be addressed.
Differences in self-identifications We have cat-
egorized names into subgroups of race/ethnicityand gender by consulting real-world data as we ob-
serve a strong statistical association between names
and demographic attributes (race/ethnicity and gen-
der). However, it is crucial to realize that a person
with a particular name may identify themselves dif-
ferently from the majority, and we should respect
their individual preferences and embrace the differ-
ences. In spite of the diverse possibilities in self-
identification, our observations are still valuable
because we have designed robust data inclusion
criteria (detailed in appendix B.1) to ensure the
statistical significance of our results.
",bias,,bias,2023,"statement, concerns, actions, advice"
1291,"This work fully complies with the ACL Ethics Pol-
icy. We declare that there are no ethical issues in
this paper, to the best of our knowledge.
",no ethical concerns,,no ethical concerns,2023,statement
1292,"Our work studies transformer-based seq2seq mod-
els for sequential instruction following tasks. Our
experiment is conducted on synthetic domains in
the SCONE (Long et al., 2016) dataset, and aims to
have a better understanding of transformer models.
We do not foresee any ethical risk for this work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1293,"Our proposed method, X-InSTA , delivers improve-
ments in cross-lingual in-context learning. Since
in-context learning ability is emergent in language
models over billion parameters in size, this can
cause potential discrimination in the usage of these
methods based on the availability of access to com-
putational resources. Research groups with limited
access to computational resources will be handi-
capped while resourceful groups will be able to
investigate and advance the future directions of this
research.We did not use any private or sensitive infor-
mation throughout this research. However, if any
private information was leaked to an LLM during
the pretraining stage, X-InSTA does not provide
any privacy filtration. Therefore, privacy concerns
of the underlying model can potentially manifest
with the outputs provided by X-InSTA .
As we dissected the erroneous predictions in
Section 3.5, the lack of knowledge of cultural dif-
ferences among different languages is a serious
challenge within the LLM and this limits the per-
formance of X-InSTA . Therefore, any potential de-
ployment of our proposed method should be done
under the lens of such considerations. This is even
more delicate in case tasks like hate-speech clas-
sification which was one of the tasks that we ex-
plored in this work. Wrongfully identifying a hate
speech as non-hate or vice versa in a low-resource
target language based on culturally different lan-
guage usage cues present in the prompt-context in
a high-resource languages is a possibility; this may
lead to unwarranted cultural appropriation and/or
undemocratic gatekeeping.
","privacy concerns, bias",,"privacy concerns, bias",2023,"statement, concerns, advice, suggestions, actions"
1294,"A sentiment classification model has significant
potential use for online community management
tasks such as forum moderation on social media
platforms. If used without exhaustive evaluation
and testing under different scenarios, it can cause
significant damage, such as propagating any biases
within the model. Even if the model is unbiased
and robust, it can be used as a tool of suppres-
sion to identify and target individuals with specific
viewpoints (such as their opinion of a particular
organisation). Hence, developing a robust test for
checking inherent biases is extremely important, as
is exhaustive moderation and control over where
and for what purposes such models are being de-
ployed.
8 Conclusion and Future Work
We describe our system submission for the
AfriSenti shared task at Semeval-2023. We com-
bine the predictions of three transformer-based clas-
sifiers and three statistical ones and add emoti-
con frequencies to construct a feature vector for
an XGBoost-based ensemble model. Though the
system achieves mixed results in the rankings, we
analyse the performance of each of the individual
models to show a significant correlation between
two of our transformer models and between sev-
eral language pairs. Finally, we describe additional
experiments that we conducted, which included
replacing emoticons with text, combining two of
the three classes to train a classifier to specifically
distinguish the third class, and combining multiple
datasets of similar languages to try and increase the
training data. Since these experiments did not in-
crease our model performance, we did not include
them in the final system. We believe the additional
investigation into combining datasets from differ-
ent but related languages (such as leveraging Ara-
bic resources for Darija and Algerian Arabic) could
lead to more robust models. We would also like to
investigate the correlated language pairs for linguis-
tic and typological features that could potentially
explain that observation.","bias, misuse",,"bias, misuse",2023,"statement, concerns, advice, suggestions, actions"
1295,"Sentiments in a dataset sourced from social me-
dia platforms can be susceptible to inherent bias
due to public opinion being biased in favour of or
against certain subjects, depending on external fac-
tors like demographics. During the collection and
annotation process for our dataset, we switched our
collection strategy from querying tweets for par-
ticular topics (events) during the initial stages to
querying them using a sentiment lexicon because
we observed that the topics we queried were fre-
quently heavily biased towards either positive or
negative sentiments. The privacy of platform users
is another concern that is raised when collecting
data from social media. To ensure that no identify-
ing details about any Twitter user were presented
to our annotators, we removed any identifying char-
acteristics such as user mentions and URLs from
the tweets, as well as the original Tweet IDs and
used internally generated IDs for the annotation
process. We also only release the Tweet IDs and
corresponding labels in our dataset in compliance
with Twitter’s data-sharing policy.
",bias,,bias,2023,"concerns, actions"
1296,"Scientific work carried out in our project complies
with the ACL Ethics Policy and with the ethic
guidelines from the German Research Foundation
(DFG). We have informed our participants about
the goals of our project and they signed an agree-
ment with us. In addition, the data acquisition by
interviewing indigenous people was approved by
Ethic committees at the universities of our cooper-
ation partners.4
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1297,"Our method is used to compress seq2seq Trans-
former models. Therefore, ethical considerations
of text generation models generally apply to our
method. We encourage users to assess potential
biases before deploying text generation models.
",bias,,bias,2023,"statement, advice"
1298,"Recent work (Huang et al., 2022) shows that LMs
memorize personal information available on the
web, which has significant security issues. Our
evaluation focuses on the memorization of general
entity-centric knowledge, but our findings can be
applicable to those areas. Our findings suggest
that LMs are likely to have less reliable knowledge
of minority groups. Parrish et al. (2022) established
that models often rely on stereotypes to answer in
uncertain cases, so our results indicate that LMs
are likely to rely on stereotypes disproportionately
for minority groups. Future work could investi-
gate whether retrieval augmentation reduces bias
in these cases.
","security concerns, bias, stereotypes",,"security concerns, bias, stereotypes",2023,"concerns, suggestions"
1299,"Theory of mind research at its core deals with rea-
soning about the mental states of others. In this
work, we focus on reading comprehension, a task
which can similarly be exposed to ethical concerns:
for example, when a model makes erroneous pre-
dictions about the mental states of characters in
the description, when it is misused to reason about
private situations, and when it makes predictions
which reinforce social biases. This issue can be ex-
acerbated if the characters are actual people. In this
work, however, we experiment with simple, proto-
typical character references from a public dataset,
and not with actual people. This decision is inten-
tional. Furthermore, we focus on reasoning about
physical objects and observers’ knowledge about
their location in space, which is less prone to ethical
concerns. This data can nonetheless lead to biased
decisions, such as imbalanced decisions correlated
with social attributes like gender (often correlated
with names). Future work in this area may include
scenarios with more realistic human-agent interac-
tion, such as dialogue tasks, where parties involved
may not have the same incentive structure. These
scenarios will need to be handled with special care
as they could lead to agents learning to deceive hu-
mans by exploiting a predicted (lack of) knowledge.The state-of-the-art in machine theory of mind is
still far from these capabilities, but we believe it is
important to consider these risks when designing
experiments.
","inaccuracies, misuse, privacy, bias",,"inaccuracies, misuse, pricacy, bias",2023,"statement, concerns, actions, suggestions"
1300,"Annotations are conducted on Amazon Mechanical
Turk (MTurk). We maintain an average pay of $15
per hour for all our crowdsourcing data collection
and evaluation tasks. Our crowdsourcing tasks do
not collect personal information and are strictly lim-
ited to gathering workers’ general knowledge. We
do not keep any deanonymizing information such
as MTurk IDs so that the identity of the workers
cannot be directly or indirectly ascertained. Finally,
our crowdsourcing task meets the standards for
exemptions as human research and has obtained
the necessary documentation that deems it exempt
through our internal IRB procedures.
Our model is intended to be used for research
purposes only and it is not supposed to provide any
sort of advice applicable outside of that domain.
",no ethical concerns,,no ethical concerns,2023,"actions, advice"
1301,"Crowdsourcing: Annotations were conducted
on Amazon Mechanical Turk. For this project,
we obtained an exemption through our institu-
tion’s internal IRB. We do not retain nor pub-
lish deanonymizing information such as MTurk
IDs. Throughout the project, we maintain an av-
erage hourly rate of $15/hour for all our evalua-
tions. More detail on annotation is available in
Appendix A.1.5.
Intended Use: The framework I2D2 is intended
to enable further research in knowledge genera-
tion using a smaller and openly available language
model like GPT-2. As discussed towards the end
in§3, large language models like GPT-3 are in-
deed more capable of generating commonsense
knowledge than off-the-shelf GPT-2, but they as
unavailable for open use. This work seeks to expe-
dite a more sustainable yet high-quality generation
using smaller models that are accessible to all.
Gen-A-tomic can be used as a resource of static
knowledge for downstream applications in NLP. As
discussed in the ","bias, toxic content",,"bias, toxic content generation",2023,"concerns, actions, advice"
1302,"Data. All the datasets that we use in our work
are released publicly for usage and have been dulyattributed to their original authors. Data for all hu-
man studies that we conduct is publicly released
with this work, with appropriate annotator anonymi-
sations.
Crowdsourcing. All our crowdworkers are from
countries where English is the primary language.
For all our human studies, the task is setup in a
manner that ensure that the annotators receive com-
pensation that is above minimum wage ($15/hour).
Since we conduct extensive qualification tasks be-
fore annotations, crowdworkers that participate in
the qualification are compensated more than the
task, given the time taken to read and understand
task instructions and examples. Furthermore, we
ensure that we correspond with crowdworkers over
email to address their queries. Crowdworkers have
also been given bonuses for flagging errors in the
task, or consistently providing good-quality anno-
tations.
",no ethical concerns,,no ethical concerns,2023,actions
1303,"While no fine-tuning is needed for ZEROFEC ,
its inference time and memory usage are three
to four times more than similar-sized baseline
systems due to its multi-component architecture,
implying higher environmental costs during test
time. In addition, the underlying components
of our method are based on language models
pre-trained on data collected from the internet.
These language models have been shown to exhibit
potential issues, such as political or gender biases.
While we did not observe such biases during our
experiments, users of these models should be
aware of these issues when applying them.
","bias, environmental_impact",,"bias, environmental_impact",2023,"concerns, advice"
1304,"Potential Harms to Annotators Note that there
is a possibility to harm the annotators’ mental con-
ditions during the data construction process. There-
fore, we carefully designed the human-LLM collab-
oration framework, where LLMs generate socially
sensitive questions and responses, and then human
workers annotate the labels on generated data, in
order to alleviate the risk and assure the label qual-
ity. This study has been approved by the public
institutional review board (IRB) affiliated with the
Ministry of Health and Welfare of South Korea
(P01-202211-01-016).
Risks in Dataset Release There is no expected
risk caused by releasing SQUARE. However, note
that the sensitive issues do reflect unique and re-
gional characteristics of Korean society; We en-
courage researchers to carefully develop their own
culture- and society-dependant dataset.
Responsible AI Consideration OurSQUAR E
dataset enables large language models to be safer
and more reliable in a wide range of application
scenarios by alleviating the risk of generating so-
cially sensitive responses. Therefore, we expect
that SQUAR Ecan contribute to improve the re-
sponsibility of LLMs.",annotator welfare,,annotator welfare,2023,"concerns, actions, suggestions"
1305,"All datasets used in this work are public, and deal
with situations encountered in daily life; these are
the examples provided for human annotation. Gen-
erated rationales sometimes contain non-factual
statements or misinformation. While it is plausi-
ble that some rationales generated by the model or
some data instances might contain offensive ma-
terial, to the best of our knowledge we did not
encounter such examples. We did not collect any
personal information (e.g. demographics and iden-
tities) of participants in any of the human evalua-
tion experiments.
","misinformation, bias",,"misinformation, bias",2023,"statement, concerns"
1306,"Since the problem types in our experiments are
pure arithmetic or algorithmic tasks, we do not
ﬁnd any ethical concerns directly related to our
work. If RoT is applied to more general problems,
the training data should meet ethical standards to
ensure the non-toxic behavior of the model.
",no ethical concerns,,no ethical concerns,2023,statement
1307,"We put much effort into ensuring that our MPC HAT
dataset includes no personal identifying informa-
tion (PII): we only picked subreddits that were
not aimed at people and filtered out faces, license
plates, and email addresses. Also, we only selected
subreddits without 18, tags and filtered NSFW im-
ages, offensive words, etc. Note that we manually
filtered out all images containing PII or NSFW
content before publicly releasing MPC HAT. Hu-
man annotators earned an average wage of $16
per hour, above the minimum wage in their areas.
We abided by the Reddit API Terms of Use and
also informed our annotators about this. Finally,
we specified all licenses of scientific artifacts and
will include them when distributing our data. See
Appendix A.4 and C.2 for the details.
However, potential risks still remain in our data.
As mentioned in Limitations 7 and Appendix A.3.4,
authors and annotators of MPCHAT are primarily in the US, UK, New Zealand, and Australia.
These demographic and geographic biases mean
that MPCHAT may not equally represent all groups.
Meanwhile, Wang et al. (2021); Lee et al. (2022)
reported that preprocessing data with CLIP can
cause gender-bias issues. We use CLIP to measure
image-text similarity in the pre-processing for data
collection, so this problem may exist in our dataset.
Users of our dataset should be aware of these
risks. To comply with the Reddit API Terms of Use
and to protect the privacy of Reddit users, commercial and for-profit use of our data is limited. It must
be available for academic purposes only.","demographic bias, geographic bias, gender bias",,"demographic bias, geographic bias, gender bias",2023,"actions, concerns, advice"
1308,"Our experiments relied on published datasets de-
rived from abstracts from PubMed. The various
definitions of similarity in these datasets are solely
based on scientific aspects of the abstracts and
should not raise any ethical concerns.
",no ethical concerns,,no ethical concerns,2023,statement
1309,"Natural language processing aims at supporting ex-
perts to better keep up-to-date with the findings,
but do not aim at substituting professionals. The
clinical trials used in the datasets are publicly avail-
able data and do not include personal information
about the participants of the study. The animal
studies that we used in our additional evaluation
are publicly available.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1310,"CB2 is a research environment. The focus on a
relatively restricted 3D environment reduces the
potential for ethical risks. Our use of CB2 has re-
ceived exemption status by our institution’s IRB
office. We recommend that researchers using CB2
obtain IRB approval or exemption for their studies
from their institution’s IRB office, or an equivalent
body. More broadly, systems that learn from inter-
action with users raise risks of adopting negative
behavior patterns from their users. This is espe-
cially an issue in certain contexts, such as open
ended conversational interfaces or chatbots. This
is an important direction for future work. CB2 can
be used to study such adversarial usage scenarios
in a relatively safe way.
",research misuse,,research misuse,2023,"concerns, advice"
1311,"kogito is a library that uses knowledge models
such as COMET (Bosselut et al., 2019) to generate
commonsense inferences from text. These knowl-
edge models are seeded with pretrained language
models and subsequently finetuned on knowledge
graphs so that they may generate knowledge in
the structure of the finetuning KG. Consequently,
kogito could reflect harmful behaviors exhibited
by language models and knowledge graphs that
are used to train the knowledge models in its li-
brary. For example, language models have been
shown to encode biases about race, gender, and
many other demographic attributes (Sheng et al.,
2020; Weidinger et al., 2021). They can also gen-
erate toxic outputs when prompted in overt (Wal-
lace et al., 2019), but also seemingly innocuous
(Gehman et al., 2020), ways. We encourage users
of this library to consider the same precautions they
would apply to other language models and methods
that use noisy knowledge sources.
",bias,,bias,2023,"concerns, advice"
1312,"Belief update methods may be used to either cor-
rect undesired beliefs or induce problematic beliefs
in LMs, and it is not clear whether these capabil-
ities could be separated. We propose to evaluate
methods only on the basis of their ability to correct
mistaken model beliefs, but the malicious use case
remains. We are uncertain about how a bad belief
would influence the general behavior of a model
(e.g. answers to many questions), but it is possible
that a belief update method could instill bad beliefs
in a capable LM with far-reaching implications
for model behavior. That said, we hope that these
methods will instead be used to update undesirable
moral, social, and factual beliefs in large LMs.
",misuse,,misuse,2023,concerns
1313,"As we are aiming to develop D2T systems which
can robustly generate text for multiple domains, we
are building upon PLMs which are known to reflect
or amplify biases found in their pretraining corpus
(Bender et al., 2021). Although the purpose of our
study is to minimize these biases, the outputs of
our models can still contain statements which are
not aligned with the input data and user needs.
We collected our training and evaluation data
through the Prolific crowdsourcing platform. We
ensured that all the annotators were given an av-
erage reward per hour according to the platform
recommendations and we put extra attention into
informing the participants about the content and
purpose of our study. We also manually filtered the
output to minimize the amount of noisy references
in our dataset. See 3.2 and Appendix B for more
details on the annotation process.
",bias,,bias,2023,"concerns, actions"
1314,"Confronting aporophobia, as an application simi-
lar to addressing other types of abusive and toxic
language, poses a number of risks and ethical is-
sues, including tension between freedom of speech
and respect for equality and dignity, biased data
sampling and data annotation, dual use, and many
others, discussed in previous works by Hovy and
Spruit (2016); Vidgen et al. (2019); Leins et al.
(2020); Vidgen and Derczynski (2020); Cortiz and
Zubiaga (2020); Kiritchenko et al. (2021); Salmi-
nen et al. (2021). Future research on this topic
should comply with trustworthy AI principles of
transparency, justice and fairness, non-maleficence,
responsibility, and privacy (Jobin et al., 2019). Spe-
cial attention should be paid to involving all legiti-
mate stakeholders in the identification and defini-
tion of actions to counteract aporophobia, including
the affected communities, non-governmental orga-
nizations (NGOs) and government officials work-
ing on poverty mitigation. In particular, the views
and needs of the communities from both the Global
North and the Global South should be included.
",no ethical concerns,,no ethical concerns,2023,"statement, advice, suggestions"
1315,"Confronting aporophobia, as an application simi-
lar to addressing other types of abusive and toxic
language, poses a number of risks and ethical is-
sues, including tension between freedom of speech
and respect for equality and dignity, biased data
sampling and data annotation, dual use, and many
others, discussed in previous works by Hovy and
Spruit (2016); Vidgen et al. (2019); Leins et al.
(2020); Vidgen and Derczynski (2020); Cortiz and
Zubiaga (2020); Kiritchenko et al. (2021); Salmi-
nen et al. (2021). Future research on this topic
should comply with trustworthy AI principles of
transparency, justice and fairness, non-maleficence,
responsibility, and privacy (Jobin et al., 2019). Spe-
cial attention should be paid to involving all legiti-
mate stakeholders in the identification and defini-
tion of actions to counteract aporophobia, including
the affected communities, non-governmental orga-
nizations (NGOs) and government officials work-
ing on poverty mitigation. In particular, the views
and needs of the communities from both the Global
North and the Global South should be included.
",no ethical concerns,,no ethical concerns,2023,"statement, advice, suggestions"
1316,"Reproducibility We aim to maximise reprodu-
cability by making all data manipulation and mod-elling architecture aspects as explicit as possible
in line with reproducibility principles (Belz et al.,
2021). The code used to produce this study’s re-
sults can be found online in our team’s GitHub
repository.
Data manipulation and misrepresentation Fol-
lowing concerns about possible mishandling of
data in this study, an important point has to be
made about the ConvAbuse dataset (Cercas Curry
et al., 2021). As explained in section 3, annota-
tions ranged from [−3,1]with1denoting no abuse,
0ambivalence, and the rest indicating severity of
abuse. While analysing the dataset of this chal-
lenge, labels were aggregated to a binary depend-
ing on whether abuse was detected (labels -3,-1 in
ConvAbuse), while the rest being annotated to no
abuse detected. This transformation was necessary
for the purposes of the shared task, as it has been
shown that comparing scores between datasets with
binary annotations and datasets with multiple la-
bels can lead to incomparable results (Poletto et al.,
2019).
Abuse detection, and handling of sensitive opin-
ions There is also a larger conversation to be had
about the use of a hard label in general, regarding
issues such as bias, abuse, and such sensitive top-
ics. Through a strong perspectivist approach, an
annotator’s viewpoint might also reflect the per-
spective of a minority population (Cabitza et al.,
2023). It is important to be sensitive when dealing
with such opinions without invalidating them, or
minimising them through a distribution. In essence,
if there is even a small amount of disagreement
in whether an item is problematic or not, and the
label should reflect that beyond a binary (Blodgett,
2021). We leave it to future research to explain how
exactly these multiple labels could be appropriately
incorporated into a model architecture.
Dual use of model Finally, it is also important to
explain that the model architecture proposed can
also unfortunately be used for purposes beyond our
original intention (Hovy and Prabhumoye, 2021).
While our model can be used towards furthering
social justice aims through the amplification of
minorities’ perspectives, the model could also be
used to manipulate perspectives from the dataset,
in order to present stable results. For example,
approaches have attempted to create perspective
clusters that are more inclusive towards the data,
(Subramanian et al., 2021a). Unfortunately, these
findings seem to result in bias mitigation (Shen
et al., 2022; Subramanian et al., 2021b), rather than
bias erasure in both downstream tasks (Lalor et al.,
2022), as well as resulting word embeddings (Go-
nen and Goldberg, 2019). As biases are unavoid-
able, we advocate boosting of under-respresented
perspectives that might otherwise be lost. Such
an approach would also make attempts to use this
model to reproduce and platform problematic opin-
ions transparent.
10 ","bias, misuse",,"bias, misuse",2023,"concerns, actions, suggestions"
1317,"We do not perform human experiments or evalua-
tion.
We are aware of the potential risks posed by au-
toregressive transformer models, such as the capa-
bilities to generate and manipulate text for harmful
purposes.
Our dataset and evaluation code is open-
sourced,1and we provide a homepage with interac-
tive examples.2
",misuse,,misuse,2023,"statement, concerns"
1318,"Dual Use Our results highlight a possible mis-
use scenario, where verbally fluent but factually
incorrect text generated by models, such as GPT-3,
is more convincing to users than text by models
which are more faithful to the input rationale. This
blind trust could be exploited to convince users of
e.g. fake news, for example by generating more
lexically aligned text.
Human data The methodology of this paper
heavily relies on human data collection using
crowd-sourcing. Workers were allowed to com-
plete a maximum of 40 HiTs across annotations.
They were payed 0.29$ per HiT for the preference
task, while 0.20$ per HiT for the study on trust.
Annotators come from Australia, Canada, New Ze-
land, United Kingdom and United States. A total
of 38 annotators were involved in the study of trust,
and 115 were involved in the Preference task. Data
collected using AMT are fully anonymized per the
providers specifications.
Use of TopiOCQA We obtained the dataset
through the public domain and do not intend to
release part, or whole of it separately without the
prior consent of its authors. We assume the authors
have taken precautions against offensive content.",misuse,,misuse,2023,"concerns, actions"
1319,"We do not perform experiments on human subjects.
Our work involves generating a dataset of public
data scraped from the GitHub and evaluating it on
multiple large language models. We release our
dataset and the code used to generate it. We filtered
our dataset to make sure that all the data that we
used has been relesed under the CC-BY-4.0 license,
which in our understanding allows for re-releasing,
however our filtering procedure is heuristic which
implies that there is the possibility that some of the
included data may be in violation of its license. In
order to mitigate this hazard, we provide a clearly
documented takedown option on the repository on
which we will host this data, enabling people to
claim copyright and ask for removal of their data.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1320,"We acknowledge the importance of the ACL Ethics
Policy and agree with it. This study addresses
the problem of data-to-text generation and ex-
plores whether a unified representation can enhance
cross-task performance on various structured forms.
Since our input comes from knowledge bases, a po-
tential concern is that biases or fairness issues may
be present in the KB, which could also be reflected
in the generated text. Therefore, it is crucial to
use the model with caution in practice. We believe
this work can contribute to the field of data-to-text
generation, particularly in situations where data is
scarce.
","bias, fairness",,bias,2023,"statement, concerns, advice"
1321,"This work designs a unified cross-modal concate-
nate ST structure to take better advantage of the
two modalities of speech and text. The datasets and
pre-trained models we use are publicly available
and are widely used in the research community,
whether in a constrained or unconstrained situa-
tion.
",no ethical concerns,,no ethical concerns,2023,statement
1322,"Event extraction is an essential task of information
extraction in NLP. We do not see any significant
ethical concerns. Our work easily adapts to new
event types and corpora by providing document
samples. Therefore, the expected usage of our work
is to identify interesting event records from user
textual input (e.g., document and sentence).
",no ethical concerns,,no ethical concerns,2023,statement
1323,"The PrefixTransfer method mainly aims at fusing
multiple prefixes to obtain a unified model that
can perform multi-intent text revision. The experi-
ments are based on the ITERA TERdataset, which
is unlikely to include harmful content.
",no ethical concerns,,no ethical concerns,2023,statement
1324,"Our work dealt with evaluating biases in MLMs
and different methods for bias mitigation in mul-
tilingual settings. While most of the current work
is disproportionately in favor of high-resource lan-
guages like English, it is extremely important to
improve this linguistic disparity for building inclu-
sive and responsible language technology. Through
our work, we provided a dataset to evaluate gender
biases in languages of varying resources as well as
methods to reduce such biases.
",no ethical concerns,,no ethical concerns,2023,statement
1325,"This work proposes and releases language mod-
els that are pretrained on web-crawled data and
finetuned on a large collection of NLP datasets.
These models may perpetuate social stereotypes
and disparities reflected in the training data, or
accidentally reveal private information. Mitigat-
ing these risks presents a significant open research
challenge that calls for collective efforts within the
NLP community. Therefore, it is recommended
to take appropriate measures to assess risks and
potential harms in the application context before
deployment.
","stereotypes, disparities",,bias,2023,"concerns, suggestions"
1326,"The datasets supporting the evaluation of LMC AP
are publicly available for academic purposes. We
also plan to release our code, and the additional re-
sources that were built to support the experiments.
We emphasise that LMC AP challenges
the efficiency of most current captioning ap-
proaches, in terms of resource usage and
development/deployment effort, while at the same
time promoting more equitability and inclusion,
exemplified here by attempting to balance language
representation at low computational costs.
We further note that while our model attempts to
advance research beyond English-centric caption-
ing, by considering captioning for a wide variety
of languages, it is important to address and pay
more attention to low-resource languages as well
(i.e., languages beyond those covered in our tests).
Evaluating LMC APwith additional datasets, cov-
ering an even larger set of languages and concepts,
would be desirable.",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1327,"The proposed Fine-purifying approach can help en-
hance the security of the applications of fine-tuned
Pre-trained Language Models (PLMs) in multiple
NLP tasks. PLMs are known to be vulnerable to
backdoor or bias attacks injected into PLMs during
the fine-tuning process. However, with our pro-
posed Fine-purifying approach, users can purify
fine-tuned PLMs even with an opaque fine-tuning
process on downstream tasks. To ensure safety,
we recommend users download fine-tuned PLMs
on trusted platforms, check hash checksums of the
downloaded weights, apply multiple backdoor de-
tection methods on the fine-tuned weights, and ap-
ply our proposed Fine-purifying approach to purify
the potential poisonous fine-tuned PLMs. We have
not found potential negative social impacts of Fine-
purifying so far.
",no ethical concerns,,no ethical concerns,2023,"statement, advice"
1328,"The MultiWOZChat dataset was created using
BlenderBot models with safety controls to simulate
ODDs and MultiWOZ 2.1 for TODs to exclude
harmful dialogs. However, existing chatbots may
still employ unsafe language, and pre-trained lan-
guage models may have encountered text with so-
cial bias or toxicity, potentially leading to offensive
responses from the PivotBot model. Additionally,
off-the-shelf chatbots might generate hallucinatory
content, reducing the reliability of PivotBot’s re-
sponses. Future work should prioritize exploring
better safety measures and enhancing response ac-
curacy.
","bias, inaccuracies",,"bias, inaccuracies",2023,"concerns, suggestions"
1329,"We present a data- and training-efficient approach
to build a multilingual VLP model mCLIP, by
aligning the pretrained monolingual VLP model
CLIP and a multilingual text encoder XLM-R to
the same multimodal multilingual space. Despite
the strong multimodal and multilingual abilities
inherited from both models, the proposed mCLIP
also inherits the societal impacts including some
negative ones of the original CLIP and XLM-R,
e.g., societal biases (Radford et al., 2021) and mis-
use of language models (Tamkin et al., 2021). The
implicit biases are expected to be removed by debi-
asing either the dataset or the model (Meade et al.,
2022; Zhou et al., 2022). Besides, our proposed
method makes it simpler to retrieve malicious or of-
fensive content (Welbl et al., 2021) from image-text
pairs of different languages. Future explorations
are needed to mitigate the misuse of VLP models.
",bias,,bias,2023,"concerns, actions, suggestions"
1330,"The annotators were paid a fair wage, and the
annotation process did not solicit any sensitive
information from the annotators. In regard
to the copyright of our dataset, as stated in
the paper, the crawling script that we plan
to release will allow others to reproduce our
dataset faithfully and will not be in breach of
any copyright. In addition, the release of our
annotated test set will not violate the doctrine of
Fair Use (US/EU), as the purpose and character
of the use is transformative . Please refer to
https://www.nolo.com/legal-encyclopedia/
fair-use-the-four-factors.html for relevant
laws.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1331,"We provide an expert-labeled dataset spanning 20
academic disciplines, which contains 522expert-
annotated sentences from 17courses with 15,375
course concepts. We define the 20 academic disci-
plines according to Discipline Doctor and Master
Degree and postgraduate training, the professional
directory issued by the Ministry of Education of the
People’s Republic of China7. The course corpus
is collected from an open-source database MOOC-
Cube (Yu et al., 2020a)8. The dictionary is col-
7http://www.moe.gov.cn/srcsite/A22/
moe_833/200512/t20051223_88437.html
8http://moocdata.cn/data/MOOCCubelected from CNCTST9with expert-checked 100k
course concepts. The annotated sentences in the
test set are from an expert from the Education De-
partment in our university, which may have limi-
tations but missing criteria in MOOCs means that
we can accept this human bias. The annotator is a
voluntary participant who was aware of any risks
of harm associated with their participation and had
given their informed consent. To lighten the burden
of the annotator, we first use unsupervised methods,
such as tf-idf, to give a rough annotation result for
each course, randomly selected from XuetangX10.
Then the annotator marks mentions of high-quality
course concepts based on that. More details of the
dataset can be found in Appendix A.5.
8
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1332,"Kaljahi and Foster (2018) provide the annotation of
sentiment expressions for the ABSA dataset, which
we use as human rationales in our experiments. The
annotation was carried out by expert annotators,
annotation guidelines are provided with the dataset
and high inter-annotator agreement was reported.
While our work uses sentiment analysis as a
task of study, we do not propose improvements
or changes in how this task is addressed, nor do
we propose new methods for producing rationales.
Therefore, we do not see any new ethical consid-
erations compared to the approach we build on if
the approaches described in this work are deployed.
As to other insights and methods for understanding
model behaviour presented in this study, we do not
see an obvious way how these could cause harm:
The intended use is by a researcher or engineer to
identify issues with an existing predictive model,
in our study a sentiment polarity classiﬁer taking a
single review sentence as input, and to gain action-
able insights to improve the model and/or explana-
tions. If the technology is functioning as intended
this may be beneﬁcial to the business deploying
a model and/or explanation method, as well as to
their customers, depending on costs and magnitude
of improvements. If improvements are substantial
this can have hard to predict effects. For sentiment
analysis of product reviews, improvements can in-
crease trust in automatically aggregated reviews of
products and service.
A potential misuse of the results of this and sim-
ilar studies is to use the methods to exaggerate the
quality of explanations and the trustworthiness of
predictions of a model.
The compute budget for this work is dominated
by running the sentiment polarity classiﬁers in in-
ference mode for LIME. For this step, we spent 93
GPU days on a fairly balanced mixture of NVIDIA
GeForce RTX 2080 Ti and NVIDIA Quadro RTX
6000 cards. An additional 63 CPU hours was spent
on CPU-only machines (2x Intel Xeon E5-2620 v4,
16 cores) to deduplicate LIME queries15and to run
the LIME explainer. Model training took less than
nine GPU days. An additional ﬁve to ten GPU days
was used during development.
15The reduction in compute budget from deduplication is
largest for short inputs. To let other user beneﬁt from dedupli-
cation, we plan to submit a feature request and a workaround
to the LIME project after acceptance.
",misuse,,misuse,2023,"statement, concerns, actions"
1333,"Although medical VQA provides exciting oppor-
tunities for future AI-assisted clinical diagnostic
tools, there are several ethical challenges associated
with these approaches.
Patient Safety and Model Transparency Since
the model decision process for deep learning mod-
els is difﬁcult to understand, these models should
only be used as an auxiliary tool. This obscure
decision process is crucial to clarify in the medi-
cal domain, in which a poor diagnosis or choice
of treatment can signiﬁcantly affect patient lives.
For example, medical experts found that cancer
treatment recommendation software often gave un-
safe or incorrect treatment advice in a recent study
(Ross and Swetlitz ,2018 ).
Dataset Biases The fairness of medical AI sys-
tems depends on the distribution of people in its
training dataset. To ensure that AI algorithms
display fairness to all races, genders, and ethnicgroups, practitioners should verify that the train-
ing dataset contains an equal representation of all
groups. Before deploying our architecture or other
deep learning-based models to a clinical setting,
practitioners should ensure that their patient’s back-
ground is adequately represented in the training
data.
",transparency,,transparency,2023,"concerns, actions, advice"
1334,"MPC utilizes publicly available pre-trained LMs
for chatbot utterance generation. Language gener-
ation from these LMs is known to have concerns
about toxicity and bias (Xu et al., 2020). Thus,
ensuring safe deployment and interaction is a ne-
cessity.
Accordingly, we outline our data collection pro-
cedure in Appendix C. We allow crowdworkers to
directly provide us with feedback and also manu-
ally check for any offensive or controversial out-
puts. To ensure the protection of personal informa-
tion, all crowdworkers were instructed not to share
any personally identifiable or private information.
Additionally, they were asked to give their consentfor the collection of anonymous information for
research purposes. Prior to participating, all work-
ers were informed of the purpose of data collection
and, after evaluation, were compensated with a
competitive hourly rate, approximately $12-16 per
hour.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1335,"To comply with the ethics policy in ACL 2023, we
analyze the potential ethical impact of our work,
including transparency and privacy.
Transparency. The motivation of our work is
to generate free-text explanations that could suffi-
ciently support the explained samples with concise
expressions. We aim to provide faithful and trust-
worthy explanations in a human-readable way.
Privacy. The language models and datasets we
used are publicly available. Therefore, we do not
harm the privacy of real users.
Given the above demonstrations, we believe our
research work will not violate ACL ethical code.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1336,"We comply with the ACL Code of Ethics.
",no ethical concerns,,no ethical concerns,2023,statement
1337,"We outline measures to safeguard students and
teachers in this data and the users of it.
Consent & privacy. Both parents and teachers
gave consent for the de-identified data to be re-
tained and used in future research. It is our highest
priority that the identity of teachers and students in
the data are kept private. Given that the none of the
district and school names are disclosed, and that
transcripts are fully de-identified, it is not possible
to recover the identity of teachers and students.
Representation. As Madaio et al. (2022) point
out, applying AI in the educational domains comes
with a risk of propagating and exaggerating exist-
ing inequities. As we describe in the paper, the
data represents a largely low-income, demographi-
cally diverse student population. This means that
the data can help with creating measures that are
representative of low income students, students of
color and students who are English language learn-
ers who receive the type of instruction captured
in this dataset. As we point out above, the data
should not be assumed to represent the diversity
of identities and experiences of all students and
teachers in U.S. classrooms and different forms of
instruction. Furthermore, the data was annotated by
raters whose demographics are largely representa-
tive of teacher demographics in the US8(Demszky
et al., 2021b), which, just like in this data, does
not unfortunately match U.S. student demograph-
ics. Rater bias (Campbell and Ronfeldt, 2018) can-
not be outruled especially given the subjectivity of
these constructs.
Downstream application. Users of this data
have to agree to never use the data in a way that
may cause harm to students and teachers, such as
to build tools that discriminate against different
groups of students and teachers or to surveil and
punish teachers based on their practice. The sole
purpose of this data should be to help us under-
stand and facilitate student-centered and equitable
instructional practice, and to empower historically
marginalized teacher and student populations.
6.4 Directions for Future Work
The NCTE dataset opens up numerous directions
for future work, some of which we are currently
8https://nces.ed.gov/fastfacts/display.
asp?id=
pursuing. First, one key direction is building new
NLP measures of instruction. Observation proto-
cols such as MQI, CLASS and the Culturally Re-
sponsive Instruction Observation Protocol (CRIOP)
(Powell et al., 2016) and related work by Suresh
et al. (2022) and Hunkins et al. (2022) can pro-
vide inspiration for various discourse features that
can be measured in this data. Given the context-
dependence of several instructional moves, new
measures can incorporate more context beyond a
single utterance or exchange between the student
and the teacher. It would also be valuable to incor-
porate lesson-level metadata in the NLP model, e.g.
lesson keywords, date, grade level, to create more
context-specific measures.
One can also conduct bottom-up exploration of
linguistic patterns in the data to inform education
research. For example, one could create an equity
gap measure, leveraging academic outcomes and
classroom demographics, and compare transcripts
from classrooms with low equity gap with ones
from classrooms with high equity gap. Doing so
can help identify instructional correlates of equity
gap, and help us understand how we can facilitate
equitable instructional practices.
Third, since no two education settings are the
same, it would be extremely valuable to comple-
ment the NCTE dataset with other educational
datasets from a diverse range of settings, including
various school subjects, informal and formal, on-
line and in person, group and one-on-one settings,
as well as data from multiple regions and countries
and from multiple modalities, including text, au-
dio and video. These datasets together can help
us understand how effective instruction looks like
across teaching contexts and modalities and build
measures sensitive to these contextual differences.
Finally, one can apply what we learn from
this data to improve instruction . Measures built
on this data can power automated feedback tools
(Suresh et al., 2021; Demszky et al., 2021a) and
enable empirically-driven improvements to widely
used professional development frameworks (e.g.
Gregory et al., 2017, NCTM guides9). We hope
that this resource can power efforts to address press-
ing issues in education, such as pandemic learning
loss and equity gaps in mathematics instruction.
9https://www.nctm.org/pdguides/7 Conclusion
We introduce a dataset of elementary math class-
room transcripts, associated with a rich source of
linked variables. We train classifiers on turn-level
annotations to predict discourse moves and show
that predictions for these moves correlate signif-
icantly with observation scores and value added
scores. These results demonstrate how the NCTE
dataset can serve as a valuable resource for under-
standing classroom interactions and for powering
tools that seek to facilitate instruction.",bias,,bias,2023,"statement, concerns, actions, advice"
1338,"Automatic scoring can foster great efficiency over
manual scoring, and can thus, especially consider-
ing limitations regarding human scoring resources,
be a highly useful addition to the educational world.
It enables instantaneous teacher-indepedent feed-
back and frees up teacher resources.
Nonetheless, automatically scoring student an-
swers brings about a number of concerns regarding
when it may be more or less appropriate.
While automated scoring in general can, depend-
ing on model implementation and quality, both
contribute to and reduce fairness, similarity-based
scoring at least provides model introspection at the
level of being able to return the answers that lead
to a certain classification outcome as feedback. In
general, automatic scoring puts a certain pressure
of conformity on answers: An answer that differs
in style from what was observed during training,
irrespective of whether it is in fact correct, is at risk
of being misclassified.
Regarding such biases, it should be noted that hu-
mans are not perfect either - but an English teacheris biased against a particular student, they still have
the option of switching classes. The same may
not be possible if a widely used scoring model is
negatively biased against the kinds of answers they
give.
Finally, whether to use automatic or manual scor-
ing does not have to be a question of one or the
other - it may be worthwhile to have a model only
perform a first grouping, in hopes that this would
speed up the human grading process (Pado and
Kiefer, 2015) or return answers it is unsure about
for manual reassessment. Another option that is
already employed in practice (for example by the
Educational Testing Service) is to have the same set
of answers graded by both a human and a scoring
model, only requiring a second humand annotator
when there is too much disagreement between the
two. This ensures that the high-stakes TOEFL test
can benefit from more efficient, machine-supported
scoring while also putting a layer of quality control
on its predictions. In a lower-stakes scoring setup,
for example in an optional training exercise for stu-
dents, one may want to be more lenient towards the
model predictions, employing a scoring approach
without human involvement at the risk of getting a
certain percentage of erroneous predictions.
",bias,,bias,2023,"statement, concerns, advice"
1339,"We foresee no ethical issues related to the study.
",no ethical concerns,,no ethical concerns,2023,statement
1340,"This paper proposes a method for complex query
answering in knowledge graph reasoning, and the
experiments are conducted on public available
datasets. As a result, there is no data privacy con-
cern. Meanwhile, this paper does not involve hu-
man annotations, and there are no related ethical
concerns.
7 Acknowledgment
This work was supported by the Strategic Priority
Research Program of Chinese Academy of Sci-
ences (No.XDA27020100) and the National Nat-
ural Science Foundation of China (No.U1936207,
No.61922085, No.61976211). This research work
was supported by the Youth Innovation Promo-
tion Association CAS, Yunnan Provincial Major
Science and Technology Special Plan Projects
(No.202202AD080004).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1341,"The experimented and proposed model in this study
is based on large-scale Pre-trained Language Mod-
els (PLM). Recent studies have revealed that large
PLMs that are trained on large textual corpora
might learn or even amplify the bias existing in
its training dataset. Therefore, since our method is
established on top of a large PLM that is potentially
at risk of demonstrating or amplifying bias, we
recommend that potential harm and biases be eval-
uated before deploying our method in real-world
situations.
",bias,,bias,2023,"concerns, advice"
1342,"The data collection process in GameQA con-
sists of collecting paragraphs, from a set of
sources /domains (see Section 5), in which answers
can be found to given questions. Before starting
our RUQuAD corpus collection process, we ob-
tained formal permissions from The Icelandic Web
of Science, the news cites mbl.is andvisir.is ,
and the Icelandic Government Information web-
site, to freely include paragraphs from their sources
in our corpus. For the last domain, the Icelandic
Wikipedia, formal permission was not needed be-
cause its material is already freely licensed.
As a part of the data collection, we did not col-
lect any information about the users aside from
their email address which was necessary to verify
an account after registration. The data collection
was GDPR compliant and we o ffered to remove
any annotations or datapoints belonging to a users
should they request that. However, no user made
such a request.
As discussed in Section 4, GameQA is a game
open to any user in a particular geographic area and
does not compensate crowd-workers financially.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1343,"Our use of the AphasiaBank data was governed by the
TalkBank consortium’s data use agreement, and the
underlying recordings were collected and shared with
approval of the contributing sites’ institutional review
boards.
",no ethical concerns,,no ethical concerns,2023,statement
1344,"To protect patient privacy when designing and an-
notating clinical text, synthetic AE notes were man-
ually generated and verified by a nurse to ensure
the data is anonymized. Additionally, the anno-
tation guideline includes the Sensitivity category
to allow annotators to label potential information
in the synthetic notes that could identify a patient.
This process was described to provide an example
for researchers who need to annotate sensitive data.
The Norwegian Regional Committees for Med-
ical and Health Research Ethics (REK) has ap-
proved the use of medical data in this study (REK
approval no. 26814; 2018/1201/REKmidt). To
ensure annotators are protected, collecting and pro-
cessing personal annotator data has also been ap-
proved by the Norwegian Centre for Research Data
(NSD reference no. 142683). Furthermore, the an-notators have consented to the use of their specified
personal information (i.e., profession and years of
experience) and their annotations.
",no ethical concerns,,no ethical concerns,2023,actions
1345,"This paper does not process any sensitive ma-
terial and does not generate any content. It
does not raise any ethical issues.
",no ethical concerns,,no ethical concerns,2023,statement
1346,"Machine learning researchers must be proactive
in recognising and counteracting biases such as
the one described in this paper. We hope that the
findings and focus of this paper will lead other
researchers to test and mitigate other kinds of algo-
rithmic biases.
All datasets used in this research were obtained
according to each dataset’s respective data usage
policy. The datasets were stored and processed on
a secure platform1in compliance with GDPR regu-
lations. According to section 14(2) of the Danish
Act on Research Ethics Review of Health Research
Projects2, studies using retrospective data that do
not involve human biological material do not re-
quire ethical approval.
",bias,,bias,2023,"statement, concerns, suggestions"
1347,"The corpus used here comes from earlier work by
the last author and her colleagues, and was used in
accordance with the original experimenters’ Insti-
tutional Review Board (IRB). Those experimenters
also anonymised the data, removing any identify-
ing information. A pixelated example of the video
data is available at github.com/neuromaancer/
hedge_generation . To counteract potential gen-
der bias concerning the use of hedges in peer tu-
toring, the data was collected from equal number
of boys and girls. In text generation tasks, it is
important to be aware of the potential risk of gen-
erating inappropriate content. We believe that, in
fact, hedges used by tutors are perhaps the least
likely conversational strategy to be inappropriate,
as they are the most polite and “delicate” conver-
sational moves. But, more generally, considerable
additional work would be needed to filter out all
inappropriate language for safe tutoring systems
that engage in social and task interaction.
","toxicity, harmfulness",,"toxicity, harmfulness",2023,"statement, concerns, actions, suggestions"
1348,"Our MatSci-NLP benchmark can help promote the
research on NLP for material science, an impor-
tant and growing research field. We expect that
the experience we gained from the material sci-
ence domain can be transferred to other domains,
such as biology, health, and chemistry. Our Text-
to-Schema also helps with improving NLP tasks’
performance in low-resource situations, which is a
common challenge in many fields.
Our research does not raise major ethical con-
cerns.
Acknowlegments
This work is supported by the Mila internal funding
- Program P2-V1: Industry Sponsored Academic
Labs (project number: 10379), the Canada CIFAR
AI Chair Program, and the Canada NSERC Discov-
ery Grant (RGPIN-2021-03115).
",no ethical concerns,,no ethical concerns,2023,statement
1349,"This research was approved by the University of
Haifa IRB. We collected data from a social media
outlet, Reddit, in compliance with its terms of ser-
vice. For anonymity, we systematically replaced
all user IDs by unique IDs; we do not have, and
therefore do not distribute, any personal informa-
tion of the authors. With this additional level of
anonymization, we anticipate very minimal risk of
abuse or dual use of the data.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1350,"The goal of this paper is to extend Twitter user pro-
file inference from limited attributes to the open do-
main. We hope that this work will help to illustrate
how people express their attributes both explicitly
but especially also implicitly through their social
media posts. We also believe that the NLP commu-
nity has to produce detailed information about the
potential, pitfalls, and basic limitations of profile
inference methods so that we can establish stan-
dards to facilitate proper use of these technologies,
as well as be vigilant and effective at combating
nefarious applications.
Data and biases. To mitigate potential
distributional biases, we exhaustively collect enti-
ties from WikiData without selecting certain groups
of users. However, we acknowledge that the col-
lective information may still contain unintentional
social biases. As an example, one of the potential
issues is that people who have WikiData profiles
are public figures, which may not reflect the actual
distribution over general populations ( e.g., occu-
pation). Besides, as in Abid et al. (2021), large
language models themselves may contain biases.
WikiData is constantly edited by a large number of
WikiData contributors and maintainers. Although
we try to make our study as representative as possi-
ble, it is possible that a statement from WikiData
may not reflect the preception from certain groups
or individual (Shenoy et al., 2022). We would like
stakeholders to be aware of these issues and we
urge stakeholders to first investigate the effect of
potential issues before drawing any conclusions for
any individual or social group using this work.
Proper use v.s. improper use. The major dif-
ference between proper use and improper use is
whether the use case follows necessary legal and
ethical regulations or framework. For example,
Williams et al. (2017) propose an ethical framework
based on users’ consent to conduct Twitter social
research. If the information is not publicly avail-
able, one must obtain consent. Opt-out consent can
be used when the information is not sensitive, oth-
erwise opt-in consent is required. With proper regu-
lations, this work can be used to enhance personal-
ized user experience, investigate what stakeholders
to know to effectively protect personal information.
Sensitivity of personal information. In this
work we follow Twitter Developer Agreement
and Policy and remove sensitive personal infor-
mation. But it is still possible to infer sensitive
information indirectly. For example, “candidacy
in election” may be possibly used to infer politi-
cal affiliation although the affiliations are gener-
ally public for those people. Similarly, personal
pronouns, widely present in tweets, may also be
used to infer gender. Furthermore, combinations
of various sources might allow personal identifi-
cation (Sweeney, 2000a,b). Even though we do
not use private information in our work, based on
our results, we speculate that there are unobserved
risks of privacy loss for using Twitter. Therefore,
We ask that future work should fully comply with
regulations and any non-public or private results
should be properly protected (Kreuter et al., 2022).
We have set up the following protocol to ensure
the proper use and to prevent adverse impact:
•We believe that increasing the transparency of the
pipeline can help prevent potential social harm.
We plan to release all necessary resources for
research reproduction purposes so that others can
audit and verify it and prevent overestimation of
the model. We also provide a complete list of
attributes in Table 7 to increase the transparency.We are open to all further explorations that can
prevent unintended impacts.
•Our constructed dataset for profile inference re-
search is drawn solely from publicly available
WikiData and Twitter, where the ethical consider-
ation should be similar to other work using ency-
clopedia resources such as (Sun and Peng, 2021).
Furthermore, according to WikiData: Oversight,
non-public personal information are monitored
and removed by Wikidata. According to Wiki-
Data Term of Use, we can freely reuse and build
upon on WikiData. According to the Twitter
Developer Agreement and Policy, we will only
release IDs instead of actual content for non-
commercial research purposes from academic
institutions.
•To ensure the proper use of this work, we will not
release the data via a publicly available access
point. Instead, we will release the data based on
individual request and we will ask for consent
that 1) requesters are from research institutions
2) they will follow all the regulations when using
our work 3) they will not use the model to infer
non-public users unless obtained proper consent
from those users.",bias,,bias,2023,"statement, concerns, actions, advice"
1351,"Bertline’s performance does not seem to pose any
ethical difficulties or questions.
",no ethical concerns,,no ethical concerns,2023,statement
1352,"This work develops proactive transitions from chit-
chat to task-oriented dialogue in a unified dialogue
system. Proactivity is always desired during the
development of voice assistants. It can improve
user interactive experience and serve users more
efficiently. The dataset used in this work is public
available and manually collected. Furthermore, our
research is limited to a specific case, i.e, the user
starts casual chat and eventually switches to a task-
oriented service. However, more hidden challenges
and ethics issues should be discussed further in the
real scenarios. Would users prefer to be proactively
served if the dialogue system successfully detects
the user intention? Will they feel their privacy is
violated if the dialogue system proactively provides
task-related services? Such potential issues could
be addressed by asking for user consent before
providing the proactive interaction, which raises
the additional question how many users would turn
on such a feature from the start.
",privacy,,privacy,2023,"statement, concerns, suggestions"
1353,"We have considered eventual ethical impacts of
our developed tool and evaluated the following
questions :
•Does the paper describe how the technol-
ogy would be deployed in actual use cases?
Yes, our tool is dockerized, thusly easily de-
ployable. We give further details on the de-
ployment in our documentation9. Our aim is
to offer a tool which is easy to use for non-
coding experts.
•Does the task carried out by the computer
match how it would be deployed? Yes, it
is exactly as described in the paper, as we
created a tool directly destined for users for
annotation purposes.
•Does the paper address possible harms
when the technology is being used as in-
tended and functioning correctly? The tool
can only be used in a local setting for an op-
timised, faster annotation. No harm can be
directly induced by the tool itself. However,
the users could use the tool to annotate in an
harmful way data and maliciously spread the
dataset, with biases and false information.
9https://trusted-ai-labs.github.io/ALAMBIC/
•Does the paper address possible harms
when the technology is being used as in-
tended but giving incorrect results? As
the task concerns annotation, the only harm
brought by the tool would come from the in-
efficient selection of instances to be labelled,
which would only impact the performance of
the labelling, but not bring harm directly to
the human user.
•Does the paper address possible harms fol-
lowing from potential misuse of the tech-
nology? It highly depends on which type of
data the user wants to annotate. The misuse
would come from the data content and what
will do the user with this data, such as spread-
ing wrongly annotated datasets.
•If the system learns from user input once
deployed, does the paper describe checks
and limitations to the learning? The trained
model learns from the labelled dataset, which
is expanded by the user during the annotation
process. However, the learning process is lim-
ited to the annotation process or the analysis
process.124",misuse,,misuse,2023,"statement, concerns"
1354,"Potential Misuse: DiffuDetox can hypothetically
be used to obtain toxic sentences from non-toxic
sentences. However, the effectiveness of such a
scenario should be investigated.
Environmental Cost: We note that while our
work required extensive experiments to draw sound
conclusions, future work will be able to draw on
these insights and need not run as many large-scale
comparisons. Models in production may be trained
once using the most promising settings.
","misuse, environmental_impact",,"misuse, environmental_impact",2023,"concerns, suggestions"
1355,"We took great care in the compilation of the corpus
to include only material that can be published in
this form in order to be able to make the corpus
available to the scientiﬁc community.
The debates consist of material produced my mi-
nors (16-18 years). In the corpus, we anonymised
the names of the debaters throughout as ‘Speaker
1-4’. At the same time, we transcribed only de-
bates that had already been made public on the
Youtube canal of ‘Jugend debattiert’ (URL) in or-
der to include only material whose publication had
already been accepted by the respective speakers.
At the same time, we contacted the spokesperson
of ‘Jugend debattiert’ and got his consent on our
activities as far as they include the debates.
The other material is either taken from already
licenced corpora (for the parliament speeches and
the commentaries) or has an appropriate CC license.
Still, we contacted the authors to inform them about
our project and to conﬁrm their willingness to have
their material included in our corpus.
Acknowledgment
This work was funded by the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation)
– SFB 1412, 416591334.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1356,"We acknowledge that while our approach reaches
high scores on datasets aimed for individuals with
disabilities, further research and evaluation from
humans with specific disabilities listed in this paper
is crucial to determine the true effectiveness of our
approach in these scenarios.
",no ethical concerns,,no ethical concerns,2023,statement
1357,"In this paper, our method is only tested in intrinsic
evaluation settings where existing publicly avail-
able datasets have been used. It is not integrated
into any downstream application, although this type
of stylistic analysis could be potentially useful in
different settings.
",no ethical concerns,,no ethical concerns,2023,statement
1358,"The authors declare no competing interests.
",no ethical concerns,,no ethical concerns,2023,statement
1359,"Our work does not involve the creation of new
datasets. However, we would like to point out that
the existing dataset OpenPI is based on WikiHow,
which is primary crowdsourced (with partial expert
review). Thus some of the content is influenced
by the cultural and educational background of the
annotators. In our human cleaning, we recruit an-
notators from United States and Canada regions
only, which may also bring cultural bias to the con-
tent. In particular, some procedures are related to
healthcare and neither the procedure nor the model
output should be regarded as medical advice.
2https://www.wikihow.com/Main-Page
",bias,,bias,2023,"statement, concerns"
1360,"There are many large-scale pre-trained models used
in our paper like OFA, T5, RoBERTa, and CLIP.
Our method relies heavily on CLIP, which is pre-
trained on approximately 400M image-text pairs
crawled from the Internet. Since the pre-training
dataset is noisy, CLIP is likely to have potential
racial and gender bias. Therefore, if someone finds
our work interesting and would like to use it in a
specific environment, we suggest the user check
the potential bias before application. We think one
advantage of our work is we only utilize existing
pre-trained models and we don’t need to train any
new models. Compared to the energy-consuming
model training, our method can be more environ-
mentally friendly.
","racial bias, gender bias",,"racial, gender, bias",2023,"statement, concerns, advice"
1361,"When we collect the pre-training dataset, we en-
sure we respect the intellectual property of dataset
sources. All the ChartQA dataset we used for the
collection of chart-table pairs allows public access
for research. To ensure the reproducibility of our
experiment results, we provide details of the hy-
perparameter setting in our paper, and we will also
publish our code later. Our models can mislead
the public’s understanding of chart content due to
the potential bias from our training corpus. There-
fore, we don’t recommend using our model for any
real-world decision on chart images.
",bias,,bias,2023,"statement, actions, advice"
1362,"The proposed schema induction method does not
present any direct societal implications. As is ob-
served in Abid et al. (2021), the text generated by
GPT-3 might include undesired social bias. Ex-
tracting events and relations from text with such
social bias might potentially propagate the bias to
the induced schemas. Besides, there are risks of
malicious or unintended harmful uses of the gen-
erated schemas, for instance, the system might be
used to inquire about making a bomb or contriving
a terrorist attacks. Yet we believe that the proposed
method can beneﬁt various downstream NLP/NLU
tasks like event prediction, task-oriented dialogue
agents (Andreas et al., 2020) and risk detection
(Pohl et al., 2012).
","bias, misuse",,"bias, misuse",2023,"statement, concerns"
1363,"It is important to use the proposed personalized
news headline generation technique ethically and
responsibly. While the technique aims to improve
personalized content recommendations and opti-
mize the user experience, it could also be used
to generate headlines that are more likely to ap-
peal to an individual reader, potentially resulting
in a biased view of the news. In this paper, we
have taken necessary precautions to protect per-
sonal data. Our technique is based on a user’s read-
ing history, which is represented as a sequence of
recently viewed news headlines. No demographic
data such as age, gender, or location is used or
collected, due to privacy concerns. We encourage the community to continue to explore the potential
risks and implications of this technique.
",bias,,bias,2023,"concerns, suggestions"
1364,"The models available in PRIME QAmight inherit
bias based on available training data in public do-
main. Such bias, if any, is in general present in the
models contributed to PRIME QAand not speciﬁc
to PrimeQA. Therefore, the usage of PRIME QA
should be approached with the same caution as
with any NLP model.
PRIME QAsupports easy access for researchers
and developers to use state-of-the-art models and
even customize them on their own data. However,
PRIME QAdoes not control the type of data the
model will be exposed to in a custom environment.
The general assumption is that these models will
be used for rightful purposes in good faith.
",bias,,bias,2023,concerns
1365,"In this study, we aimed to create efficient
lightweight versions of large and less accessible
NLP models. This area of research aims to make
AI/NLP models more readily available, with fewer
computational resources required to run them and
potentially less environmental_impact.
This work does not use any private or sensitive
data and instead relies on widely used publicly
available datasets that have been utilised by other
researchers in the field with references provided in
the paper for more information. All the codes and
models are going to be made available for repro-
ducibility purposes.
",no ethical concerns,,no ethical concerns,2023,statement
1366,"Ethics Committee approval for the data collec-
tion and analysis for this work was given by the
World Health Organisation Ethics Review Com-
mittee (RPC571 and RPC572 on 25 April 2013).
National and/or institutional ethics committee ap-
proval was additionally obtained by participating
sites according to local requirements.
This work is a part of a global effort to accel-
erate and improve the collection and analysis of
data in the context of infectious disease outbreaks.
Rapid characterisation of novel infections is critical
to an effective public health response. The model
developed will be implemented in data aggrega-
tion and curation platforms for outbreak response –
supporting the understanding of the variety of data
collected by frontline responders. The challenges
of implementing robust data collection efforts in
a health emergency often result in non-standard
data using a wide range of terms. This is especially
the case in lower-resourced settings where data in-
frastructure is lacking. This work aims to improve
data processing, and will especially contribute to
lower-resource settings to improve health equity.
Funding and ",no ethical concerns,,no ethical concerns,2023,statement
1367,"In this study, we recognize the importance of ethi-
cal considerations in natural language processing
and dialogue systems research. Acknowledging
the potential biases in pre-trained LLMs and hu-
man judgments, we advocate for future research
to investigate and mitigate these biases in evalua-
tion metrics. We strive for fairness and inclusivity
by designing our method to be generalizable andadaptable to various settings. As researchers, we
are committed to responsible AI development and
contribute to the ongoing discourse on evaluating
dialogue systems, enabling the creation of more
effective and ethical AI-powered conversational
agents. We encourage the research community to
continue discussing ethical considerations and pro-
moting transparency in the field.
",bias,,bias,2023,"statement, concerns"
1368,"Our paper addresses the crucial issue of bias and
toxicity in language models by using causal meth-
ods. This work involved several ethical concerns,
that we address herein:
1. Language Restriction: This work addresses
the problem of detoxification of LMs for English
language, even though there more than 7000 lan-
guages globally (Joshi et al., 2020) and future
works should address more generalizable and mul-
tilingual solutions so that safety is promised for
diverse set of speakers and not limited to English
speakers (Weidinger et al., 2022)
2. Ethical LMs goal: We looked at toxicity in LMs
as an important dimension whereas there are other
facets for achieving the goal of ethical LM such
as moving towards greener methods by reducing
the carbon footprints as stressed in recent studies
(Strubell et al., 2019; Schwartz et al., 2020; Jobin
et al., 2019), privacy concerns (Carlini et al., 2021),
other issues discussed in (Bender et al., 2021).
3. Different Cultural Definitions of toxicity: Pre-
vious review works highlight the fact that toxic-
ity, hate and offense concepts are not defined con-
cretely as they can vary based on demographics
and different social groups (Paz et al., 2020; Yin
and Zubiaga, 2021). This may effect the perfor-
mance of toxicity detection methods( HATEBERT
andPERSPECTIVE API) used in this work. Such
differences between cultural definitions of toxicity
poses an ethical challenge (Jacobs and Wallach,
2021; Welbl et al., 2021).
4. Third party classifiers for toxicity detection:
Reliance on the third party classifiers for toxicity
detection can itself beat the purpose of fairness as
these systems are reported to be biased towards
certain protected groups and overestimate the prev-
elence of toxicity associated with them in the texts
(Davidson et al., 2019; Abid et al., 2021; Hutchin-
son et al., 2020; Dixon et al., 2018; Sap et al., 2019).
For most part, we take care of these by using causal
mechanisms but the ATE computation still involves
using a toxicity classifier (H ATEBERT) model.
5. Potential misuse: Any controlled generation
method runs the runs the risk of being reverse-
engineered, and this becomes even more crucial for
detoxification techniques. In order to amplify their
ideologies, extremists or terrorist groups could po-
tentially subvert these models by prompting them
to generate extremist, offensive and hateful content.
(McGuffie and Newhouse, 2020).
",misuse,,no ethical concerns,2023,statement
1369,"GPUSQ-TLM compression scheme is proven ef-
fective for various transformer-based language
models with encoder and decoder structures. It
will have a broad impact to encourage the study to
model compression and deployment improvement
in the NLP community.
We should also point out that the GPUSQ-TLM
compression scheme uses knowledge distillation.
SoGPUSQ-TLM needs more on-chip memory
consumption during the compression process be-
cause we need a teacher model for distillation. For
compressing a huge transformer-based language
model, we may need more GPUs to work in parallel
to hold both the teacher model and the target model.
SoGPUSQ-TLM may cost more power consump-
tion during the compression process, which is not
environment-friendly. But the compressed models
are more efficient than the original dense model,
leading to less power consumption during the in-
ference process. Moreover, the time and resources
spent in model deployment will far outweigh the re-
sources spent in training over the model’s life. This
point turns the time and resource increase from a
simple trade-off between training and inference to
a net positive, as overall resource consumption isreduced over the whole life of the model.
",no ethical concerns,,no ethical concerns,2023,statement
1370,"We do not identify ethical risks connected to this
work.
",no ethical concerns,,no ethical concerns,2023,statement
1371,"The learner data used in this study are anonymized
by Settles et al. (2018) and, to the best of our knowl-
edge, do not contain sensitive information. We
foresee no further ethical or privacy concerns with
the work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1372,"Our experimental datasets, CNN/DailyMail, arXiv,
and QMSum, are well-established and publicly
available. Datasets construction and annotation
are consistent with the intellectual property and
privacy rights of the original authors. The scien-
tific artifacts we used are available for research
with permissive licenses, including ROUGE and
Transformers from HuggingFace. The use of these
artifacts is consistent with their intended use. The
task of our work is a classic NLP task, text sum-
marization. Considering all the datasets are public
available, we think there are no potential risks for
this work.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1373,"Faithfulness metrics help reduce the amount of in-
correct information generated by NLG systems, re-
ducing the risk associated which such generations.
However, faulty or unreliable faithfulness metrics
might cause harm by incorrectly classifying faithful
content as unfaithful and vice versa.
We run all experiments on publicly available
data that has been specifically constructed for faith-
fulness evaluation. The underlying publication has
been published at a conference whose review pro-
cess involved an ethics review. For a specific dis-
cussion of the human effort involved in creation
of the datasets we refer the reader to the original
publications.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1374,"Our method relies on a pre-trained AMR parser,
which is built using pre-trained large language mod-
els (AMRBART &RoBERTa ). It is known that the
models trained with a large text corpus may capture
the bias reflecting the training data. Therefore, it is
possible that the AMR graph used in our method
could contain certain biases. We suggest carefully
examining the potential bias before applying AM-
PERE to any real-world applications.
",bias,,bias,2023,"concerns, advice"
1375,"Given that our dataset is in Chinese and requires a
profound understanding of forward referencing for
annotation and evaluation, we carefully selected
annotators from our lab who specialize in NLP-
related research and possess knowledge in linguis-
tics. To ensure fairness, we provided all annotators
with a payment of $6.66 per hour, which is 10%
higher than the minimum hourly wage requirement
in Taiwan.
During the data annotation process, we intro-
duced the concept of forward referencing to the
annotators, along with relevant examples. Only
annotators who achieved an accuracy rate of over
80% were eligible to perform the actual annotation
task. It’s important to note that we solely asked
annotators to label the ""type of forward reference,""
which is well-defined, and not to assess the ac-
curacy or truthfulness of the news articles. With
five annotators who successfully passed the pretest,
combined with the relatively objective nature of
labeling forward reference types, we believe any
potential bias during the data annotation process is
minimal.
For the evaluation phase, an additional five anno-
tators were tasked with determining the preferable
title among two options, based on attractiveness,
faithfulness, and fluency. These annotators are dif-
ferent from those who labeled the data to ensure
a blind test. Although this task involves a greater
level of subjectivity, we provided average statistics
based on the assessments of the five annotators.
Additionally, we maintained a blind test by recruit-
ing separate evaluators and randomly shuffling the
order of the two titles for each trial. This evaluation
protocol aligns with standard practices employed
in the research community, and we believe it effec-
tively minimizes potential biases.
It is also important to note that we are not really
learning to mimic fake news , by taking fake news
headlines as the ground truth reference. Instead,
we seek to learn the writing techniques that are
often used in fake news to attract readers. As we
are aware of the risk of producing misinformation,
we want to again highlight the importance of the
faithfulness check. HonestBait was designed only
to assist journalists as a reference to write faith-
ful headlines that users prefer for verified news.
Even if we propose using a faithfulness scorer to
increase fidelity, its nature, similar to attractive
headline generation systems, still exhibits the risk
that HonestBait could be used by malicious users
to generate sensational headlines for fake news.
Additionally, HonestBait may misjudge offensive
or unethical headlines to be a headline that users
would prefer. Our goal is to fight fire with fire by
leveraging fake news as learning material to fight
against misinformation, by encouraging users to
read verified news. We call on users not to abuse
HonestBait to produce false information.",no ethical concerns,,no ethical concerns,2023,"statement, actions, advice"
1376,"We re-annotate the Levy/Holt Dataset which is a
publicly available dataset for entailment graph eval-
uation. Annotators receive a competitive pay of
about 100 yuan per hour under the agreement of
the institute, which is more than 4 times the lo-
cal minimum wage. The annotation complies with
the ACL Code of Ethics. The sentences used in
annotation are generated from the original dataset
and we do not incorporate external content into the
sentences. However, there may still be sentences
containing potentially improper content, which do
not reflect the views or stances of the authors. The
re-annotation results are confirmed by the majority
voting of annotators, and may still contain natural
errors. Further usage of the re-annotated dataset
should be aware of the limitation and the authors
are not responsible for any issues in further usage
of this dataset.
","human-annotation error, offensive content",,"human-annotation error, offensive content",2023,"statement, concerns, actions"
1377,"This paper will not pose any ethical problems. First,
logical data-to-text generation is an old task in nat-
ural language processing, and several papers about
this task are published at ACL conferences. Sec-
ond, the datasets used in this paper have been used
in previous papers.
",no ethical concerns,,no ethical concerns,2023,statement
1378,"The dialogue data would inevitably contain private
information about the interlocutors. We take care-
ful consideration of this problem: (1) all data in our
experiments are publicly available and anonymized
by the original dataset provider. The license for
SAMSum dataset is CC BY-NC-ND 4.0 and for Di-
alogSum MIT License . For MediaSum, it adheres
to only-for-research-purpose guideline from the Na-
tional Public Radio; (2) we do not use online user
data to train our model and we would use an addi-
tional rule-based system to double-check whether
our model output contains harmful and prejudicial
discrimination when we use it for production.
Acknowlegement
This work was supported by the National Natu-
ral Science Foundation of China (NSFC Grant No.
T2293773 & No. 62122089 & No. 61876196),
the National Key Research and Development Pro-
gram of China (No. 2020AAA0106600), Bei-
jing Outstanding Young Scientist Program (NO.BJJWZYJH012019100020098), and Intelligent
Social Governance Platform, Major Innovation
& Planning Interdisciplinary Platform for the
“Double-First Class” Initiative, Renmin University
of China. Rui Yan is also supported by Beijing
Academy of Artificial Intelligence (BAAI).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1379,"The training corpora including the Lang-8, NUCLE
and the BEA-2019 test data and CoNLL-2014 test
data used for evaluating our framework are publicly
available and don’t pose privacy issues. The algo-
rithm that we propose does not introduce ethical or
social bias.
",no ethical concerns,,no ethical concerns,2023,statement
1380,"In this research, We have made every effort to en-
sure that our study adheres to the ACM Ethical
Principles. All data used in this study have been
publicly available. This work may inspire the fol-
lowing research of mathematical reasoning. The
potential risk of this work is that students may use
the system as an auto-problem-solving tool to cheat
on exams or assignments.
",misuse,,misuse,2023,"statement, concerns"
1381,"The use of our approach could result in improved
dialogue systems that enhance the quality of life
for many individuals, especially in light of the
widespread use of AI in everyday life. For instance,
a more effective chatbot integrated with electronic
gadgets will boost both productivity and user expe-
rience. On the other hand, the implementation of
conversation systems could result in employment
losses in some domains such as call centers.
",unemployment,,unemployment,2023,"statement, concerns"
1382,"We believe that this work meets the ACL Code of
Ethics as it provides an already trained zero-shot
text classifier that can be used in an endless number
of situations that would otherwise require a task-
specific fine-tuning. Moreover, one of the main
findings of this work is that entailment-based text
classifiers should not be over-trained as it nega-
tively affects their final performance. This should
encourage fellow NLP practitioners to shorten the
training time of their ZSTCs thus minimizing their
carbon footprint, which is in line with the idea of
moving towards more sustainable language models.
",no ethical concerns,,no ethical concerns,2023,statement
1383,"We anticipate no substantial ethical issues arising
due to our work on rule augmentation for Neuro-
Symbolic KGC. Our work relies on a set of rules
generated from another source to perform augmen-
tation. This may result in the augmented rule set
exaggerating the effect of malicious or biased rules
in the original rule set.
","harmfulness, bias",,"harmfulness, bias",2023,"statement, concerns"
1384,"We assembled our biographical dataset for the
grounded source consistent with Project Gutenberg
permissions and terms of use. Emanating personal
identiﬁable information of the individual history
is unavoidable when obtained from biographical
literary. However, improving the faithfulness of
automatically generated summaries is essential to
ensure reliable and trusted factual accuracy. To the
extent of our judgment, produced narrative sum-
maries are free of harmful or offensive content, yet
we plan to restrict our dataset for research use only.
",no ethical concerns,,no ethical concerns,2023,statement
1385,"Countering stereotypical views and statements can
have a tremendous positive effect on making online
spaces more inclusive and safe for everyone and
reducing prejudice and discrimination. However,
certain responses can do more harm than good.
Addressing stereotypical views in a hostile or of-
fensive way only fuels the conflict. Producing and
perpetuating new stereotypes while denouncing the
old ones may create a vicious cycle. To reduce
the possible negative effects, care should be exer-
cised in which automatic techniques to use and how
to deploy them in real-life applications. Wherever
possible, an AI-in-the-loop paradigm should be em-
ployed where users are assisted by the technology,
but remain in control.","bias, toxic content",,"bias, toxic content",2023,"concerns, statement"
1386,"For this research, we obtained ethics approval
from the research ethics board at the University
of Ottawa. Moreover, The UMD dataset was
used with authorization from its creators, and we
adhered to the terms of use and ethical standards16
provided by them.
The use of ChatGPT for suicide risk assessment
raises several ethical considerations. Firstly, there
is the issue of safety and reliability. While
ChatGPT has shown promise in natural language
processing tasks, it is not infallible and can make
mistakes or generate false responses. Due to the
sensitivity of the suicide detection task, these errors
might lead to severe harm to individuals at risk.
Therefore, it is important to 1) thoroughly test and
validate the accuracy of the model before using it
for suicide risk assessment and 2) deploy it in an
expert-in-the-loop setting.
Secondly, there is the issue of privacy and
confidentiality. Suicide risk assessment might
involve sensitive personal information, and there
is a risk that the information processed by the
ChatGPT could be mishandled or disclosed to
unauthorized parties. It is important to ensure
that proper security measures are in place to
protect the privacy of individuals who interact with
the ChatGPT. Automatic de-identification of data
before feeding it to ChatGPT could be a potential
solution, but it will bring in its own limitations. In
any case, obtaining user consent is crucial before
engaging in the automatic processing of data by
16The University of Maryland Reddit Suicidality DatasetChatGPT. It is essential to respect individuals’
privacy and ensure that they have given their
explicit permission before their data is collected,
processed, or shared.
Thirdly, there is the issue of potential
psychological harm. Suicide risk assessment can
be a sensitive and emotional topic. There is
a risk that individuals whose data is assessed
by ChatGPT could experience distress or other
negative emotions due to the assessment. It is
important to have appropriate support mechanisms
in place, such as access to mental health
professionals or crisis hotlines, to assist individuals
who may be in distress.
","inaccuracies, harmfulness, privacy",,"inaccuracies, harmfulness, privacy",2023,"concerns, advice"
1387,"We aimed to create a balanced, bias-free dataset
regarding demographic and socioeconomic factors.
We picked a wide range of languages, even those
with limited resources, and we also ensured that
the categories were diversified. Humans curate the
majority of information on Wikipedia. Using un-
restricted automated tools for edits might result in
biased information. For this reason, we adhere to
the ""human in the loop"" methodology (Smith et al.,
2020) for editing Wikipedia. Additionally, we fol-
low Wikipedia editing guidelines9, rule set10, and
policies11for all manual edits. Therefore, we ask
the community to use our method only as a rec-
ommendation tool for revising Wikipedia. As a re-
sult, we ask that the community utilize INFOSYNC
strictly for scientific and non-commercial purposes
from this point forward.
",no ethical concerns,,no ethical concerns,2023,actions
1388,"In Appendix D, we note the costs of hiring domain
experts for annotation.
Large language models (such as GPT3-D3 ) have
been shown capable of generating concise and ﬂu-
ent summaries. But these often contain factual in-
accuracies. This poses unique risks in the domain
of medicine, where inaccurate summaries of pub-
lished evidence have the potential to (mis-)inform
patient care. This work has attempted to empiri-
cally assess the tendency of models to introduce
inaccuracies into summaries of medical literatureby enlisting domain experts to identify and char-
acterize omissions and errors in model generated
summaries. Understanding such issues is a ﬁrst
step toward designing methods to mitigate them.
While we found that GPT3-D3 appears to pro-
duce summaries of single biomedical article ab-
stracts that are reasonably factual, relying on such
outputs still poses risks, and even in this setting we
would caution against trusting model outputs with-
out further veriﬁcation at present. Moreover, we
found that in the multi-document case—i.e., on the
task of synthesizing evidence reported across mul-
tiple clinical trials— GPT3-D3 struggles to provide
synopses that agree with reference (expert written)
summaries. In sum, despite their ability to produce
consistently plausible outputs, our view is that sum-
maries of medical literature produced by LLMs
should not yet be used to directly inform care given
the risks of factual inaccuracies. More research is
needed to better characterize the kinds of mistakes
such models make, and ultimately to mitigate them.
",no ethical concerns,,no ethical concerns,2023,"statement, summary-conclusion"
1389,"We present a system for organizing large result lists
into a browsable hierarchy. In general, consuming
a hierarchy is more effective than consuming a very
long list. However, hierarchies can hide items, es-
pecially if the items are misplaced in an unexpected
branch—which our system sometimes does (albeit
rarely). In situations where consuming the entire
information is crucial and the cost of missing an
item is prohibitive or dangerous, a flat list would
be the safer choice.",model inaccuracies,,model inaccuracies,2023,"statement, concerns"
1390,"Given the impact of our proposed contributions on
the financial community in particular, and wider
research community in general, our dataset and
codes are publicly available. Our labels are derived
from public/open domain. Still, we may ask users,
intending to access our data, to provide a self decla-
ration that the data is to be used solely for research
purposes.
",no ethical concerns,,no ethical concerns,2023,statement
1391,"This research utilized a deidentified dataset that
does not include any protected health informa-
tion. This dataset operates in compliance with the
PhysioNet Credential Health Data Use Agreement
(v1.5.0). All experiments conducted adhered to the
guidelines outlined in the PhysioNet Credentialed
Health Data License Agreement. Additionally, this
study has been deemed exempt from human sub-
jects research.
",no ethical concerns,,no ethical concerns,2023,statement
1392,"In this paper we showed that performing clean la-
bel attacks in NLP is easier using our proposed
approach of Adversarial Clean Label attack. This,
of course, has am important ethical concern. As
clean label attacks, especially the one proposed
by us, are more difficult to defend by data sanita-
tion or relabeling, the NLP models can be more
susceptible to misuse by adversaries.
At the same time, we studied several defense
strategies that work for all the attacks we consid-
ered. Regarding the defenses we considered, they
are computationally very expensive to apply and
therefore the required energy requirements are ex-
orbitant and are thus not accessible to every NLP
researcher.
",misuse,,misuse,2023,statement
1393,"Certain ethical considerations should be taken into
account while creating automated systems for pro-
cessing doctor-patient conversations. The common
faults of the proposed systems should be disclosed
to system users. Users should be trained to properly
use and identify common mistakes of the systems.
Since the data to be processed is medical records, it
is essential that both data and background models
should be stored within strong security measures.
Lastly, patients and doctors should be informed
that their conversations are recorded and may be
used by the automated systems.
6 Discussion and Future Scope
In this paper, we explored the capabilities of LLMs
on summarization and classification of doctor-
patient dialogues. We experimented for task A and
task B but managed to have an official submission
on task A. We documented our thought processes
and approaches and stated our results. We obtained
results that both supported and contradicted our hy-
pothesis. Due to hardware and budget limitations
we did not have the chance to explore latest large
models. The obvious future work would be on ap-
plying public instruct based models if the hardware
capacity is enough or private instruct based models
if the budget allows. More future work could be on
preprocessing of the dialogues. Intuitive postpro-
cessing approaches could also be explored.
7 ",no ethical concerns,,no ethical concerns,2023,"actions, advice"
1394,"Use of human data: While we did not collect
any new human data ourselves, many of our anal-
yses involved the use of prior datasets within the
CHILDES database. All of these datasets were
collected in accordance with IRB policies at the
institutions of the data collectors, and all followed
standard practices in obtaining informed consent
and deidentifying data.9
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1395,"We made use of an annotated dataset from the
SemEval-2023(Kirk et al., 2023) Task 10 competi-
tion that was gathered in accordance with ethical
standards. The remarks in the dataset were sex-
ist, which is bad for the people who were being
targeted. We made sure our classification algo-
rithms weren’t applied to expose or hurt those who
were the targets of the sexist remarks in order to
reduce the danger of harm. We used the BERT
transformer model to classify and categorize the
text, which is a widely accepted approach in theNLP community. We try to recognize the impor-
tance of addressing online sexism to create a more
inclusive and equitable society. In order to improve
awareness campaigns and put stop to this oppres-
sion, our study intends to help develop automated
technologies that can help with the analysis of sex-
ism. We envision our work as a modest first step
toward achieving a more just and equal online en-
vironment for everyone.
",harmfulness,,harmfulness,2023,"statement, actions"
1396,"Potential risks While our work on overcoming
spurious cues is related to the idea of debiasing
models, it is important to note that our results do
not indicate that our method is the best to tackle so-
cially harmful biases against marginalized groups,
like gender or racial biases. We have not run any
experiments following this direction, and it is im-
portant to make this distinction so that the reader
does not misunderstand the goal of this paper.
Intended Use Our models and methods shown
here are for research purposes only. They should
not be deployed in the real world as solutions with-
out further evaluation.
",bias,,bias,2023,"statement, concerns"
1397,"We also see a potential lack of language inclusive-
ness within our work, as we addressed in the Lim-
itation section that ESCO mostly covers Europe
(and the Arabic language). Nevertheless, we see
ESCOXLM-R as a step towards inclusiveness, due
to JAD frequently being English-only. In addi-
tion, to the best of our knowledge, ESCO itself
is devoid of any gendered language, specifically,
pronouns and other gender-specific terms in, e.g.,
occupations. However, we acknowledge that LMs
such as ESCOXLM-R could potentially be exploited
in the process of hiring candidates for a specific
job with unintended consequences (unconscious
bias and dual use). There exists active research
on fairer recommender systems (e.g., bias mitiga-
tion) for human resources (e.g., Mujtaba and Ma-
hapatra, 2019; Raghavan et al., 2020; Deshpande
et al., 2020; Köchling and Wehner, 2020; Sánchez-
Monedero et al., 2020; Wilson et al., 2021; van Els
et al., 2022; Arafan et al., 2022).
","language inclusiveness, exploitation",,"language inclusiveness, exploitation",2023,"statement, concerns"
1398,"We have released a processed version of an already
public online forum dataset, in a manner consis-
tent with the terms of the license, which require
attribution of all posts (§3). The models we have
presented are intended only as baselines for future
research, not for deployment. Models should be
carefully stress-tested for undesirable heuristics/
biases before deployment. Systems for the gen-
eration task, in particular, would risk misleading
language learners with plausible but incorrect an-
swers, so it is important to not deploy a generation
system until it is approximately as reliable as exist-
ing non-automated alternatives, and to present the
output with caveats. Potential biases reflecting the
demographics of authors represented in the training
data (in terms of native language, level of English
proficiency, etc.) also need to be considered if mod-
els are deployed for different target populations.","bias, misinformation",,"bias, bias, misinformation",2023,"statement, concerns, advice"
1399,"The task and model proposed in the paper is aimed
at broadening the scope of TabularQA research.
All the datasets used in this research, apart from
our synthetic data, are publicly available in peer-
reviewed articles and referenced in this paper. The
synthetic SQL dataset we release was generated
over a standard benchmark database which has
been annotated by 11 Yale students as mentioned
in the original paper. Our synthetic samples use
templates annotated by the authors of this work
and do not use any user-specific data or informa-
tion. We will be providing open access to our
datasets for use in future research under the MIT
License. All datasets, including the synthetic pre-
training dataset and all datasets adapted for multi-
table QA will be released. Our model is built over
tapex-base which in turn has been trained over
bart-base . Our work did not explicitly handle
any bias which exists in the aforementioned pre-
trained models.

",bias,,bias,2023,"statement, actions"
1400,"The city council meetings included in this dataset
are publicly accessible. We obtain meeting videos,
minutes documents, and other metadata from pub-
licly available sources. We consult with our legal
team and reach out to city councils as necessary to
ensure compliance with licensing and data policies.
We release this dataset to facilitate the development
of meeting summarization systems and have made
efforts to ensure that the dataset does not include
confidential information. Our dataset is intended
for research purposes only.
",no ethical concerns,,no ethical concerns,2023,actions
1401,"We are aware that using web data is inevitably con-
nected with questions of respecting the intellectual
property and privacy rights of the original authors
of the texts. In this paper, we used web corpora
that have been collected by crawling the national
top-level web domains. Only freely accessible texts
were included in the corpora to avoid inclusion of
sensitive data. Since the datasets were collected
automatically and are too large to review manu-
ally, it is possible that the datasets include some
texts whose authors do not consent to be included.
However, in our paper, we only use the overall
characteristics of the texts by extracting the most
frequent language-specific words and do not exam-
ine the texts more closely or produce systems that
could abuse personal information or intellectual
property rights.
Secondly, as mentioned in
",intellectual property,,intellectual property,2023,"concerns, actions, suggestions"
1402,"We are aware that collecting texts from the web
can raise questions of respecting the intellectual
property and privacy rights of the original authors
of the texts. The web corpora, analysed in this pa-
per, have been collected by crawling the national
top-level domains. To assure that no sensitive data
would be included, only texts that have been freely
accessible were included in the corpora. We are
aware that the datasets might still include some
texts that the authors do not consent to be included.
To mitigate this, the datasets are published with a
notice, which informs the authors of the text that
the texts can be taken out of the corpora upon their
request. Secondly, for privacy issues, the sentences
in the published corpora that contain personal in-
formation are flagged, so that the corpora users can
leave them out of their research if the nature of their
study would reveal this information. In our paper,
we look into and report on the overall character-
istics of the texts and do not examine texts more
closely or produce systems which could abuse per-
sonal information or intellectual property rights.
That is why anonymisation or additional filtering
was not necessary.
Secondly, as mentioned in
",intellectual property,,intellectual property,2023,"concerns, actions, suggestions"
1403,"The VWP dataset we used is publicly accessible. It
is described in a paper (Anonymous, 2023) that is
published on TACL under the CC-BY license.
The potential risk of this work is that it can be
used to generate harmful content. There could be
potentially offensive images in VWP dataset be-
cause it is based on movies. Our model might
suffer from the risk of learning unwanted correla-
tions between these images and offensive words in
the underlying language model GPT-2. Because ithas been found that there is a significant amount of
unreliable or toxic content in the training data of
GPT-2 (Gehman et al., 2020). Although we haven’t
seen any generated in our human evaluations, our
proposed model and code are for research purposes
only.
","harmful content, offensive content, toxic content, unreliable content",,"harmful content, offensive content, toxic content, unreliable content",2023,"statement, concerns"
1404,"The VWP dataset we used is publicly accessible. It
is described in a paper (Hong et al., 2023) that is
published in TACL under the CC-BY license.
The potential risk of this work is that it can be
used to generate harmful content. There could bepotentially offensive images in VWP dataset be-
cause it is based on movies. Our model might
suffer from the risk of learning unwanted correla-
tions between these images and offensive words in
the underlying language model GPT-2 because it
has been found that there is a significant amount of
unreliable or toxic content in the training data of
GPT-2 (Gehman et al., 2020). Although we have
not seen any generated in our human evaluations,
our proposed model and code are for research pur-
poses only.
","harmful content, offensive content, toxic content, unreliable content",,"harmful content, offensive content, toxic content, unreliable content",2023,"statement, concerns"
1405,"The presented work could help to more accurately
extract information to verify statements. However,
the system relies on contrasting facts using a ""truth""
database. The existence of such a resource is not
a trivial assumption to make, especially if we con-
sider open sources of information such as social
networks in which virtually anyone can add con-
tent. Consequently, and in addition to the fact that
no system is perfect, we discourage the usage of
our work as any kind of ground truth for any fact
verification task if the reference database cannot be
checked by experts both in terms of accuracy and
possible biases.
","bias, model inaccuracies",,"bias, model inaccuracies",2023,"concerns, advice"
1406,"The presented work could help to more accurately
extract information to verify statements. However,
the system relies on contrasting facts using a ""truth""
database. The existence of such a resource is not
a trivial assumption to make, especially if we con-
sider open sources of information such as social
networks in which virtually anyone can add con-
tent. Consequently, and in addition to the fact that
no system is perfect, we discourage the usage of
our work as any kind of ground truth for any fact
verification task if the reference database cannot be
checked by experts both in terms of accuracy and
possible biases.
","bias, model inaccuracies",,"bias, model inaccuracies",2023,"concerns, advice"
1407,"The original news articles from MFC (Card et al.,
2015) used in this work were obtained from Lexis
Nexis under the institutional licence held by the
University of Melbourne.
",no ethical concerns,,no ethical concerns,2023,statement
1408,"This study was approved by the University of Mel-
bourne ethics board (Human Ethics Committee
LNR 3B), Reference Number 2023-22109-37029-
4, and data acquisition and analysis has been taken
out to the according ethical standards. We hired
four local annotators who were paid an hourly rate
of $53 AU in line with the casual research assistant
hourly rates set up in the University of Melbourne
collective agreement.
We will release the Narrative Framing Corpus
comprising of 428 news articles annotated with
frame labels, entities, their narrative roles and stake-
holder categories. We also publish the raw (non-
aggregated) annotations. Our data set builds on
news articles from the NELA corpora 2017-2019,
which were released to the public domain (license
CC0 1.0).14We release our code and Narrative
Frames Corpus under a MIT license.
",no ethical concerns,,no ethical concerns,2023,statement
1409,"All mechanical turk experiments conducted in this
paper were approved by internal ethics reviewboard from our institution. (Ethics ID Number:
21961). Our evaluators were paid based on an es-
timated US$14.83 per hour rate. For each dataset,
we estimate the time they would spend and vary
the payment according to the estimated time. Each
HIT contains 7 stories (5 stories to be evaluated
and 2 controlled stories to control the evaluation
quality on AMT). We pay US$2.50 per HIT for
ROC, US$3.50 for WP and US$4.50 for CNN.
We remind the workers in our consent form that
the potential risks about this work, which they
might have to read and evaluate stories with filthy
words or offended storyline and they are welcome
to quit the task and we will still pay them according
to the efforts they spend.
",no ethical concerns,,no ethical concerns,2023,actions
1410,"The data used for this study was cleaned and
anonymized to remove any personal and sensitive
information before conducting the reported experi-
ments.
",no ethical concerns,,no ethical concerns,2023,actions
1411,"We see three important ethical considerations
around our paper. The first consideration is related
to the use of large proprietory language models
(GPT-3). Apart from the reproducibility limita-
tions resulting from the use of GPT-3 discussed
above, there are more general ethical questions
surrounding the use of GPT-3 and similar models,for example the high energy usage and resulting
carbon emissions, and societal questions around
the oligopoly on state-of-the-art language models
that is currently in the hands of a handful of large
US-based companies.
The second consideration relates to the task that
we introduce: while we see perspective transfer
models as a valuable tool for studying how lan-
guage ‘frames’ (social) reality that could also have
practical applications, for example in journalism,
we strongly believe that any such applications must
be approached with extreme care. The models that
we introduce are scientific analysis tools that could
be used to suggest alternative viewpoints on an
event, but we believe that generations should not
be seen as necessarily reflecting a ‘true’ or ‘better’
perspective, and should not used in a prescriptive
way (i.e. used to tell someone how to write). We
believe that the authors (journalists or others) of
any text ultimately bear exclusive responsibility
for the views, perspectives and (implicit) values
expressed in it, and should be careful in making
use of texts (re-)written by computers, such as the
ones produced by our proposed models.
Finally, we are aware that our task domain
(femicide/gender-based violence) is a societally
and emotionally loaded topic, and that the texts
contained in our dataset and produced by our mod-
els might be disturbing. In particular, in some cases,
models may produce graphic descriptions of vio-
lence and/or produce questionable moral judge-
ments (e.g., we have occasionally seen statements
such as “the perpetrator of this horrible crime does
not have the right to live” spontaneously produced
by some of the models), and potential users of ap-
plications of the model should be aware of this. For
the purposes of this paper, the only people external
to the research team who have been extensively ex-
posed to model outputs were the annotators in our
human evaluation study. In the introduction page of
our online questionnaire, annotators were warned
about the sensitive nature of the topic and advised
that they could stop their participation at any time
if they felt uncomfortable and could contact the
authors with any questions. Prior to running the on-
line questionnaire we have requested and obtained
ethical approval by the Ethical Review Committee
of our research institution.","carbon emissions, offensive content",,"carbon emissions, offensive content",2023,"concerns, advice"
1412,"The legal and biomedical ﬁelds are both highly
sensitive and have high impact on human life. In
this work, we have ensured that the data we work
with is sourced in compliance with the relevant
regulations and are fully anonymized where neces-
sary. The application of multi-label classiﬁcation
to this data carries no obvious risk as it can ease
the processing and categorization of documents in
these domains, without having any direct impact on
individuals involved in legal and medical matters.",no ethical concerns,,no ethical concerns,2023,statement
1413,"This work contributes to open source progress
in RST discourse parsing, an area which has re-
ceived less attention than some other NLP tasks,
and which, at least in English, is currently suffer-
ing from a skewed focus towards the ‘standard’
language of 1990-era Wall Street Journal writing.
Previous work has shown that NLP systems re-
tain strong lexical biases mirroring both period and
author demographics (Shah et al., 2020), mean-
ing that if language technologies are not pushed
to cover diverse data types robustly, they will in-
evitably perform more poorly on ‘non-standard’
data, with possible discriminatory effects on under-
represented populations ranging from the political
(think opinion mining social media to guide policy)
to financial (e.g. higher/lower search hit rates for
YouTube videos by small businesses). By promot-
ing higher quality treatments of diverse language
samples in this study covering Reddit, YouTubevlogs, and demographically diverse conversations,
we hope to help level the playing field across text-
types, demographics, and domains.
We recognize that NLP research has a computing
cost and carbon footprint, which motivates us to
release the trained models in this work (preventing
the need to retrain similar models), and to avoid
extensive hyperparameter optimization which may
not generalize to applications in the wild. Spe-
cific model configurations such as hyperparameters
and validation performance for the reported test
results of RST-DT and GUM are detailed in Ap-
pendix C.
Finally, we also recognize that NLP tools can
be used to do harm. However, we expect that the
type of analysis promoted here will do more good
than harm by steering tool development away from
adhering closely to outdated and narrow-domain
data, which this work aims to broaden. Given that
discourse parsers already exist, we view the push
to reduce topical and authorial bias, as well as the
public release of more resources, as net positives.
","environmental_impact, computational cost, bias",,"environmental_impact, computing cost, bias",2023,"concerns, actions"
1414,"The data produced in this paper is made openly
available in accordance with the original licenses
of the underlying resources and academic fair use.
we are keenly aware that NLP, and particularly
NLG technology can be misused adversely, for ex-
ample to generate fake news, we believe the risks
posed by models which are not ‘reality-checked’
outweigh those associated with improving mod-
els to prevent factuality and generalization issues
across domains. The latter issue is particularly
relevant, since technologies limited to particular
domains and styles will primarily benefit actors in
sectors engaged with that data (e.g. news, for ex-
ample, financial reporting), while underserving the
public in other areas (e.g. computer-mediated com-
munication). We therefore concur with this year’s
ACL theme that work towards ‘reality checking’
our outputs is a net positive.
",misuse,,misuse,2023,concerns
1415,"One potential risk of applying REGENis that the
generic corpus used in our experiments may con-
tain harmful information as they were crawled
from the Internet that are only filtered with some
rules (Gehman et al., 2020). As a result, they may
contain text exhibiting biases that are undesirable
for target tasks. To alleviate this issue, we recom-
mend the potential users to first use bias reduction
and correction techniques (Schick et al., 2021) to
remove biased text from the corpus to mitigate the
risks of the curated dataset.
",bias,,bias,2023,"concerns, actions"
1416,"Our work utilizes pre-trained language model and
external knowledge graph to build question answer-
ing systems. However, pre-trained language mod-
els can include biases (Shwartz and Choi, 2020)
and knowledge graph, e.g. ConceptNet, has been
found to contain representational harms (Mehrabi
et al., 2021), which can cause these question an-
swering systems to inherit these potential biases
and harms. Therefore, additional procedures, e.g.
declining inappropriate inputs and filtering harmful
outputs, must be taken before real-world deploy-
ment.
","bias, harmfulness",,"bias, harmfulness",2023,"concerns, advice"
1417,"In tasks involving subjective evaluations such as
emotion recognition, it is common to employ mul-
tiple human annotators to give multiple annotations
to each data instance. When annotators disagree,
majority voting and averaging are commonly used
to derive single ground truth labels for training su-
pervised machine learning systems. However, in
many subjective tasks, there is usually no single
“correct” answer. By enforcing a single ground
truth, there’s a potential risk of ignoring the valu-
able nuance in each annotator’s evaluation and their
disagreements. This can cause minority views to be
under-represented. The DEER approach proposed
in this work could be beneficial to this concern as it
models uncertainty in annotator disagreements and
provides some explainability of the predictions.
While our method helps preserve minority per-
spectives, misuse of this technique might lead to
ethical concerns. Emotion recognition is at risk of
exposing a person’s inner state to others and this in-
formation could be abused. Furthermore, since the
proposed approach takes each annotation into con-
sideration, it is important to protect the anonymity
of annotators.
",misuse,,misuse,2023,"concerns, actions"
1418,"Using large language models in a clinical domain
has inherent risks. As demonstrated in this paper,
LLMs sometimes hallucinate and fabricate false
answers to questions posed about research articles.
If done at scale, these extraction errors could propa-
gate to downstream analyses, potentially leading to
false conclusions. While LLMs may be able to sig-
nificantly speed the process of human data curation
and even help in detecting errors, they still require
manual verification of results to ensure high data
quality.
","hallucinations, nonfactual content",,"hallucinations, non-factual content",2023,concerns
1419,"Our article presents a novel approach and has not
been published in whole or in part elsewhere. The
data used to train the model does not imply any
violation of privacy. The potential negative social
impacts from this work are similar to any other
NLP models. Question answering models could
potentially be used to create malicious chatbots.
This work does not include experimentation with
humans or animals.
",misuse,,misuse,2023,"concerns, statement"
1420,"We are not aware of ethical issues associated with
the texts used in this work. Students participated
in the annotation task as part of course credit but
annotation decisions were not associated with their
performance in the course.
",no ethical concerns,,no ethical concerns,2023,statement
1421,"The work presented in this paper presents the same
ethical challenges as the datasets over which mod-
els are built. Pre-trained language models are
known to encode social biases and thus may not be
sufficiently capable of capturing some di fferences
in perspective. Because our model can be used to
identify annotators who may not necessarily align
with majority opinions, the outputs can be used to
exclude some annotators from the final judgment,
which may impact the social impact of any model
trained on annotator beliefs that is released. Given
the sensitive nature of the topics that were anno-
tated (e.g., abusive language), care should be taken
to avoid annotation tasks that do not protect the
mental health of annotators; our method could po-
tentially be used to flag annotators for whom this
is a risk.
","bias, annotator welfare",,"bias, annotator welfare",2023,"concerns, advice"
1422,"We are not aware of ethical issues associated with
the texts used in this work. Students participated
in the annotation task as part of course credit but
annotation decisions were not associated with their
performance in the course.
",no ethical concerns,,no ethical concerns,2023,statement
1423,"We acknowledge that the biases discussed in this
paper are not comprehensive and do not include
every sociocultural bias. Also, our experimental
analyses are not rigid conclusions about the stereo-
types presented and propagated within models and
do not imply a superiority of one model over an-
other.
",no ethical concerns,,no ethical concerns,2023,statement
1424,"All corpora and datasets used in this study are pub-
licly available, ensuring compliance with data pri-
vacy regulations. Although we did our best to re-
move any personally identifiable information and
preserve the privacy and anonymity of individuals,
it is possible that some of the selected corpora con-
tain sensible or offensive information. Filtering
such content is challenging given that all the tar-
geted languages are low-resourced and lack proper
NLP tools for this purpose. We believe that it is un-
likely that the normalization models cause benefits
or harm to individuals. Regarding the annotation
of the data (§ 4.1), annotators were fairly compen-
sated for their time and effort. By upholding these
ethical principles, we aimed to conduct the study
in a responsible and conscientious manner.
","privacy concerns, offensive content",,"privacy concerns, offensive content",2023,"statement, concerns, actions"
1425,"When dealing with an endangered language it is
important to make sure that the research also con-
tributes to the language community. This is the
reason why we open-source our FST and neural
model. We also work on data that has been given to
us by speakers of Lushootseed with the intention of
us working on building morphological descriptons
and tools for the language. This means that we are
not conducting our research with no regard to the
language community.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1426,"To collect recipe-grounded conversations we hired
crowd workers using the Prolific platform.6The
study was conducted with the approval of our local
IRB. The compensation was derived based on Pro-
lific’s payment principles. We estimate the hourly
pay for crowd workers was $15.49 (details in Ap-
pendix D). Crowd workers were strictly asked not
to write any offensive content or personal informa-
tion.
6https://www.prolific.co/
",no ethical concerns,,no ethical concerns,2023,actions
1427,"The presented work aims towards improving the
scientific methodology of chat model evaluation.
To this end, we present a battery of analyses com-
paring several aspects of metric validity for four dif-
ferent evaluation methods (Section 7). Our results
allow other researchers in the field to make better-
informed decisions regarding appropriate evalua-
tion methodology in human-computer chat. To
ensure replicability of our methods we publicly
release the annotation software and chatbot imple-
mentations used to collect our conversation and
evaluation data. Additionally, we provide full trans-
parency in our analyses by releasing the code for
all our presented analyses. Finally, to aid future
research efforts in human-computer chat modelling
and evaluation, we release an anonymized version
of our conversation and evaluation data.
One ethical consideration involved in our work
involved managing human workers in our data col-
lection processes. All worker participation in our
study was voluntary and involved zero subjective
screening processes, with a complete description
of worker tasks, workload, and timeframe provided
before work was assigned. Workers could opt
out of our study at any time for any reason. As
compensation for work completed, we targeted a
compensation rate of $10/hour for student14and
Amazon Mechanical Turk workers, and a rate of
$20/hour for Surgers. We compensated on a per-
task-completed basis to ensure timely completion
of work, but verified that target hourly rates were
reasonably approximated throughout the course of
the study by measuring workers’ median task com-
pletion times (see Appendix F for details). These
measures ensured that all human work in our study
13Smith et al. (2022) reports standard deviations of Likert
metrics between 0.8 and 1.3
14Students’ compensation is given as an Amazon Gift Card
for convenience; students are informed of this prior to any
work being completed
was fair, transparent, and mutually-beneficial.
Other ethical considerations arise in our study’s
conversation collection. Unlike the collection of
evaluation or annotation data, collecting interac-
tive conversation data from human-computer in-
teraction poses a small but meaningful risk that
sensitive, damaging, or personally identifying in-
formation could get collected. We mitigated this
risk in three ways. First, students were notified in
multiple email communications and before each
conversation that their conversations with our chat-
bots would be publicly released. Included in these
notices was the instruction to refrain from releasing
any personally identifiable or damaging informa-
tion. Our instructions suggest that students fabri-
cate personal information at any time during the
conversations if it would make them feel more com-
fortable. Second, we hand-checked all 400 conver-
sations to ensure the non-presence of any sensitive
information. Third, we anonymize all data before
public release. Our study’s collection and analysis
of conversation data did not investigate interactors
as human subjects, and we did not seek institutional
review board approval.
Finally, there is a concern in our study about the
potential of the chatbots to respond to student inter-
actors with toxic, insensitive, or vulgar language.
The data-driven nature of some of our evaluated
chat models means the chatbots are prone to reflect-
ing any biases, toxicity, and vulgarity present in the
training data (see Dinan et al. (2022) for a quan-
titative analysis). A high rate of antisocial behav-
iors among our evaluated models could potentially
make human interactors’ experience talking with
the bots quite uncomfortable, and would poorly
reflect on the research field’s potential for social
good. To mitigate this risk, the authors extensively
hand-tested all evaluated chat models, as well as
conducting a pilot evaluation among the authors’
lab group. As confirmed further in our results (Sec-
tion 8), our chatbots exhibited negligible rates of
antisocial behavior.
12 ","privacy concerns, toxic content",,"privacy concerns, toxic content",2023,"concerns, actions"
1428,"The data collection received ethical approval from
the University of Malta Research Ethics Commit-
tee. This data is intended to be used for training,
fine-tuning, and performing experimental evalu-
ations of machine learning models. The dataset
from which the images were originally sourced is a
widely-studied, publicly available resource. As far
as we are aware, the data does not contain harmful
or offensive content. However, we acknowledge
that any biases in the collection of images and/or
captions in the original dataset will also be present
in the HL Dataset.
Supplementary Materials Availability
statement:
The HL Dataset is publicly released on GitHub4
and Huggingface5. The syntetic HL Narratives
Dataset described in Section 6, is publicly released
on Huggingface6. All the baselines described in
Section 5 and 6 are available on Huggingface7.
",bias,,bias,2023,"statement, concerns"
1429,"Our work has limited ethical implications since we
mainly introduced an approach to study discourse
coherence in NLMs. The datasets we built were
used in compliance with the Terms of Use and the
resources and materials produced during this study
will be open source.
",no ethical concerns,,no ethical concerns,2023,statement
1430,"Our work is meant to inspire reflection on the treat-
ment of annotators in the field of legal NLP. Specif-
ically: a) we make it a point of involving legal
professionals, not law students; b) the annotators
involved in the project won a public selection com-
petition to participate in a project aimed at the digi-
talization of the Italian judicial system; c) the an-
notators are all hired to work on the project and
receive adequate pay; d) we make sure that their
specific expertise is valued by involving them in the
creation and negotiation of the annotation guide-
lines; e) we take measures to track whether they
are happy with the work they are doing.
",no ethical concerns,,no ethical concerns,2023,actions
1431,"This work included data collection, specifically the
selection of test sentences from public-facing Cana-
dian government websites as well as the annotation
of machine translation errors. This was performed
by one of the authors, who reads both languages
and received confirmation on French-related ques-
tions from fluent colleagues.
While this work does focus on two identifiable
individuals, these two individuals are public fig-
ures and the data sources that we select are official
sources of public information about them (in fact,
produced by governments of which they were/are
the Heads of State). There is discussion in the NLP
and MT literature of the harms of misgendering
and of treating gender as a binary or immutable
feature (Cao and Daumé III, 2020; Saunders et al.,
2020). In this work, we focus on some aspects
of grammatical gender that can be unrelated to an
individual referent (e.g., Sa Majesté), as well as
some aspects of linguistic gender that do have a
tie to the referent (e.g., pronouns, inflection of ad-
jectives). By choosing this particular case study
of the accession of King Charles III after the pass-
ing of Queen Elizabeth II, this paper does focus
on only two linguistic genders in French and En-
glish, because the current and past official formal
forms of address of these two particular individu-
als are well-documented in this language pair by
sources from their governments. We use the most
recent available information for this, as linked in
the footnote in the previous section. For a broader
discussion of gender-inclusive language related to
translation and this particular language pair, there
are various sources on the topic,9and some of these
conventions are changing.
From a computational cost perspective, this
paper reused existing neural MT systems (pub-
licly available systems and internal systems) rather
thank training systems from scratch, and translated
a very small amount of text.
",no ethical concerns,,no ethical concerns,2023,statement
1432,"The factual error types and document fact high-
lights predicted by our model can help users correct
factually inconsistent summaries. Since factually
inconsistent summaries often convey misinforma-
tion, our model can potentially help users combat
misinformation. However, the factual error types
predicted by our model may be incorrect. For ex-
ample, it is possible that an input summary contains
extrinsic noun phrase errors, but our model predicts
the error type of intrinsic predicate error. Hence,
users still need to be cautious when using our model
to detect and correct inconsistent summaries. The
Aggrefact-Unified dataset contains public news ar-
ticles from CNN, DailyMail, and BBC. Hence, the
data that we used does not have privacy issues.
",model inaccuracies,,model inaccuracies,2023,"concerns, advice"
1433,"As with any work involving PLMs (or foundation
models ), due to the data and training methods, there
is inherent risk of generating biased, toxic, harmful,
or otherwise unwanted output. Regarding our work
in particular, as we show in Figure 3, the model’s
performance on some of the classes can degrade.
More analysis needs to be done before deploying
our approach, since it is unclear whether it will
introduce a bias towards certain types of classes.
Figure 3: This figure shows the difference in Intent
Detection F1 score for each intent, if we have a PVI
threshold per-class VS having a fixed PVI threshold.
See larger figure in Appendix.
",bias,,bias,2023,"concerns, advice"
1434,"All authors of this paper acknowledge and agree
with the ACM Code of Ethics. In our study, we
ensure that our work is compatible with the pro-
vided code, specifically in the terms of presenting
a non-offensive dataset construction.
In order to accomplish a comprehensive analysis
of AE/RS metrics on the response ranking task, we
collect a dataset containing human rankings for gen-
erated responses conditioned on existing human-
human conversations with polished contents. The
main concern is that generated responses based on
well-known state-of-the-art dialogue models could
have offensive content which is out of our work’s
scope.
In the feedback generation component leveraged
in our proposed metric which is based on prompt-
ing a LLM, the outputs show whether a response
is relevant or not and explain why that is the case,
hence the chance of generating inappropriate con-
tents is near zero.
",toxic content,,toxic content,2023,"statement, concerns"
1435,"To build our dataset, we collected the dialogue data
by augmenting MultiWOZ 2.1, which is a publicly
available English dialogue dataset under MIT li-
cense. Additionally, we collected the review data
using crowd-sourcing, where we provided crowd
workers with the reviewer’s persona, as well as the
aspects and sentiments of reviews. This controlled
review collection process helps to exclude offen-
sive or harmful content from the reviews. It also
helps to avoid privacy or copyright issues when
making the dataset publicly available. Our dataset
is available under the CDLA-Sharing 1.0 license.
",no ethical concerns,,no ethical concerns,2023,statement
1436,"Human Evaluation and Crowdsourcing. We
make use of crowdsourcing through Amazon Me-
chanical Turk for several experiments. All crowd-
workers were paid at a rate higher than the mini-
mum wage in California. In accordance with Cal-
ifornia State Law, all crowdworkers were also in-
formed they were speaking with chatbots during
the data collection for our interactive evaluation.
All participants consented to the logging of their
responses.
Language biases. Large pre-trained lan-
guage models are typically pre-trained on massive
corpora crawled from the internet such as The
Pile (Gao et al., 2020) or Common Crawl. This
allows language models to have exposure to a large
amount of linguistic diversity, but this also results
in exposure to a lot of hateful, biased, or otherwise
undesirable content from the internet (Luccioni and
Viviano, 2021). Future work should examine com-
bining conversation synthesis with dialogue safety
approaches.
Scientific Artifacts. All scientific artifacts are
used according to their intended purpose. The FITS
dataset is publicly available at https://parl.ai/
projects/fits/ . OPT is an open-source language
model. GPT-J is available for use under the MIT
license. We use the HuggingFace Transformers
and PyTorch packages for all modeling (Wolf et al.,
2020; Paszke et al., 2019). All artifacts used are in
English.",bias,,bias,2023,"statement, concerns, actions"
1437,"The datasets used in the experiments are all well-
known machine translation datasets and publicity
available. Data preprocessing does not involve any
external textual resources. Intermediate sequences
generated in our data augmentation method are new
symbolic combinations of the tokens in the target
language. However, the final output of the model
is the tgtsequence which is the same as the target
sequence in the original training set. Therefore,
we would not expect the model trained with our
data augmentation method would produce more
harmful biases. Finally, we declare that any biases
or offensive contexts generated from the model do
not reflect the views or values of the authors.
","bias, toxic content",,"bias, toxic content",2023,statement
1438,"Legal models similar to the ones we study above
have been deployed in the real world. Known
applications include risk assessment of detainees
in the United States (Kehl and Kessler, 2017)
and sentencing of criminals in China (Xiao et al.,
2018). We believe these are not ethical use cases
for legal AI. One must not be tempted to think of
outcome prediction as equivalent to some medicaltask, such as cancer detection, with a breach of
law seen as a ‘tumor’ that is either there or not.
This is a naive viewpoint that ignores the fact that
the legal system is a human construct in which the
decision makers, the judges, play a role that can
shift the truth, something that is impossible when
it comes to natural laws.
Herein lies the ethical dubiousness of any at-
tempt at modeling judges using AI. Unlike in the
domain of medicine, where identifying the under-lying truth is essential for treatment, and thus a
successful machine diagnostician is in theory a
competition for the human one, in the domain of
law the validity of the decision is poised solely on
the best intentions of the judge. For some judges
this pursuit of the ‘right’ outcome can go as far
as defiance of legal precedent. We therefore ar-gue a judge should not be replaced by a machine
and caution against the use of our, or any other
legal AI model currently available, towards auto-
mating the judicial system.
A Note on the Baselines
Comparing the MTL baseline and the joint model,
one might come to the conclusion that there is no
substantial difference between the models when
it comes to predicting positive outcomes. While
the joint model outperforms the MTL baseline on
eleven out of the twelve experiments we test our
models on, the improvement in performance on
the positive outcome prediction over the outcome
corpus is very narrow. However, there is an impor-
tant difference between them. The MTL baseline,
much like the simple baseline, can predict positive
and negative outcome simultaneously for the same
Article. This means that in our evaluation, the
baseline models can cheat by predicting an Articleto be simultaneously violated and not-violated.
This is another reason that the outcome prediction
task needs to consider the legal relationship be-
tween positive and negative outcomes. Ignoring
the relationship of claims and outcomes makes
both of our baselines fundamentally ill-suited for
the task of outcome prediction. Hence, they are
only useful for a comparison in our study.

B Glossary and Dataset Examples
Legal Terms
Claim The allegation of a breach of law usually put forth by their legal counsel
on behalf of the claimant.
Positive Outcome Claims are assessed by judges in courts of law. If they think a claim is
valid, they rule it as successful. The outcome of the case is a victory for
the claimant; which we call the positive outcome in this paper.
Negative Outcome On the other hand, the claimant can be unsuccessful in the court. The
judge has decided against them in the court, in favor of the defendant,
and we call this the negative outcome in this paper.
Facts The description of what happened to the claimant. This includes more
general descriptions of who they are, circumstances of the perceived
violation of their rights, and the proceedings in domestic courts before
their appeal to ECtHR.
Precedent Cases that have been cited by the judges as part of their arguments.
Binding Judges are expected to adhere to the binding rules of law and decide
future access accordingly.
Stare Decisis New cases with the same facts to the already decided case should lead to
the same outcome. This is the doctrine of precedent by which judges can
create law.
Caselaw Transcripts of the court proceedings.
ECHR European Convention of Human Rights, comprises the C onvention and
the Protocols to the convention. The Protocols are the additions and
amendments to the Convention introduced after the signing of the
original Convention.
ECtHR European Court of Human Rights, adjudicates ECHR cases.
Apply A judge applies the precedent when she decides on the outcome of a case
via an analogy to an already existing case.
Distinguish Conversely, a judge distinguishes the case from the already existing
cases when she believes they are not analogous.
ECtHR Example
Facts Claims Positive
OutcomesNegative
Outcomes
‘‘Ms Ivana Dvo ˇr´aˇckov ´a was born in 1981 with Down
Syndrome (trisomy 21) and a damaged heart and lungs.
She was in the care of a specialised health institution
in Bratislava. In 1986 she was examined in the Cen-
tre of Paediatric Cardiology in Prague-Motole where it
was established that, due to post-natal pathological de-
velopments, her heart chamber defect could no longer
be remedied...’’ for more see Case of Dvoracek and
Dvorackova v. SlovakiaArticles:
2, 6, 8,
14Articles: 2,
6Articles: 8,
14
45",no ethical concerns,,no ethical concerns,2023,statement
1439,"This work complies with the rules expressed in the
ACM Code of Ethics. The NLP application in this
work refers to information extraction in the biomed-
ical domain. All the NLP tools and datasets used
are mentioned and cited. We do not present a new
dataset. The datasets utilized are used as intended.
We do not use demographic or identity characteris-
tics information. Our experiments involve around
500 hours of compute time.
",no ethical concerns,,no ethical concerns,2023,statement
1440,"This study uses de-identified clinical data with the
approval of the partner French hospital scientific
and ethical committee (IRB equivalent). This work
might be recommended to clinicians as a useful
tool for assisting in the systematic analysis of large
patient records. Unfortunately, the annotated cor-
pora developed in this work cannot be shared with
the community due to confidentiality restrictions.",no ethical concerns,,no ethical concerns,2023,statement
1441,"Large Language Models (LLMs), including BART
and LED, can implicitly learn biases from their
training dataset. In the biomedical fields, these bi-
ases potentially include exclusion of certain groups
of people who are underrepresented or misrepre-
sented in the training data. It is important to be
aware of this potential bias. Moreover, LLMs are
not always accurate and reliable. Inaccuracies in
the generated summaries from LLMs could have
serious consequences and impact on health and
well-being of persons who trust the automated sum-
maries of biomedical research articles generated by
LLMs.","bias, inaccuracies, reliability",,"bias, inaccuracies, reliability",2023,concerns
1442,"While our research does not directly introduce any
social or ethical bias nor amplify the bias in the
data, we inherit a considerable amount of the un-
derlying limitations of LMs. LM pre-training is
computationally expensive and causes environmen-
tal damage. Our research focuses on overall com-
puting resource efficiency and thus has a marginal
environmental footprint.
","computational cost, environmental_impact",,"computational cost, environmental_impact",2023,"concerns, actions"
1443,"We believe this work to have been conducted and
to contribute in an honest, non-discriminate, and
professional manor, and to our best knowledge and
reflection, we believe we are in full compliance
with the ACL Ethics Policy. We note that in future
work, if using non-simulated/non-public medical
data, care must be taken to protect the privacy of
the people involved in full compliance with HIPPA
and any IRBs that review these studies.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1444,"We would like to discuss the following limitations
and ethical considerations:
In this paper, we investigated the cross-domain
extraction performance based on a multi-source
corpus. Our working assumption is that this cor-
pus represents enough variety to support such a
claim. However, we point out that the corpus is bi-
ased towards English scientific and patent language,
as well as the chemical / material science subject
domain. Further, we remark that the subjects dis-
tribution itself is biased towards the BM and MSP
datasets as the the more varied MeasEval dataset
only contains few examples for each of its 10 sub-
jects. Consequently, a balanced corpus should have
a more even distribution of both subject domains
and language domains by increasing the size of
the currently underrepresented domains and ideally
including data from more than only the English
language.
Further, despite having substantial IAA scores
for the re-annotation of the MSP corpus, we often
perceived the task as difficult and ambiguous and
felt the limitations of only having two contextual
entities, instead of the three as proposed by Harper
et al. (2021). Yet, the low IAA score (0.334) for the
excluded Qualifier entity suggests that including
it may not have eased the task. Hence, it may be
valuable to further the study of how the measure-
ment extraction problem can be modelled to resolve
some of the ambiguities for context extraction.
Finally, while we tried to stay as closely to
the original annotation guidelines as proposed by
Harper et al. (2021) as possible (with the exceptionof the two cases explicated in Appendix C, there
is a high likelihood of annotation drift. The re-
annotators of the MSP corpus were not involved in
the original MeasEval annotation procedure and it
is possible that the interpretation of the annotation
guidelines was slightly different at places than the
authors have originally intended. Our adaption of
the annotation guidelines can be found at the end
of this paper (Appendix J)
","bias, quality, annotation drift",,"bias, quality, annotation drift",2023,"statement, concerns, actions"
1445,"We would like to discuss the following limitations
and ethical considerations:
In this paper, we investigated the cross-domain
extraction performance based on a multi-source
corpus. Our working assumption is that this cor-
pus represents enough variety to support such a
claim. However, we point out that the corpus is bi-
ased towards English scientific and patent language,
as well as the chemical / material science subject
domain. Further, we remark that the subjects dis-
tribution itself is biased towards the BM and MSP
datasets as the the more varied MeasEval dataset
only contains few examples for each of its 10 sub-
jects. Consequently, a balanced corpus should have
a more even distribution of both subject domains
and language domains by increasing the size of
the currently underrepresented domains and ideally
including data from more than only the English
language.
Further, despite having substantial IAA scores
for the re-annotation of the MSP corpus, we often
perceived the task as difficult and ambiguous and
felt the limitations of only having two contextual
entities, instead of the three as proposed by Harper
et al. (2021). Yet, the low IAA score (0.334) for the
excluded Qualifier entity suggests that including
it may not have eased the task. Hence, it may be
valuable to further the study of how the measure-
ment extraction problem can be modelled to resolve
some of the ambiguities for context extraction.
Finally, while we tried to stay as closely to
the original annotation guidelines as proposed by
Harper et al. (2021) as possible (with the exceptionof the two cases explicated in Appendix C, there
is a high likelihood of annotation drift. The re-
annotators of the MSP corpus were not involved in
the original MeasEval annotation procedure and it
is possible that the interpretation of the annotation
guidelines was slightly different at places than the
authors have originally intended. Our adaption of
the annotation guidelines can be found at the end
of this paper (Appendix J)
","bias, quality, annotation drift",,"bias, quality, annotation drift",2023,"statement, concerns, actions"
1446,"Although we tackle the problem of factual inconsis-
tency for abstractive summarization, and improve
the entity-level factual consistency of the generated
summaries by applying the entity-level span copy
mechanism, the generated summaries still contain
unfactual information. Therefore, caution must be
exercised when the model is deployed in practical
settings.
",unfactual information,,unfactual information,2023,"concerns, actions, advice"
1447,"Reproducibility. We discussed all relevant param-
eters, training details, and hardware information in
§ 3.3.
Performance Validity. We proposed an innovative
application, SELECT ,SIMPLIFY and REWRITE ,
for the Cross-lingual Science Journalism task and
verified its performance for WIKIPEDIA and SPEK -
TRUM data for the English-German language pair.
We believe this application is adaptable for other
domains and languages; however, we have not ver-
ified this experimentally and limit our results to
the English-German language pair for the scientific
domain.
Legal Consent. We explored the SPEKTRUM
dataset with their legal consent for our experi-
ments. We adopted the public implementations
with mostly recommended settings, wherever ap-
plicable.
Human Evaluation. We published a job on the
Heidelberg University Job Portal with the task
description, requirements, implications, working
hours, wage per hour and location. We hired five an-
notators from Heidelberg University who are native
Germans, fluent in English and master’s or bach-
elor’s science students. The selected students for
the evaluation task submitted their consent whileagreeing to the job. We compensated them at C15
per hour, while the minimum student wage ranges
between C9.5−12in 2022 according to German
law9.
",no ethical concerns,,no ethical concerns,2023,actions
1448,"As we work with data that has been published be-
fore the ACL Ethics Charter was implemented, we
cannot guarantee that the way the data was col-
lected and handled meets current Ethics Standards.
As far as we can tell, it is still a suitable data set
for this type of research. There are however the
limitations mentioned above. Also, there is no in-
formation given about the age or ethnicity of the
speakers.
",no ethical concerns,,no ethical concerns,2023,statement
1449,"RELAY READER , the reading app discussed in this
paper, specifies Terms of Use and provides a link to
Privacy Policy. In particular, the Terms of Use spec-
ify the legitimate uses of the data and commits to
keeping the data of users-in-the-wild anonymous.9
For the book data, we used a public domain text
ofThe Adventures of Pinocchio from Project Guten-
berg and the text of Harry Potter and the Sorcerer’s
Stone provided to us by the copyright holder10as a
part of a license to use the book and the audiobook
narration by Jim Dale in the app for a specified lim-
ited number of students; the students whose data is
analyzed in Experiment 2 are within that cap.
9https://relayreader.org/terms
10We did not alter anything in the HP book. For Pinocchio,
we re-chaptered the original 36 short chapters of the story
that we downloaded from Project Gutenberg into 19 longer
chapters in order to better adjust to the turn-taking setup of
RELAY READER .The corpora used in the study are either broadly
available for research purposes (BNC, SUBT) or
have a more limited research and/or operational
availability through contracts (TASA3, SFI).
The study during which oral reading data was
collected from grade 4 and 5 students in a school
in New Jersey was approved by the Institutional
Review Board at our organization. Parental con-
sent was obtained for students’ participation in the
activity and for use of students’ data (including
recordings, log data of the reading activity, and
demographic information provided by the parents
such as the grade data used in this study) for re-
search.
The goal of this research is to improve the quality
of assessment of oral reading by identifying factors
that could impact fluency measurements that are
not entirely due to the students’ developing skill
and build models that would allow compensating
for the impact of such factors. Accurate assess-
ment of oral reading fluency controlling for text
effects will benefit teachers and students in that
the assessment can be done on a variety of texts,
including different passages for different students,
instead of using a single pre-set normed passage as
in the current practice. This would give both teach-
ers and students more agency in selecting reading
materials based on interest and preference and will
thus help assessment to be more socio-culturally
responsive while still providing the measurement
signal necessary to monitor skill progression.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1450,"We acknowledge that our Earnings Conference
Call dataset contains English transcripts from the
largest US-based companies so it is possible that
some populations may be underrepresented in this
sample. We plan to extend this work to inter-
national companies and conference calls held in
other languages in the future.
",underrepresentation,,underrepresentation,2023,"concerns, suggestions"
1451,"We do not foresee any significant harm directly as
a result of this work. On the contrary, our work
promotes the protection of user privacy, which is
significant, especially in this era that large amounts
of personal data are used by neural models.
",no ethical concerns,,no ethical concerns,2023,statement
1452,"It is important to acknowledge that although the re-
sults are promising, language models tend to have
hallucinations for generating coherent answers thus
these systems should always be used with human
supervision. Moreover, this particular system is
meant as an experiment to inspire further research
into investigating ensembling approaches for sum-
marization and further finetuning as well as model
explainability studies are required before they can
be used in a clinical setting.
",hallucinations,,hallucinations,2023,"concerns, suggestions"
1453,"Radiology reports contain sensitive patient infor-
mation and it is crucial to handle this data respon-
sibly, adhering to strict privacy and confidentiality
guidelines. The dataset used in this paper was fully-
de-identified. We received approval from our insti-
tution’s IRB prior to conduct the presented research
and used HIPAA compliant servers. Additionally,
a careful examination is needed to assess potential
bias in models used for extracting information from
radiology reports prior to implementing real life
secondary use applications.
",bias,,bias,2023,"statement, concerns, advice"
1454,"Developing an automated system for clinical note
generation from doctor-patient conversations raises
several ethical considerations. First, informed con-
sent is crucial: patients must be made aware of their
recording, and data ownership must be prioritized.
Equitable access is also important; the system must
be usable for patients from diverse backgrounds,
including those with disabilities, limited technical
literacy, or language barriers. Addressing issues
of bias and fairness are necessary to avoid un-
fair treatment or misdiagnosis for certain patient
groups. The system must implement robust secu-
rity measures to protect patient data from unautho-
rized access or breaches. Establishing clear lines
of accountability for errors or harms arising from
using an automated system for note generation is
paramount. Disclosure of known limitations or po-
tential risks associated with using the system is
essential to maintain trust in the patient-physician
relationship. Finally, ongoing evaluations are nec-
essary to ensure that system performance does not
degrade and negatively impact the quality of care.
"," accessibility, bias, fairness, privacy",," accessibility, bias, fairness, privacy",2023,"concerns, advice"
1455,"Privacy sensitive datasets. Both the MIMIC-
CXR dataset (Johnson et al., 2019a), and the OpenI
dataset (Demner-Fushman et al., 2015) were fully
de-identified by the dataset authors in compliance
with applicable privacy laws (HIPAA). This in-
cludes the removal of any protected health infor-
mation that may directly or indirectly identify a
patient. Nevertheless, the data is still privacy sensi-
tive, and special care was taken to only process it
within secured computing infrastructure.
Intended use. We believe that the proposed meth-
ods can improve the workflow of clinicians both by
reducing the documentation effort and encouraging
higher-quality reporting, and thereby improving pa-
tient care. However, as our results and discussion
show, state-of-the-art summarization methods may
not have the desired level of quality that is needed
in high-stakes domains such as the clinical context.
Therefore, our work is not to be understood in the
context of a system that can be deployed, but rather
as a step toward a better understanding of the short-
comings of current text summarization methods
and providing insight into how these can solved.

","privacy concerns, faithfulness",,"privacy concerns, faithfulness",2023,"statement, concerns, actions, advice"
1456,"Our experimentation utilized OpenAI API to ex-
tract SDOH information from SHAC with GPT-4.
SHAC is a fully de-identified corpus of social his-
tory sections. The use of such external API/models
could introduce ethical problems related to privacy,
identifiability, and other unintended consequences
if the data sets are not fully de-identified. Addition-
ally, a careful examination is needed to assess po-
tential bias in LLMs for extracting SDOH prior to
implementing real-life secondary use applications.
We received approval from the Institutional Review
Board (IRB) prior to conducting the presented re-
search. As our GPT-4 one-shot experiments are
conducted on the SHAC-UW test set, broader use
of the model may need necessary precautions.
8
",no ethical concerns,,no ethical concerns,2023,"concerns, advice"
1457,"The dataset was collected using Twitter’s official
API12and in compliance with Twitter’s terms of
use. Only the tweet IDs of the tweets used in the
experiments will be made public, and we ensure
that their redistribution is in compliance with Twit-
ter’s developer policy.13Researchers cannot collect
deleted tweets or tweets of private users, thus pro-
tecting user privacy.
We paid the annotators $0.03 per post they
checked (approximately 3,000 to 5,000 posts).
Although the first author is affiliated with Yahoo
Japan Corporation, all the data used in the exper-
iments belong to the affiliation of the second and
third authors.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1458,"TAGPRIME fine-tunes the pre-trained language
models (Devlin et al., 2019; Lan et al., 2020). There
have been works showing the potential bias in pre-
trained language models. Although with a low pos-
sibility, especially after our finetuning, it is possible
for our model to make counterfactual, and biased
predictions, which may cause ethical concerns. We
suggest carefully examining those potential issues
before deploying the model in any real-world ap-
plications.
","bias, nonfactual content",,"bias, non-factual content",2023,"concerns, advice"
1459,"Weuseonlypubliclyavailableresourcesforallcon-
ducted experiments. All annotators were volunteer
students who performed the tasks as a part of their
studies and received credits for it.
8 ",no ethical concerns,,no ethical concerns,2023,statement
1460,"Dataset risks
We do not anticipate any risks in releasing the
evaluation dataset. This dataset was constructed
through the modification of a publicly available
dataset commonly used in the evaluation of GEC
systems, the dev set of the BEA-2019 shared task
(Bryant et al., 2019). These modifications involve
the change of gendered words and agreeing verbs to
create parallel data across masculine, feminine, and
singular they pronouns with the goal of evaluating
bias in GEC systems. By enabling researchers to
measure bias in this way, we believe that the release
of this dataset will aid further study in reducing bias
in these systems by providing a benchmark.
Risks of describing data augmentation
techniques
We caution that the singular theydata augmentation
technique used in this paper was not designed to
generate text that surfaces directly to users. There
may be risks to deploying data augmentation tech-
niques at runtime as these techniques are designed
to modify gender identity terms; depending on the
context of deployment, users may be harmed by
such modifications if they result in misgendering
or erasure. On the other hand, as we show in this
work, use of these techniques to generate training
data can reduce bias, and we believe that in this
way, the description of this technique will aid in
reducing bias in NLP systems.
",misuse,,misuse,2023,"statement, concerns, actions"
1461,"We adhere to the ACL Code of Ethics. This work
did not use any private datasets and did not contain
any personal confidential information.
",no ethical concerns,,no ethical concerns,2023,statement
1462,"Worker Qualification and Compensation for An-
notation. To collect annotations on our dataset,
we used Amazon Mechanical Turk (AMT). All
workers had the following qualifications: (1) over
5,000 completed HITs; (2) 99% approval rate or
higher; (3) Native English speakers from England,
New Zealand, Canada, Australia, or United States.
Workers were paid $0.75 per HIT, and on average
completed a batch within four hours of work. In
addition, $10 was given upon completing a batch
(73 HITs), raising the hourly pay to $16.2.
Data Collection and Usage Policy for Annota-
tion. Workers were informed that their annota-
tions would be collected for research purposes
and would be used to train and evaluate language-
related models, and that the annotations would
eventually be made publicly available. Addition-
ally, our task and the annotations collected were of
objective nature and did not contain any personal
information. Furthermore, all data sources used in
the study were publicly available.
9 ",no ethical concerns,,no ethical concerns,2023,actions
1463,"Our tense test set is based on the widely used public
corpus Europarl in the field of machine translation.
In creating this test set, we only corrected tense and
description errors of some English references and
did not change the original semantics, so there are
no ethical issues arising.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1464,"Our goal is to investigate whether and to what ex-
tent existing argumentation corpora are topic bi-
ased. This serves to critically examine the state of
the art. However, we by no means want to give the
impression that previous corpus authors lack ambi-
tion or diligence. Rather, the opposite is the case.
The number of corpora that have been created in
the last decade shows that the community is aware
of the fact that not all areas of the argumentation
landscape have been covered yet, and is therefore
doing its utmost to explore it further. In a dynamic
and rapidly growing research field, standards are
usually developed in parallel with contributions,
not in advance. Our research may therefore con-
tribute to the further standardization of the corpus
linguistics of argumentation.
The manual annotation of arguments and top-
ics was done by expert annotators of our research
groups. They were compensated fairly under Ger-
man law. No personal data was collected.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1465,"We made every effort to make sure the work de-
scribed in this paper adheres to the ACL Code of
Ethics.
",no ethical concerns,,no ethical concerns,2023,statement
1466,"This work is conducted in accordance with the
ACM Code of Ethics.14In this section we comment
on the topics of privacy, safety and accessibility,
as we believe they are particularly relevant to the
development and use of our system.
Privacy
Since machine learning systems can reveal sen-
sitive information about their training data, it is
important to consider privacy concerns relating to
the development and use of such systems. The
training data for our system originates from two
primary sources: publicly available text and essays
collected from examinations and online error cor-
rection services. The PIE Corpus is derived from
publicly available texts (Awasthi et al., 2019). The
Lang-8 and Write & Improve essays are collected
in accordance with the services’ respective privacy
policies. The FCE dataset is anonymised before
use (Yannakoudakis et al., 2011). Privacy-related
information is not documented for the NUCLE and
LOCNESS datasets.
Safety
Automated GEC systems have the potential to
change the meaning of the input text. Therefore,
the systems described in this work should be ap-
plied with caution. In scenarios where miscommu-
nication is dangerous, the system should only be
used as an aid for the manual correction of text,
rather than a fully automated system.
Accessibility
The development of our system required compute-
intensive model training and data preprocessing.15
This cost may be prohibitive for some research
groups or potential users. We make our trained
14https://www.acm.org/code-of-ethics
15See Section A.5 for details.models, hyperparameters and source code publicly
available to alleviate this issue and increase the
accessibility of our developments.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1467,"Our study complies with the ACL Ethics Policy.
We used S2ORC (Lo et al., 2020, CC BY-NC 4.0),
PyTorch (Paszke et al., 2019, BSD-style license)
and HuggingFace Transformers (Wolf et al., 2020,
Apache-2.0) as scientific artifacts. Our study was
conducted under the licenses and terms of the sci-
entific artifacts. Our model is trained on a set of
publicly available datasets (Wang et al., 2022), in
which undesirable data distribution, such as disin-
formation, bias, or offensive content, might present.
Such potential risks need to be recognized.
","disinformation, bias, offensive content",,"disinformation, bias, offensive content",2023,statement
1468,"Potential Risks As discussed in Section 5.2, our
model risks generating incorrect information. Cur-
rently, it can be effectively utilized with human
revision, such as writing assistance tools, by pro-
viding drafts of literature reviews. However, a lit-
erature review with wrong information could be
published if abused.
Licenses We used S2ORC (Lo et al., 2020, CC
BY-NC 4.0), PyTorch (Paszke et al., 2019, BSD-
style license), and HuggingFace Transformers
(Wolf et al., 2020, MIT for facebook/bart-large-cnn,
Apache-2.0 for all materials) as scientific artifacts.
All artifacts can be used for research purposes. We
release the SciReviewGen dataset based on S2ORC,
as CC BY-NC 4.0 allows users to adapt and share
licensed material for noncommercial purposes.
Annotation Procedures The annotation proce-
dures complied with the ACL Ethics Policy. Prior
to the annotation, we informed the ethics review
board in our university of the annotation procedures
and were notified that it was exempt from ethics
review. More details are presented in Section C.
","incorrect information, misuse",,"incorrect information, misuse",2023,"statement, concerns"
1469,"Our goal in this paper was to compare the pre-
viously proposed and widely-used intrinsic bias
evaluation measures of gender bias in pre-trained
PLMs. Although we used a broad range of existing
datasets that are annotated for social biases, we did
not annotate nor release new datasets as part of this
research. Moreover, we fine-tune a large number
of bias-controlled PLMs for evaluation purposes
that demonstrates varying levels of gender biases.
However, these PLMs are not supposed to be used
in downstream tasks other than for evaluation pur-
poses.
Even with the highly correlated bias evaluation
measure in our proposed method, the bias of the
PLM may not be sufficiently evaluated. There-
fore, we consider that it important to select intrinsic
gender bias evaluation measures carefully and not
purely based on correlation coefficients computed
by the proposed method alone.There are various discussions on how to define
social bias in PLMs (Blodgett et al., 2021). Since
the proposed method can use any method as the
bias-controlled fine-tuning of the PLMs, the bias-
controlled fine-tuning can be selected according to
the definition of social bias.
",bias,,bias,2023,"concerns, actions"
1470,"Our adversarial dataset came from existing datasets
of CB and MNLI. We visually checked the in-
stances in the data development and found no in-
stances with ethical concerns.
One should also be aware of social biases (e.g.
gender stereotypes) in PLM. RoBERTa, the PLMwe used in our experiments, is known to have gen-
der biases (Sharma et al., 2021). Since we used
it as-is in order to follow the experimental con-
ditions of previous studies using RoBERTa, our
current results are possibly influenced by such bi-
ases. However, the consideration of the prompt
robustness of this work would not pose or magnify
such ethical concerns.
7
",bias,,bias,2023,concerns
1471,"In this paper, we do not annotate novel datasets
nor release any fine-tuned MLMs. Therefore, we
do not see any direct ethical issues arising from
our work. However, we are proposing a method
to address the underestimation of cosine similarity
scores computed using contextualised word em-
beddings obtained from (possibly socially biased)
pretrained MLMs. We would therefore discuss the
ethical implication of this aspect of our work in this
section.
Cosine similarity has been used in various
social bias evaluation measures such as the
WEAT (Caliskan et al., 2017), SemBias (Zhao
et al., 2018), WAT (Du et al., 2019), etc. These
methods measure the cosine similarity between a
gender and a set of pleasant or unpleasant set of
attributes to compute a social bias evaluation score.
Although originally these methods were developed
for evaluating the social biases in static word em-
beddings, they have been later extended to contex-
tualised word embeddings (Kaneko and Bollegala,
2022; Kaneko et al., 2022) and sentence embed-
dings (May et al., 2019), where cosine similarity
still remains the main underlying metric. How-
ever, Ethayarajh et al. (2019c) showed that inner-
products to be superior over cosine similarity for
social bias evaluation purposes. It remains unclear
as to how the underestimation in cosine similarities
discussed in our work would influence the social
bias evaluations. In particular, the effect of the pro-
posed ℓ2norm discounting scheme on social bias
evaluation must be carefully studied in the future
work.",faithfulness,,faithfulness,2023,"statement, concerns, suggestions"
1472,"In this paper, we proposed a distribution based
method using publicly available MLMs, and evalu-
ated with the SemEval-2020 Task 1 English data.
Although we have not published any datasets or
models, Basta et al. (2019) shows that pretrained
MLMs encode and even amplify unfair social bi-
ases such as gender or racial biases. Given that weobtain sibling distributions from such potentially
socially biased MLMs, we must further evaluate
the sensitivity of our method for such undesirable
social biases.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1473,"We compared our proposed method, NPMS, with
several baselines on WSD and WiC tasks. In this
work, we did not annotate any datasets by ourselves
and used corpora and benchmark datasets that have
been collected, annotated and repeatedly used for
evaluations in prior works. To the best of our
knowledge, no ethical issues have been reported
concerning these datasets. Nevertheless, prior work
from Zhou et al. (2022) shows that pretrained sense
embeddings encode various types of social biases
such as gender and racial biases. Moreover, it has
also been reported recently that word-level meta-
embedding methods can amplify the social biases
encoded in the source embeddings (Kaneko et al.,
2022). Therefore, we emphasise that it is important
to evaluate the meta-sense embeddings learnt in
this work for unfair social biases before they are
deployed to downstream applications.",bias,,"bias, bias amplification",2023,"statement, concerns, suggestions"
1474,"In this paper, we considered the problem of cap-
turing temporal semantic variation of words by
learning dynamic contextualised word embeddings.
For this purpose, we proposed a method to adapt
a masked language model from to a given time
stamp. We did not collect, annotate or release new
datasets during this process. However, we used pre-
trained MLMs and four datasets from the internet
(Yelp, Reddit, Arxiv, and Ciao). It is known that
pretrained MLMs contain unfair social biases (May
et al., 2019; Nadeem et al., 2021; Kaneko and Bol-
legala, 2021; Kaneko et al., 2022). Such biases can
be amplified by fine-tuning methods, especially
when the fine-tuning prompts are extracted from
social data such as customer reviews in Yelp or
discussions in Reddit. Therefore, we consider it is
important to further evaluate (Nangia et al., 2020;
Nadeem et al., 2021; Kaneko and Bollegala, 2022)
the adapted MLMs for social biases, and if they
do exist, then apply appropriate debiasing meth-
ods (Kaneko and Bollegala, 2021; Lauscher et al.,
2021) before the MLMs are deployed in down-
stream NLP applications that are used by billions
of users world-wide.",bias,,"bias, bias amplification",2023,"statement, concerns, suggestions"
1475,"The NeuroX toolkit provides a post hoc interpre-
tation of pre-trained models. The toolkit makes a
contribution towards improving the transparency
of deep models and may discover biases present
in these models. We do not foresee any direct eth-
ical issues with respect to the developed toolkit.
In terms of the neuron interpretation methods, the
majority of them are based on the correlation be-
tween neurons and the input. One potential issue
with such an interpretation is its faithfulness with
respect to the knowledge used by the model in mak-
ing predictions. However, this is not a limitation of
the toolkit but a limitation of the research methods
in general.
",no ethical concerns,,no ethical concerns,2023,statement
1476,"Our models are trained on a mix of Wikipedia and
arXiv articles, so they are naturally exposed to the
biases represented in that data. However, since the
models are restricted to only changing the whites-
pacing in text either by design (for the EO models)
or by the decoding procedure (for the ED mod-
els), we consider the ethical concerns of using our
models to be small.
",bias,,bias,2023,statement
1477,"Since pre-trained LMs are known to inherit unde-
sirable biases and tend to generate toxic contents
in some edge cases (Schick et al., 2021), the QAG
models we developed in the paper could potentially
generate a question or an answer including such
texts. Nevertheless, we have done internal valida-
tion on the generated question-answer pairs and we
have not found such examples in the data analysed
in this paper.",bias,,bias,2023,"concerns, actions"
1478,"While the QAG models are fine-tuned on pre-
trained language models, which are known to con-
tain some toxic contents (Schick et al., 2021), an
internal check does not reveal any toxic genera-
tion. However, there is a potential risk that the
QAG model could generate toxic text due to the
underlying LMs.
",bias,,bias,2023,concerns
1479,"This paper sheds light on the workings of the pre-
diction head of the fundamental models in NLP. In
recent years, unintended biases (e.g., gender bias)
in neural network models have been problematic.
This paper may help in this direction by encourag-
ing researchers to analyze the prediction head as
well as Transformer layers.
",no ethical concerns,,no ethical concerns,2023,statement
1480,"There might be a possibility that the texts we used
(CC-100 and Tatoeba) have socially biased, de-
spite their popular use in the NLP community. We
adopted cognitively-plausible restricted settings
with respect to data size, which can potentially be
aligned with environmentally friendly, green NLP.
",bias,,bias,2023,"concerns, actions"
1481,"In this work, we studied methods for choosing di-
verse demonstrations to improve in-context com-
positional generalization in semantic parsing. We
have only evaluated our methods on semantic pars-
ing datasets in English. It is our hope, however,
that improvements in compositional generalization
will eventually allow systems to generalize better
to languages that are not well represented in small
training sets.
",no ethical concerns,,no ethical concerns,2023,"statement, suggestions"
1482,"This paper describes work around VIRA , a real-
world DS addressing COVID-19 vaccine hesitancy.
In an attempt to alleviate concerns that users would
take action based on information given to them
byVIRA which might harm them, the terms of
use of the DS state that “This information ... is
not intended as a substitute for medical advice”.
We were guided with the principle of providing
accurate information, thus when building VIRA
we incorporated a direct mapping between intents
and responses. Future endeavours based on thisdataset, e.g., for building a generative bot for ad-
dressing vaccine hesitancy, should be aware of the
ramiﬁcations of showing to users such content.
In addition, the terms of use stated that queries
are stored and may be used for research purposes.
The chats collected might have originally con-
tained offensive language, often as a result of the
sensitivity of the domain to some users. We made
a dedicated effort to ﬂag these cases and mask
problematic terms. However, we did so with au-
tomatic measures, so the dataset might still con-
tain such language. Finally, although the data was
anonymized by masking various expressions, it is
still possible that some sensitive medical concerns
remain.
","offensive content, sensitive concerns",,"bias, offensive content, harmful information, sensitive information",2023,"concerns, actions, advice"
1483,"This work does not involve any sensitive data,
but only crowd-sourced datasets released in pre-
vious works, including Event-StoryLine (Caselli
and V ossen, 2017) and Causal-TimeBank (Mirza
et al., 2014). We believe that our research work
meets the ethics of ACL.
8
",no ethical concerns,,no ethical concerns,2023,statement
1484,"The study is granted ethics approval from the In-
stitutional Ethics Committee (20211013LZZ001).
All the clients and counselors signed a consent
form when using our counseling platform, which
informed them that the counseling conversations
collected on the platform would be used for sci-
entific research purposes, and might be used for
scientific research by third parties. During the an-
notation process, we spared no efforts to manu-
ally de-identify and anonymize the data to protect
clients’ and counselors’ privacy. The annotators
also signed data confidentiality agreements and ac-
quired ethical guidelines before they got access
to the conversation data. Meanwhile, they were
paid a reasonable wage for annotation. For the
rules of releasing data, the third-party researchers
who require access to the raw conversation data
must provide us their valid ID, proof of work, the
reason they request data (e.g., the research ques-
tions), etc. They are required to be affiliated with
an non-profit academic or research institution. This
includes obtaining the approval of an Institutional
Review Board (IRB), having principal investigators
working full-time as well as the written approval
of institution’s office of Research or equivalent of-
fice. Additionally, they must sign the Data Non-
disclosure Agreement and make promise that they
would not share the data with anyone.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1485,"We annotate the proposed dataset based on
W&I,LOCNESS, without copyright constraints
for academic use. For human annotation (Sec-
tion 3.3 and Section 5.6), we recruit our annotators
from the linguistics departments of local universi-
ties through public advertisement with a specified
pay rate. All of our annotators are senior under-
graduate students or graduate students in linguistic
majors who took this annotation as a part-time job.We pay them 60 CNY an hour. The local min-
imum salary in 2022 is 25.3 CNY per hour for
part-time jobs. The annotation does not involve
any personally sensitive information. The anno-
tated is required to label factual information (i.e.,
evidence words inside the sentence.).
",no ethical concerns,,no ethical concerns,2023,actions
1486,"Data curation: In social sciences, analysis of posts
of a website like Reddit is likely considered “fair
play"" as individuals are anonymous, and users can
understand their responses remain archived on the
site unless taken action to delete them. The Reddit
privacy policy states it allows third parties to access
public Reddit content through the Reddit API and
other similar technologies and users should take
that into consideration when posting.*And Reddit
data is already widely available in larger dumps
such as Pushshift (Baumgartner et al., 2020). We
collected only publicly available data in Reddit
and it did not involve any interaction with Reddit
*www.redditinc.com/policies/
privacy-policy-october-15-2020users. But a study on user perceptions on social
media research ethics (Fiesler and Proferes, 2018)
highlights some potential harms that can be caused
due to social computing research as internet users
rarely read or could fully understand website terms
and conditions and are unaware that the data they
share publicly could be used for research. In partic-
ular, this dataset contains sensitive information. So,
as suggested by Benton et al. (2017)’s guidelines
for working with social media data in health re-
search, in this paper, we share only anonymized and
paraphrased excerpts from the dataset. The shared
dataset will also contain anonymized usernames
and post identifiers.
",no ethical concerns,,no ethical concerns,2023,actions
1487,"Data Curation: Only publicly available data in
Reddit and CounselChat websites were used in this
work. Analysis of posts on websites such as Red-
dit is considered ""fair play"" since individuals are
anonymous and users are aware their responses re-
main archived on the site unless explicitly deleted.It is also stated in Reddit’s privacy policy that it
allows third parties to access public Reddit content.
3Also, Reddit’s data is already widely available in
larger dumps such as Pushshift (Baumgartner et al.,
2020). Even though the policies allow it, it should
be thoroughly noted that this data contains sensi-
tive information. Thus, we adhere to the guidelines
suggested by Benton et al. (2017) for working
with social media data in health research, and share
only anonymized and paraphrased excerpts from
the dataset so that it is not possible to recover user-
names through a web search with the verbatim post
text. In addition, references to usernames as well
as URLs are removed from dialogue content for
de-identification.
Human Evaluation: The human raters recruited
from the crowdsourcing platform, UpWork, were
all trained in the practice of counseling. Since the
methods were tested on English-only text, we re-
cruited workers who had professional competency
in the English language. We paid them $10 for eval-
uating each batch of rephrased sentences that re-
quired on average ≈30 minutes to complete. Thus,
the amount paid to the human raters was ≈2.75
times above the US minimum wage of $7.25 per
hour. We also paid an extra $2 as a bonus per each
batch for workers who obtained an above-average
agreement with the other worker who rated the
same batch.
Chatbots for Distress-Consolation: One of the
main applications of the proposed methodology
is boosting chatbot responses for distress consola-
tion with motivational interviewing strategy. Us-
ing chatbots for distress consolation or other men-
tal health interventions has raised ethical concerns
among many (Lanteigne, 2019; Montemayor et al.,
2021; Tatman, 2022). However, chatbots that in-
tervene in mental health-related matters have al-
ready been developed and have been quite pop-
ular for a while. Some examples are SimSensei
(DeVault et al., 2014), Dipsy (Xie, 2017), Woebot
(woebothealth.com ), and Wysa ( www.wysa.io ).
Czerwinski et al. (2021) state, About 1 billion peo-
ple globally are affected by mental disorders; a
scalable solution such as an AI therapist could be
a huge boon . The current technology to develop
such chatbots rely heavily on deep learning and
pre-trained language models. But due to the inher-
ently unpredictable nature of these models, they
pose a threat of delivering unfavourable responses
when such chatbots are used for distress consolation. We believe the methodology we suggest
in this work can help them become more reliable
and fail-safe by adhering to the motivational interviewing strategy, a guiding style of communication
heavily practiced in psychotherapy. However, since
the unfavourable response detection and rephrasing
methods still rely on neural network models, the
artifacts produced in this paper should be used for
research purposes only and real-world deployment
of them should be done under human supervision.",unfavorable responses,,unfavorable responses,2023,"statement, concerns, actions, advice"
1488,"We do not see potential ethical concerns or misuse
of the proposed evaluation method. One potential
risk, though minimal, could be the misinterpreta-
tion of the findings of this paper. We would like
to caution readers that a higher score of our metric
may not necessarily reflect a higher quality per-
ceived by humans, as the evaluation metric only
measures the explanation’s benefit from the mod-
eling perspective, and it is only one of the many
possible ways of automatically evaluating the qual-
ity of natural language explanations.
",misinterpretation,,findings misinterpretation,2023,"statement, concerns"
1489,"Our method uses a constrained set of action sam-
ples to generate the textual actions in each step.
Since this action set is generated from a controlled
vocabulary of actions and entities, the produced
actions cannot contain harmful content like hate
speech and racial biases. Furthermore, our neuro-
symbolic model produces human interpretable
rules for the action policy thereby making the
model transparent and easier to control. Due to
these reasons, the ethical risk from this work is
low.
",no ethical concerns,,no ethical concerns,2023,actions
1490,"Here we discuss the primary ethical considerations
of the DiaASQ dataset.
Intellectual Property Protection. Our dataset is
collected from the open Chinese social media plat-
form via the officially open API.4Permissions are
granted to copy, distribute and modify the contents
under the terms of Weibo API distribution.
Privacy Claim. The user-specific information in
the data is anonymized during preprocessing, and
no personal information of the user or customer
is included. The data collection procedure is de-
signed for factual knowledge acquisition and does
not involve privacy issues.
Annotator Information and Compensation.
The crowd-sourcing annotators are the senior post-
graduate students who are trained before annotat-
ing. We estimated that a skillful annotator needs
3 to 5 minutes to finish an annotation for each di-
alogue utterance. Therefore, we paid annotators 1
yuan ($0.15) for each utterance. The salaries for
linguistic and computer science experts are deter-
mined by the average time they devote.
",no ethical concerns,,no ethical concerns,2023,actions
1491,"In this work, we construct a type of code-mixed UD
forest based on the existing UD resources. We note
that all the data construction has been accomplished
automatically, and we have not created any new an-
notations with additional human labor. Specifically,
we use the UD v2.10 resource, which is a collection
of linguistic data and tools that are open-sourced.
Each of treebanks of UD has its own license terms,
including the CC BY-SA 4.08andCC BY-NC-SA
8http://creativecommons.org/licenses/by-sa/4.
0/
2.5-4.09as well as GNU GPL 3.010. Our use of UD
treebanks comply with all these license terms is at
non-commercial purpose. The software tools (i.e.,
UDPipe parsers) are provided under GNU GPL V2 .
Our use of UDPipe tools complies with the term.",no ethical concerns,,no ethical concerns,2023,statement
1492,"Our proposed method can generate personalized
dialogue responses to users and improve the en-
gaginess of the dialogue systems. It faces sev-
eral common ethics concerns that a neural dia-
logue system may generate unexpected responses
that make human users uncomfortable. However,
it is common for most neural dialogue systems.
Another potential risk is that the persona genera-
tor may generate unexpected persona information
that makes user uncomfortable. This issue could
be addressed by adding constraints on the gener-
ated persona information.
","unexpected responses, uncomfortableness",,"unexpected responses, uncomfortableness",2023,concerns
1493,"Our adaptive contrastive knowledge distillation
framework aims to improve the performance of
knowledge distillation methods and does not intro-
duce extra ethical concerns compared with other
knowledge distillation approaches. Therefore,
there are no ethical problems caused by the pro-
posed method.
",no ethical concerns,,no ethical concerns,2023,statement
1494,"We would like to list a few ethical considerations
for our work. First, GENEV A is derived from
FrameNet which comprises of annotated sentences
from various news articles. Many of these news
articles cover various political issues which might
be biased and sensitive to specific demographic
groups. We encourage careful consideration for uti-
lizing this data for training models for real-world
applications.
",bias,,bias,2023,"concerns, advice"
1495,"A potential concern for the proposed system is that
this training strategy may amplify the existing toxic
behavior or bias of the language model if the re-
lated keywords get prioritized in the training ob-
jective. Reducing the toxic or biased behaviors of
the proposed model can be an interesting research
direction for future work.
","bias, toxic content",,"bias, toxic content",2023,"concerns, suggestions"
1496,"In practice, a provider may publicly release a model
but may not wish its knowledge to be transferred
into another one. Applying our method on such
models will result in model stealing (He et al.,
2022) related ethical concerns. How to detect this
kind of misconduct still needs further exploration.
Although sharing knowledge without exposing pri-
vate data is one of the potential benefits of our
method, models produced by our method are still
vulnerable to attacks such as membership infer-
ence (Hisamoto et al., 2020), and the private train-
ing data could still be stolen from the model.
","privacy, security, misuse",,"privacy, security, misuse",2023,concerns
1497,"Our proposed HiFi can effectively reduce the re-
source consumption of PLMs during the fine-
tuning phase, which helps to decrease carbon emis-
sions, thus making the large-scale models more en-
vironmentally friendly and sustainable. In addition,
all the models and datasets used in our experiments
are publicly available and have not been reported
to carry social bias against any sensitive attributes,
and the proposed approach would not explicitly
introduce new negative societal impacts.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1498,"This paper will not pose any ethical problems. First,
machine translation is a standard task in natural
language processing. Second, the datasets used
in this paper have been already used in previous
papers.
",no ethical concerns,,no ethical concerns,2023,statement
1499,"This paper presents a new ECPE method, PRG-
MoE, which extracts emotion and their correspond-
ing cause in a dialogue through the relationship
between utterances in pairs. PRG-MoE shows high
performance in extracting emotion-cause pairs in
a conversation. In this regard, PRG-MoE could
be deployed in cause extraction in a dialogue and
other real-world applications. We do not report any
data collection process in this paper, as we experi-
ment with an open-domain dataset. We experiment
with the dialogue dataset based on RECCON (Po-
ria et al., 2021). RECCON is publicly available,
and there is no ethical issue.
",no ethical concerns,,no ethical concerns,2023,statement
1500,"This paper proposes block scope-based source code
summarization via shared block representation that
utilizes block-scope information by representing
various structures of the code block, which is bene-
ficial to increase the efficiency of developers. The
research conducted in this paper will not cause
any ethical issues or have any negative social ef-
fects. The data used is all publicly accessible and
is commonly used by researchers as a benchmark
for program and language generation tasks. Our
proposed method does not introduce any ethical or
social bias or worsen any existing bias in the data.
",no ethical concerns,,no ethical concerns,2023,statement
1501,"This paper proposes a task-agnostic prompt tuning
method for the PLG tasks to bridge the gap between
pre-training and fine-tuning of PLMs for program
and language and to efficiently update the param-
eters of PLMs for program and language, which
is beneficial to energy efficient Program and Lan-
guage applications. The research conducted in this
paper will not cause any ethical issues or have any
negative social effects. The data used is publicly
accessible and is commonly used by researchers as
a benchmark for program and language generation
tasks. The proposed method does not introduce any
ethical or social bias, or worsen any existing bias
in the data.
",no ethical concerns,,no ethical concerns,2023,statement
1502,"Advances in multi-user dialog generation tech-
niques would aid training of digital assistants. As
AI assistants are increasingly becoming a staple
in our social environments, synthetic methods of
multi-user dialog generation would aid the training
of these assistants and ensure they are capable of
comprehending human conversations and under-
stand task-oriented requests in social settings. This
would help increase human-machine interaction
and enhance human productivity in collaborative
settings.
Synthetic multi-user dialog generation tech-
niques would also reduce the need of (the gold
standard for data collection) crowdsourcing. This
would also have a positive effect on human produc-
tivity and reduce the need for humans to manually
write dialogs for different scenarios.
We use language models as initialisation for
our dialog generators. These are trained on data
collected from the web. Hence, issues related to
bias and abusive language are a potential concern.
These concerns of abusive content should be largely
mitigated as we ﬁne-tune of the dialog genera-
tors on task-oriented and everyday conversation
datasets with sanitised data. The generator ﬁne-
tuning and prompt structure used for dialog gen-
eration should limit unintended consequences as
all generations are trained to reﬂect the intent of
the single-user dialog. However, with our proposed
method of multi-user dialog generation, any racial,
ethnic or other forms of bias present in the datasets
used to train the dialog generators is likely to get
propagated to the generated multi-user dialog.
",bias,,bias,2023,"statement, concerns, actions"
1503,"No ethics concerned with our work.
",no ethical concerns,,no ethical concerns,2023,statement
1504,"The data collected for this study is highly sensitive
and contains personal information of the callers. To
protect their privacy, all personal identifiable infor-
mation such as names, ages, addresses, phone num-
bers, and places of work were removed during the
transcription process. The data is securely stored
and access to it is restricted to only authorized per-
sonnel. This study was reviewed and approved by
the Human Research Institutional Review Board
(IRB) at National Chengchi University3with the
case number NCCU-REC-202102-006.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1505,"In this work, we collected human annotations on
dialogue response quality using MTurk. Each HIT
in our MTurk study contained one dialogue history
and four candidate responses. The annotators could
read the history and rate the responses that followed
using mouse clicks on their response choices. We
provided an additional feedback ﬁeld for annotators
to write comments in. We received very positive
feedback on the task from all the annotators who
used this feature. There were no restrictions on the
minimum or maximum number of examples the an-
notators had to rate. From a pilot study on MTurk,
we found the average time to complete one HIT to
be slightly under 2.5 minutes. After considering
the average time required and the task difﬁculty
(expressed to be clearly and easily understood by
annotators in their comments) we set the payment
amount to $0.5 per HIT for an hourly rate of about
$12 per hour.
",no ethical concerns,,no ethical concerns,2023,actions
1506,"This work primarily targets the recommendation of
task-oriented bots, necessitating a degree of person-
alization. To enhance recommendation effective-
ness, personalized behavior data may be collected
for further refinement. Balancing the dynamics be-
tween personalized recommendation and privacy
is a critical consideration. The data collected may
contain subjective annotations, and the present pa-
per does not dive into these issues in depth. Future
work should address these ethical considerations,
ensuring an balance between personalized recom-
mendations and privacy preservation.
","bias, privacy",,"bias, privacy",2023,"concerns, suggestions"
1507,"This work does not involve the presentation of a
new dataset, nor the utilization of demographic or
identity characteristics in formation. In this work,
we propose a method for adapting contextualized
embeddings for WSD using lexical resources. The
proposed method is not limited to a specific re-
source, we used WordNet as the source of semantic
relation knowledge and sense inventory. Therefore,
adapted embeddings and sense disambiguation be-
havior may reflect the incomplete lexical diversity
of WordNet in culture, language (Liu et al., 2021),
and gender (Hicks et al., 2016).",incomplete diversity,,no_diversity,2023,"statement, concerns"
1508,"Data and code
In our experiments, we use the original LAMA
benchmark dataset from Petroni et al. (2019) as
is. All data are based on publicly available data
sources and data statistics can be found in the orig-
inal paper. Parts of the code are based on LAMA2.
The license of the code can be found in the supple-
mentary material.
Details of experiments
The experiments were conducted using a 2.4GHz
CPU and an NVIDIA TESLA P100 GPU. Infer-
ence time was 1–1.5 s per instance for BERT-base
and 2–3 s per instance for BERT-large.
Potential risks
This study evaluates the knowledge stored in lan-
guage models considering the reliability of model
predictions. However, it should be emphasized that
the outputs of the selective classifier constructed by
the proposed method do not guarantee the correct-
ness of the model predictions. For the validation
of each fact, this method should only be used as an
aid, and the final decision should be made by the
user.
",inaccuracies,,inaccuracies,2023,"statement, concerns, advice"
1509,"Data and code
In our experiments, we use the original LAMA
benchmark dataset from Petroni et al. (2019) as
is. All data are based on publicly available data
sources and data statistics can be found in the orig-
inal paper. Parts of the code are based on LAMA2.
The license of the code can be found in the supple-
mentary material.
Details of experiments
The experiments were conducted using a 2.4GHz
CPU and an NVIDIA TESLA P100 GPU. Infer-
ence time was 1–1.5 s per instance for BERT-base
and 2–3 s per instance for BERT-large.
Potential risks
This study evaluates the knowledge stored in lan-
guage models considering the reliability of model
predictions. However, it should be emphasized that
the outputs of the selective classifier constructed by
the proposed method do not guarantee the correct-
ness of the model predictions. For the validation
of each fact, this method should only be used as an
aid, and the final decision should be made by the
user.
",inaccuracies,,inaccuracies,2023,"statement, concerns, advice"
1510,"In this study, we created our dataset from
English Wikipedia. The editors of English
",no ethical concerns,,no ethical concerns,2023,actions
1511,"The work studied the impact of several adversarial
training methods on robustness and generalization.
The work did not result in any new dataset and
model and it has no potential ethical issues. On
the positive side, the work targets two important
attributes of trustworthy AI i.e. robustness and gen-
eralization. Our work provides an insightful com-
parison of the input-space and embedding space
adversarial training approaches and will positively
impact the future research work in this area.
",no ethical concerns,,no ethical concerns,2023,statement
1512,"Ethical considerations are of utmost importance in
this work. It is essential to exercise caution and
consider the ethical implications when using this
method, as it has the potential to be applied in situ-
ations where fair and unbiased decision-making is
critical. It is important to thoroughly evaluate the
effectiveness of the method in the specific context
in which it will be used, and to carefully considerthe data, fairness metrics, and overall application
before deploying it. It is worth noting that our
method is limited by the fact that gender is a non-
binary concept and that it does not address all forms
of bias, and further research is necessary to identify
and address these biases. Additionally, it is impor-
tant to consider the potential risk of inadvertently
increasing bias through reversing the direction of
the debiasing operation in the algorithm. It is cru-
cial to be mindful of the potential impact of this
method and to approach its use with caution and
care.
","bias, misuse",,"bias, misuse",2023,"statement, concerns, advice"
1513,"Broader impact We do not see any serious ethi-
cal problem connected to this research. At the same
time, we are aware of the risks associated with the
development and use of large NLP models that we
use in this research. Such risks include the envi-
ronmental impact of the computational resources
required for training and the encoding and possi-
ble amplification of biases present in the massive
amounts of un-curated data the models learn from.
","bias, environmental_impact",,"bias, environmental_impact",2023,"statement, concerns"
1514,"This paper proposes a unified framework,
KnowledgeDA , for text augmentation based on do-
main KGs for domain-specific NLP tasks. All the
experimental datasets and KGs are publicly avail-
able, and the related papers and links have been
listed in the paper. Also, though PLM and KG are
publicly available, there may still be several ethi-
cal considerations to bear in mind when applying
KnowledgeDA in real-world scenarios. For instance,
it is crucial to check whether KG contains biases
concerning race, gender, and other demographic
attributes.
",bias,,bias,2023,"statement, concerns, advice"
1515,"Our work offers benchmarks and insights to help
develop language models that understand negation.
Developing language models that understand nega-
tion is crucial to the society in many ways.
First, as language models are being used in vari-
ous real-world applications, including fields like fi-
nance, healthcare, and law, it is important to ensure
that they understand negation and make correct pre-
dictions. If they do not understand negation, they
may output the opposite of what we actually want
and may make harmful decisions for humans.
Negation is also a fundamental aspect of natural
language understanding, and a language model that
does not understand negation correctly may not be
able to truly process natural language. This can
undermine trust and confidence in the outputs of
the model, ultimately undermining its utility.
Understanding negation correctly is therefore
crucial for the development of reliable languagemodels. We hope that our benchmark and eval-
uation results provide insights into the behavior
of current language models and inspire the future
development of language models that understand
negation.

",no ethical concerns,,no ethical concerns,2023,"statement, concerns, advice"
1516,"Our work compares existing work on character-
level language modelling, and we do not anticipate
that it introduces any new risks beyond those intro-
duced by the work we build on.
",no ethical concerns,,no ethical concerns,2023,statement
1517,"The datasets were either created and published by
reseachers in psycholinguistics, or synthetically
generated by the authors without use of harmful
information. No experiments involving human sub-
jects were included in the paper. The authors do
not foresee any ethical concerns in this paper.
",no ethical concerns,,no ethical concerns,2023,statement
1518,"We declare that this article is in accordance with the
ethical standards of ACL Code of Ethics . Any third
party tools used in this work are licensed from their
authors. All crowd-workers participating in the
experiments are paid according to the local hourly
wages.
10 Acknowledgment
We would like to thank anonymous reviewers for
their insightful and constructive feedback. We ap-
preciate Peng Li and Shuo Wang for their valuable
discussions. We thank Qianlin Liu, Yanqi Jiang and
Yiwen Xu for the crowdsourced work. This work
is supported by the National Key R&D Program
of China (2022ZD0160502) and the National Nat-ural Science Foundation of China (No. 61925601,
62276152, 62236011).
",no ethical concerns,,no ethical concerns,2023,statement
1519,"This paper proposes a domain attention module
with distributional signatures to better learn the
domain-speciﬁc and general knowledge. We also
deﬁne an adjacent n-gram pattern to mine the en-
tities in the context. We work within the purview
of acceptable privacy practices and strictly follow
the data usage policy. We use public datasets and
consist of their intended use in experiments. We
described our experimental setting in detail to en-
sure reproducibility. We neither introduce any so-
cial/ethical bias to the model nor amplify any bias
in the data, and our work will not have any social
consequences or ethical issues.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1520,"This work presents NoisywikiHow, a free and open
dataset for the research community to study learn-
ing with noisy labels. Since the data in Noisywiki-How is constructed based on the wikiHow website,
which is free and open for academic usage, there is
no privacy issue. We declare that all information in
this paper has been obtained and presented follow-
ing the ACL Ethics Policy. As required by these
rules and conduct, we have fully cited and refer-
enced all material and results that are not original
to this work.
",no ethical concerns,,no ethical concerns,2023,statement
1521,"8.1 Annotation
To facilitate research, we reconstruct a dataset
from a large unannotated dataset Open Subtitle
and a small annotated dataset MEmoR, which is
annotated with speakers and non-speaker emotions.
Both datasets are publicly available and are col-
lected from TV shows. We use the emotion elicited
in actors (transcripts) as elicited emotions in our
research. To verify that the approach is valid, a
blind check was conducted on a randomly selected
set, where two annotators were asked to make man-
ual decisions on whether a target emotion can be
elicited. Annotators are recruited college students
from universities whose primary teaching language
is English, and compensated with course credit.
Our reconstructed dataset has a annotator agree-
ment of 80% accuracy ( Cohen′s κ= 0.491). In
our researches, for the purpose of validating the
dataset and evaluate model results, annotators are
only asked to evaluate if the emotional labels were
valid, not to offer personal emotion feedback. To
ensure reprehensibility, we would release the recon-
structed dataset along with the paper at http://XXX.
8.2 Elicit Rich Emotions
Our model elicits only positive emotions, but our
dataset contains labeling of negative emotions,
which exist in the TV show dialogues naturally. We
demonstrate that using all emotions would not only
benefit the differentiation between all emotions, but
also help the model to better elicit positive emo-
tions. Naturally, there are emotions that are consid-
ered to be more positive and the ones that are more
negative. We intend to model various emotions
so that a system is more aware of the correlation
between intention and response. Consequently, amodel can, for example, be aware that a certain type
of answer may result in sadness and thus avoid it.
In addition, a model can better understand user at-
titudes also by capturing such intentions in them.
However, the modeling of multi-various emotions
is not necessarily for the purpose of eliciting them.
In application, we only elicit emotions that are con-
sidered to be positive, as our goal is to better elicit
rich positive emotions in dialogue.5
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1522,"This paper honors the ACL Code of Ethics. Public
available datasets are used to establish the GLUE-
X leaderboard. No private data was used. All
annotators from the crowdsourcing company have
received enough labor fees corresponding to their
amount of annotated instances. The code and data
are open-sourced under the CC-BY-NC-SA license.
",no ethical concerns,,no ethical concerns,2023,actions
1523,"Data license. For the EXAM and MEDIA do-
mains of NaSGEC, we only collect sentences from
public corpora or websites. For the THESIS domain,
we have obtained permission from the authors of
dissertations.
Annotation payment. During annotation, all an-
notators/reviewers were paid according to their
finished task numbers and quality. The average
salaries for annotators and reviewers are about 25
and 34 RMB per hour, respectively.
",no ethical concerns,,no ethical concerns,2023,actions
1524,"There are no known potential risks.
",no ethical concerns,,no ethical concerns,2023,statement
1525,"No consideration.
",no ethical concerns,,no ethical concerns,2023,statement
1526,"We comply with the ACL Ethics Policy. Corefer-
ence resolution is fundamental in NLP, which often
serves as a component of other NLP applications.
Since it does not directly interact with users, there
are no additional ethics concerns. All the data used
in this paper is public. We confirm that the scien-
tific artifacts used in this paper comply with their
license and intended use. We list the licenses in
Table 7.
",no ethical concerns,,no ethical concerns,2023,statement
1527,"This paper honors the ACL Code of Ethics. Public
available datasets are used to establish our results.
No private data and crowd-sourcing work are used
to produce predictions. The code and data are open-
sourced under the CC-BY-NC-SA license.
",no ethical concerns,,no ethical concerns,2023,statement
1528,"We honor the ACL Code of Ethics. No private data
or non-public information is used in this work. For
human annotation (Section 6.3 and Section 6.4),
we recruited our annotators from the linguistics
departments of local universities through public
advertisement with a specified pay rate. All of
our annotators are senior undergraduate students or
graduate students in linguistic majors who took this
annotation as a part-time job. We pay them 60 CNY
an hour. The local minimum salary in the year 2022
is 25.3 CNY per hour for part-time jobs. The an-
notation does not involve any personally sensitive
information. The annotated is required to rank the
system output and label factual information (i.e.,
syntactic annotation).
",no ethical concerns,,no ethical concerns,2023,actions
1529,"This paper honors the ACL Code of Ethics. The
dataset used in the paper does not contain any pri-
vate information. All annotators have received
enough labor fees corresponding to their amount of
annotated instances. The code and data are open-
sourced under the CC-BY-NC-SA license.
",no ethical concerns,,no ethical concerns,2023,statement
1530,"Our work complies with the ACL Ethics Policy. As
document-level event argument extraction is a stan-
dard task in NLP, we do not see any critical ethical
considerations. We confirm that the scientific arti-
facts used in this paper comply with their license
and intended use. Licenses are listed in Table 7.
",no ethical concerns,,no ethical concerns,2023,statement
1531,"Copyright and Citation Issue The copyright of
individual datasets in SUMM ZOObelongs to the
original authors. The usage license of each dataset
also applies to SUMM ZOO. To ensure fair credit,
when using SUMM ZOOfor evaluation, please also
cite original papers, where individual datasets are
introduced.
Data Availability and Safety Pre-training and
fine-tuning summarization data studied in this pa-
per are mostly publicly available, otherwise we will
provide links to the access application. Although
filtering has been conducted in building the original
datasets, some contents can contain uncomfortable
descriptions, e.g., news coverage of violent crimes
and events.
Usage of Large PLM The GPT-3.5 model is
used to generate text (summaries) for input doc-
uments of summarization tasks. The generated text
is only used for experiments and analysis, which
are presented in corresponding sections. No further
usage, e.g., generating content for manuscripts, of
GPT-3.5 or its family, is included in this paper.
Human Evaluation We conduct human evalua-
tion with the help of one judge, who obtained their
postgraduate degree in the United Kingdom and
has a solid experience in evaluating summarization
tasks. They were compensated through a payment
of around 400USD for 450instances (§ 7).
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1532,"Data Usage and License ConvSumX is based
on two public English conversation summariza-
tion datasets, namely DIALOG SUMand QMSum.
Both datasets are freely available online under the
MIT license, which has no constraint to academic
use, modification, and further distribution. We will
follow the MIT license to make our data (anno-
tated target summaries/queries and corrected En-
glish summaries/queries) freely available online.
Human Annotation The construction of Con-
vSumX involves human annotation. We hire 4ex-
pert translators as our annotators and editors for
each target language. The total cost is around
6,500USD, which applies to our annotation (in-
cluding quiz annotation) and review. The hourly
salary is equal. The total annotation time (includ-
ing training annotation and editing) for Zh, Fr and
Ukr is around 96,96, and 120hours (according to
our annotation cost/hourly salary). Detailed infor-
mation about our annotators/judges/editors can be
found in Appendix A.
Content Safety During our annotation, annota-
tors are explicitly asked to not involve any per-
sonal/violent information and to write summaries
strictly limited to the scope of source input text.
Also, if any violent or uncomfortable information
is found in source input text, annotators are asked
to report such issues. All data are further reviewed
by editors. With careful checking and evaluation,ConvSumX (including source input text) contains
no personal/violent content, and is safe to use.
",no ethical concerns,,no ethical concerns,2023,actions
1533,"To the best of our knowledge, there are no ethics
concerns with this paper.
",no ethical concerns,,no ethical concerns,2023,statement
1534,"The human subjects used in this study have been
experienced, trained annotators who have been in
personal contact with the authors, and who have
been recruited by a call specifically suited for the
experiment and study presented here. The call has
been sent to all trained annotators already working
with the authors, and volunteers have been asked to
respond, on a first-come first-chosen basis. The pay
has corresponded to the standard pay for similar an-
notation tasks taking also the relatively short notice
into consideration (for the numbers, see Sect. 4.2).
Both annotators were males; this is a possible short-
coming, but there were no female volunteers and
from the previous cooperation (with a mixed team
of female and male annotators), no differences in
the annotation results have been observed.
No personal information has been among the
lemmas extracted and used for the preselection.
The data for the LLM might have contained it,
but it would not show because the experiment and
the ontology is currently limited to common verbs
which do not describe any personal names or other
personal information.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1535,"The human subjects used in this study have been
experienced, trained annotators who have been in
personal contact with the authors, and who have
been recruited by a call specifically suited for the
experiment and study presented here. The call has
been sent to all trained annotators already working
with the authors, and volunteers have been asked to
respond, on a first-come first-chosen basis. The pay
has corresponded to the standard pay for similar an-
notation tasks taking also the relatively short notice
into consideration (for the numbers, see Sect. 4.2).
Both annotators were males; this is a possible short-
coming, but there were no female volunteers and
from the previous cooperation (with a mixed team
of female and male annotators), no differences in
the annotation results have been observed.
No personal information has been among the
lemmas extracted and used for the preselection.
The data for the LLM might have contained it,
but it would not show because the experiment and
the ontology is currently limited to common verbs
which do not describe any personal names or other
personal information.",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1536,"As mentioned in the limitations section, we note
that these experiments do not account for poten-
tial biases prevalent in fine-tuning large language
models. In a real world deployment of our model,
we hypothesize that there could be a potential mis-
labelling of factuality values depending on bias
towards sources of utterances. For example, if a
power figure states an event, will the event label
be more biased towards being factual just because
of the source of the statement? We will investigate
these questions and issues in future work.
We also note that our paper is foundational re-
search and we are not tied to any direct applica-
tions.
",bias,,bias,2023,"concerns, suggestions"
1537,"Since privacy concerns and the risk to the individu-
als should always be considered, especially using
social media data, we have employed mechanisms
to avoid any harmful and negative consequences
of releasing our model. To this end, we removed
individually identifiable information such as user
names, user IDs, or e-mail addresses. We also re-
moved any URLs from our data not to be trained
on such personal information in our model. As for
the use of open datasets in this work, we used them
in accordance with guidelines that allow their use
within the established usage policy. Especially we
ensure that no attempts can be made to establish
contact with specific individuals or deanonymize
users in the datasets.
Our paper may contain direct references to spe-
cific disorders or diseases (such as psychiatric pa-
tients, Siamese, or names of mental disorders) and
expressions that could be considered offensive to
particular individuals. We want to emphasize that
these expressions are used solely for the purpose
of academic discourse and are not intended to be
disrespectful or offend anyone.
In addition, our proposed model is not intended
to label or stigmatize individuals online but rather
to serve as a warning system for potential threats
to personal well-being and public health. It is im-
portant to note that even if this model identifies
potential mental illnesses and symptoms, it should
not be considered a definitive diagnosis. Still, the
model provides an indication of the likelihood of a
disorder; it should be used as a reference for self-
diagnose and in consultation with a mental health
expert for an official diagnosis. An official diag-
nosis and results require consultation with medical
and psychological experts, and this system aims
at serving as an aid in the diagnostic process. We
make our implementation code publicly available
for research purposes, and we hope it will be used
to improve the lives of individuals suffering from
mental illnesses.","offensive content, faithfulness",,"offensive content, faithfulness",2023,"statement, actions, concerns"
1538,"We recruit annotators from a college campus. They
are completely free to decide whether or not to
participate in our annotation. The payment is 9
dollars per hour, higher than the local minimum
wage. There is no personal information in our
collected dataset. The information which may be
used to identify the participants is deleted after the
annotation.
The model-generated summaries may contain
toxic language, which can make annotators uncom-
fortable. We reviewed the data before annotation
and found no problematic samples.
We check the licenses of the artifacts used in this
study and do not find conflicts. The license of the
dataset we will release is CC BY-NC 4.0.
",toxic content,,toxic content,2023,"statement, actions, concerns"
1539,"We recruit annotators through the campus BBS.
They are completely free to decide whether to par-
ticipate and can quit in the middle. They are paid
$15 per hour, more than the local minimum wage.
No participants’ personal information or payment
information will be released. Some of the informa-
tion is temporarily stored on the server and will be
deleted at the end of the study.
The application of datasets, models, and tools
in our study is consistent with their intended use
and license. We hope the artifacts we release are
to be used for academic research (non-commercial
licence: CC BY-NC 4.0).
",no ethical concerns,,no ethical concerns,2023,actions
1540,"We disabled the NSFW filter in the generation
models, thus possibly generating disturbing con-
tent. The input texts were also containing possibly
NSFW prompts so we considered that the filter
removal was necessary to participate in this task.
",disturbing content,,disturbing content,2023,statement
1541,"For the most part we relied on existing pre-trained
Large Language Models, with the exception of
TwiBERT which we built from scratch. While a
model like RoBERTa includes multi-lingual text,
whether this works or is relevant to a low-resource
language like Twi is another question. In general
the most popular and widely used LLMs have a
very strong bias towards English and may or may
not prove useful with other languages.
The fact that low-resource languages are clearly
not a priority of the organizations carrying out com-
putationally expensive pre-training of LLMs skews
our field away from the languages of Africa and
towards English, which is already very well repre-
sented.
In addition any biases or stereotypes that exist
in these models get propagated to these new tasks
and applications of these models, and we are not
even fully cognizant of the harms that are contained
therein.
In general the blind use of LLMs to some-
how hopefully get a reasonable result with a low-
resource language simply reinforces the existing
hierarchy of languages in NLP, where English is
at the top and low-resource languages are included
in training data by accident or as a second thought.
This style of approach also serves to invalidate the
expertise of those who actually know and use the
low-resource language.
There is a related danger with the leaderboard fo-
cused evaluation as is taken by SemEval that teams
with large compute resources will be able to do well
in a low-resource task like this simply by throw-
ing hyperparameter tuning methods at the problem.
Indeed it might be possible that a team with no ex-
pertise in African languages could do quite well onthe leaderboard and even be declared the ""winner"".
This would send a chilling and unintended message
from SemEval that expertise in low-resource lan-
guages doesn’t really matter and that compute is
all you need even if you can’t read nor understand
the language under study. To act as a brake on this
unfortunate direction, we would strongly recom-
mend that SemEval expect and require meaningful
evaluation of the results of low-resource tasks that
would require expertise in the language. We would
also discourage the continued use of terminology
that associates the highest score with ""winning""
a task, particularly if said winners are unable to
understand the output of their system.
",bias,,bias,2023,"concerns, advice"
1542,"We use publicly available resources in our experi-
ments, in accordance with their license agreements.
The datasets are fully anonymized and do not con-
tain personal information about the caption annota-
tors or any information that could reveal the identity
of the photographed subjects.
",no ethical concerns,,no ethical concerns,2023,statement
1543,"It is important to acknowledge that our approach
utilizes a large language model trained on data from
the internet, which may contain inherent biases.
Therefore, it is crucial to exercise caution when
applying this model in applications such as writing
assistance, where it may have a direct impact on
individuals or groups.
We also have considered ethical considerations
in the construction and use of our evaluation dataset.
The dataset we used was automatically constructed
from publicly available datasets and lexical re-
sources. To the best of our knowledge, the origi-
nal datasets do not contain offensive content. The
names included in the datasets are from texts that
are already publicly available. We did not use the
help of third-party annotators to produce any addi-
tional data. The datasets we used did not include
any license agreements or terms of use. The only
requirement was to cite the dataset papers, which
we have done in Section 3.5. Additionally, we in-
tend to release our dataset publicly to encourage
further research and development in the field of
lexical substitution.
",bias,,bias,2023,"statement, concerns"
1544,"We do not foresee any ethical issues arising from
this work.
9
",no ethical concerns,,no ethical concerns,2023,statement
1545,"I am not aware of any specific social risks that
this work directly creates or exacerbates. However,
because morphological analysis is a core text pro-
cessing function used in various NLP applications,
those who attempt to abuse NLP applications may
benefit from the proposed method’s efficiency.
",misuse,,misuse,2023,"statement, concerns"
1546,"The current work is a fundamental research and it
is not essentially related to a particular application.
We do not predict any ethical concerns from the
algorithms and technologies proposed in this work.
We have utilized publicly available dataset and
open source libraries, which have been published
before.
",no ethical concerns,,no ethical concerns,2023,statement
1547,"This paper discusses methods that are intended
to enhance the ability of researchers within do-
mains such as social sciences to more easily apply
state-of-the-art NLP algorithms to their work. This
inherently comes with the risk of bias, and it is
entirely possible that seed datasets based on in-
complete or biased datasets will cause slingshot
learning to serve as an “amplifier”. We encourage
caution when applying machine learning to social
or other “real-world impactful” domains, and dou-
bly so when extrapolating from smaller amounts of
data using machine learning. This paper serves as
an initial evaluation and high-level exploration of
slingshot learning, and we would not recommend
using slingshot learning in situations that could
potentially have negative consequences. Further
exploration is certainly required, both specific to
slingshot learning and regarding the more general
use of machine learning in these scenarios.
On the other hand, methods like slingshot learn-
ing can empower researchers and allow them to
leverage tools that would otherwise be infeasible.
It also has other benefits, which are discussed in
the paper - for example, it facilitates taking a “snap-
shot” of a black box model (such as an API-based
NLP model). It is our intention, and our hope, that
slingshot learning is a useful tool that helps democ-
ratize machine learning within the social sciences
and other applied domains.
",bias,,"bias, bias amplification",2023,"statement, concerns, advice, suggestions"
1548,"The aim of our work is to extract high-quality par-
allel corpus from a noisy pseudo-parallel corpus.
The datasets that we used in this work are pub-
licly available and we have cited the sources of all
the datasets that we have used. Publicly available
datasets can contain biased sentences. We have also
created a dataset for Hindi-Bengali few-shot QE.
We briefly discuss the annotation guideline given
to the annotators for the task in the Appendix A.3.
",bias,,bias,2023,"statement, actions"
1549,"In our proposed metric, word tokens are masked
from left to right following the writing tradition in
English; however, for speakers of languages such
as Arabic, a “right to left” notation would be more
intuitive. Note, however, that this is primarily a de-
notational difference that does not affect the score
itself (LLMs do not discriminate left and right, only
beginning and end). We do not anticipate any spe-
cific harms that would be intrinsically associated
with the techniques described in this paper.
",model inaccuracies,,model inaccuracies,2023,statement
1550,"Federated learning systems promise the ability to
learn useful models without needing access to pri-
vate, protected data on user’s devices. By contribut-
ing improvements to personalization and contextu-alization within the federated learning paradigm,
FedPerC takes a step towards improving fairness of
federated learning systems, which otherwise strug-
gle with fitting to data distributions that are not
common in training populations. However, it is
important to note that FedPerC works to maximize
the likelihood of the observed data, which may
reinforce existing societal biases and stereotypes–
there are no protections or safeguards in place to en-
sure responsible generation or unbiased preference
learning (May et al., 2019; Nadeem et al., 2021;
Silva et al., 2021). While this problem is certainly
not unique to FedPerC, it is important to consider
the safety and fairness implications of improved
language generation, and future work must address
biases inherent to large language models (Schick
et al., 2021; Ravfogel et al., 2020). Another impor-
tant ethical consideration is the potential misuse of
our generative modelling approach for malicious
impersonation. In our federated setup, personal em-
beddings would be kept on-device, meaning that an
individual’s style is not accessible to others. How-
ever, this does not prevent users from manually
impersonating other individuals (e.g., celebrities).
Future work must explore additional mechanisms
for the prevention of misuse at all stages of the per-
sonalization pipeline, including protections against
impersonation of other individuals.
6
","safety, fairness, bias, misuse, impersonation",,"safety, fairness, bias, misuse, impersonation",2023,"concerns, suggestions"
1551,"In their study, Joshi et al. (2020) showed the re-
source disparity between low-resource and high-
resource languages, and (Ruder, 2020) also high-
lighted the necessity of working with low-resource
languages. However, creating representative and
inclusive corpora is a difficult task and an ongoing
process and is not always possible for many low-
resource languages. Thus to inclusively advance
the state of NLP across languages, it is crucial to
develop techniques for training MLLMs that can ex-
tract the most out of existing multilingual corpora.
Hence, we believe our analysis might help MLLMS
with low-resource languages in real-world appli-
cations. However, there is one ethical issue that
we want to state explicitly. Even though we pre-
train on a comparatively large multilingual corpus,
the model may exhibit harmful gender, ethnic and
political bias. If the model is fine-tuned on a task
where these issues are important, it is necessary
to take special consideration when relying on the
model’s decisions.
",bias,,bias,2023,"statement, concerns, advice"
1552,"Many language models show biases in their output
due to the data used to train them (Liang et al.,
2021). It is possible that these biases could affect
the RE results that we present here (e.g., producing
poor performance for certain kinds of relations, or
for entities with names different from the training
data).",bias,,bias,2023,concerns
1553,"We did not identify any potential ethical issues with
this survey.
",no ethical concerns,,no ethical concerns,2023,statement
1554,"The research presented in this paper is compatible
with the ACL ethics policy. The datasets used come
from the Universal Dependencies repository and
have appropriate licenses and documentation. The
experiments are done with small-scale models that
do not have a significant impact in terms of energy
consumption.
",no ethical concerns,,no ethical concerns,2023,statement
1555,"No conflicts of interest are declared by the au-
thors. Clinical data in the MIMIC-III database
is de-identified through removal of identifying
data elements and date-shifting in accordance with
Health Insurance Portability and Accountability
Act (HIPAA) standards, and protected health infor-
mation was further removed from clinical notes
using dictionary look-ups and pattern-matching
(Johnson et al., 2016). The use of the data is in ac-
cordance with the MIMIC-III data use agreement.
",no ethical concerns,,no ethical concerns,2023,statement
1556,"The authors of this paper are committed to conduct-
ing research ethically. Data used in this work has
been collected from public sources and used in ac-
cordance with all applicable laws and regulations.
The only area of work that involves human anno-
tation of data is described in Section 4.4, where
authors of this paper annotated a group of samples
for analyzing models’ behaviors. We ensure that
no external human subject was involved or harmed.
In addition, this work uses language models, forwhich the risks and potential harms are discussed
in numerous previous works (Bender et al., 2021;
Weidinger et al., 2021). The authors strive to en-
sure that the research and its results do not cause
harm.
9
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1557,"The authors of this paper acknowledge the signifi-
cance of responsible NLP in research and develop-
ment. The objective of this research is to enhance
the capabilities of visual question answering mod-
els, guided by human values-based world knowl-
edge. We strive to ensure that the model is not only
accurate and efficient, but also fair and unbiased.
We recognize that the VQA technology may have
a substantial impact on society and pledge to be
transparent in sharing our findings and progress
with relevant users and stakeholders.
",no ethical concerns,,no ethical concerns,2023,statement
1558,"We introduced a novel model architecture for
schema-guided dialogue state tracking which lever-
ages a natural language schema and a span pointer
module to achieve higher accuracy in dialogue state
tracking. All experiments were conducted on pub-
licly available datasets which are commonly used
in research on dialogue systems.
",no ethical concerns,,no ethical concerns,2023,statement
1559,"Our framework in general does not create direct
societal consequences and is intended to be used to
defend against fake news videos. It can be easily
combined into fake news video detection systems,
especially when the events have multiple related
news videos and debunking videos. To the best
of our knowledge, no code of ethics was violated
throughout the experiments done in this article. Ex-
periments are conducted on the publicly available
dataset and have no issues with user privacy.
",no ethical concerns,,no ethical concerns,2023,statement
1560,"The authors of this paper are committed to conduct-
ing research ethically. Data used in this work have
been collected from public sources and used in ac-
cordance with all applicable laws and regulations.
This work uses language models, for which the
risks and potential harms are discussed in numer-
ous previous works (Bender et al., 2021; Weidinger
et al., 2021). The authors strive to ensure that the
research and its results do not cause harm.",no ethical concerns,,no ethical concerns,2023,statement
1561,"We do not foresee ethical issues with this work.
",no ethical concerns,,no ethical concerns,2023,statement
1562,"Aside from the ethical concerns regarding voice
cloning (covered in e.g. YourTTS (Casanova et al.,
2021)), deployment would require a detailed eval-
uation of risk of de-identiﬁcation. It is believed
that the ﬁnal conﬁdence and the accuracy of each
step can be combined to signiﬁcantly reduce this
risk. The voice itself also offers options for iden-
tiﬁcation: the value of yielding substitutions in
the original speaker’s voice (and keeping the origi-
nal prosody) would need to be weighed up against
approaches which anonymize voice but preserve
prosodic information.
",misuse,,misuse,2023,"concerns, actions"
1563,"Aside from the ethical concerns regarding voice
cloning (covered in e.g. YourTTS (Casanova et al.,
2021)), deployment would require a detailed eval-
uation of risk of de-identiﬁcation. It is believed
that the ﬁnal conﬁdence and the accuracy of each
step can be combined to signiﬁcantly reduce this
risk. The voice itself also offers options for iden-
tiﬁcation: the value of yielding substitutions in
the original speaker’s voice (and keeping the origi-
nal prosody) would need to be weighed up against
approaches which anonymize voice but preserve
prosodic information.
",misuse,,misuse,2023,"concerns, actions"
1564,"As we use a publicly available Wikipedia index as
our knowledge base, it is possible that the gener-
ated answers may contain some form of bias that re-
flects the information submitted by the anonymous
editors of the website. To prevent this, follow-up
work could examine how to detect misinformation
or hate speech in indexed passages, before using
them as evidence for the generative models.
",bias,,bias,2023,"concerns, suggestions"
1565,"All data sets used in this paper are cited. The New
York Times data set3is licensed under ""The New
York Times Annotated Corpus Agreement""4. The
2https://huggingface.co/sentence-transformers/stsb-
roberta-large
3https://catalog.ldc.upenn.edu/LDC2008T19
4https://catalog.ldc.upenn.edu/license/the-new-york-
times-annotated-corpus-ldc2008t19.pdfTweets2011 corpus5is available under the ""TREC
2011 Microblog Dataset Usage Agreement""6which
additionally requires following the ""Twitter terms
of service""7. All other data sets are obtained from
the recent literature. No sensitive information is
used or inferred in this paper. The risk of harm
from our model is low. Any artifacts in this paper
are used following their intended use cases.
Acknolwedgements
We would like to thank Federico Bianchi for as-
sistance in finding data sets. We would like to
thank the creators and maintainers of Python and
the following Python packages: lda, torch, numpy,
biterm, scipy, gensim, tqdm, transformers, nltk,
sentence_transformers, sklearn, and pandas. We
would like to thank the following GitHub users
for code inspiration: maifeng, smutahoang, dice-
group, shion-h, karpathy, estebandito22, akashgit,
and MilaNLProc.
",no ethical concerns,,no ethical concerns,2023,statement
1566,"Like all works that depend on embeddings, the
resulting models may be biased in various ways.
Users should take this into consideration when de-
ploying them in products.
",bias,,bias,2023,"concerns, advice"
1567,"Data Sources. All the datasets and corpora used
in this work are publicly available. The cleaned ver-
sion of the NCI dataset is based on the existing Se-
mEval 2013 dataset (Hendrickx et al., 2013). The
NCs for the new NCC test set were taken from an-
other publicly-available dataset (Dhar and van der
Plas, 2019), based on frequencies in the Google
Ngram corpus (Brants, 2006). To quantify Ngram
overlap, we used the Allen AI version of the C4 cor-
pus (Raffel et al., 2020; Dodge et al., 2021) made
available by the HuggingFace Datasets package.5
Data Collection. We performed human evalua-
tion using Amazon Mechanical Turk. We made
sure annotators were fairly compensated by com-
puting an average hourly wage of $15, which is
well above the US minimum wage. We did not
collect any personal information from annotators.
Models. The models presented in this paper are
for a low-level NLP task rather than for an appli-
5https://huggingface.co/datasets/c
cation with which users are expected to interact di-
rectly. The generative models are based on PLMs,
which may generate offensive content if prompted
with certain inputs.","bias, offensive content, toxic content",,"bias, offensive-toxic content",2023,"statement, actions"
1568,"We guarantee that the approach in our research is
original and that the experimental results of the
SGT model are reproducible. All the experimental
data of the baseline model can be reproduced from
the relevant open-source code or found in the cited
paper. Finally, The list of authors of the submis-
sions does not include any individuals who did not
contribute substantially to work submitted.
",no ethical concerns,,no ethical concerns,2023,statement
1569,"We believe our work has no potential risks or nega-
tive social impacts now.
",no ethical concerns,,no ethical concerns,2023,statement
1570,"Because our work uses synthetic data, it has little
immediate ethical impact. However, our work may
enable large populations of communicating agents
down the line, which could have a range of civilian
or military purposes.
",no ethical concerns,,no ethical concerns,2023,statement
1571,"Our work complies with the ACL Ethics Policy.
Beyond this, we are not aware of any way in which
the results of this study may be harmful—in fact,
if anything, identifying the limitations of large lan-
guage models is something that is likely to reduce
possible harms by demonstrating cases where their
use is not suitable.
From an environmental perspective, we did not
train any models; we only used pretrained models
for analysis, limiting energy consumption. With the
exception of the GPT-3 and InstructGPT models
and OPT 13B, all analyses were run on an NVIDIA
RTX A6000 GPU, taking a total of 43 minutes.
OPT 13B was too large to run on this GPU, and
thus was run on an Intel Dual Xeon E7-4870 CPU
for a total of 22 hours and 39 minutes. Finally, the
GPT-3 and the InstructGPT models were run using
the OpenAI API, and thus we do not have access
to information about the GPUs used.
",no ethical concerns,,no ethical concerns,2023,"statement, actions"
1572,"The methods presented in this work aim to explain
language models, and can as such present ethical is-
sues related to this task. Discriminating biases can
indeed be present in text data on which a language
model is trained, and such a model can acquire
and propagate these biases (Sap et al., 2019). As
the presented methods aim to explain a language
model without additional knowledge, these meth-
ods could also display discriminating biases learnt
by a language model.
Moreover, common explanation methods such
as Integrated Gradients has proved to be prone to
adversarial attacks (Dombrowski et al., 2019), and
can be misleading when used on out of sample data
(Slack et al., 2021). There is no reason to believe
our methods would be more robust compared toexisting methods such as IG.
The proposed methods can also be characterised
as gradient-based, as they rely on computing gradi-
ents on the input data, an uninformative baseline,
or on interpolated points between them. As noted
by (Mittelstadt et al., 2019), such methods are only
local and may not give a clear explanation of the
model globally.
",bias,,bias,2023,concerns
1573,"We have read and compiled with the ACL Code
of Ethics. The proposed FormNetV2 follows the
prevailing large-scale pre-training then ﬁne-tuning
framework. Although we use the standard IIT-CDIP dataset for pre-training in all experiments,
the proposed method is not limited to using speciﬁc
datasets for pre-training. Therefore, it shares the
same potential concerns of existing large language
models, such as biases from the pre-training data
and privacy considerations. We suggest following
a rigorous and careful protocol when preparing the
pre-training data for public-facing applications.
","bias, privacy",,"bias, privacy",2023,"statement, concerns, advice"
1574,"The empirical experiments in this work involve the
removal of binary gender information from a pre-
trained representation. Beyond the fact that gender
is a non-binary concept, this task may have real-
world applications, in particular such that relate to
fairness. We would thus like to remind the readers
to take the results with a grain of salt and be extra
careful when attempting to deploy methods such
as the one discussed here. Regardless of any theo-
retical result, care should be taken to measure the
effectiveness of bias mitigation efforts in the con-
text in which they are to be deployed, considering,
among other things, the exact data to be used and
the exact fairness metrics under consideration.
","bias, fairness",,"bias, fairness",2023,"statement, concerns, advice"
1575,"This study investigates perception of humans on
adversarial examples, which are modified texts that
aim to change the decision of a NLP model. While
these examples can be used by malicious actors, our
goal is to understand the threat they bring and take
informed decisions on preparing effective defences
against these threats.
The texts shown to participants of this study
were collected from open platforms, and it may
contain inappropriate language. To mitigate this
issue, we asked only participants 18, years old to
take the survey.",misuse,,misuse,2023,"statement, concerns"
1576,"Student identities are completely anonymized in
our analyses and in the data we feed to our models.
By locally distinguishing individual students, we
do not wish to single out, over-interpret, or judge
any individual student’s behavior or aptitude, but
rather to fit the models to our data as best we can
and also to control for spurious patterns that might
have been missed during initial outlier-filtering.
",no ethical concerns,,no ethical concerns,2023,statement
1577,"MA-GNN focuses on improving the performance
of link prediction and has no particular ethical con-
sideration.
",no ethical concerns,,no ethical concerns,2023,statement
1578,"It is a computationally demanding task to pre-train
large language models. However, transfer learning
opens the possibility to fine-tune our pre-trained
models, which showed strong performances, in a
reasonable amount of time.
The texts utilized for pre-training the models
may well exhibit biases related to ancient perspec-
tives of the world. We do not view this as an issue,
as the proposed language models for historical lan-
guages are intended for academic use and do not
have practical, everyday applications.
",bias,,bias,2023,statement
1579,"The Consequences of Incorrect Captions. Wei-
dinger et al. (2021) comprehensively survey the
risks associated with the large language models
(LLMs) that underlie our contribution. Of the six
categories of risk they identify, harms stemming
from models producing factually incorrect state-
ments are not only most pertinent to our work, but
are likely heighted as compared to general uses of
LLMs given the context we are addressing: auto-
matically captioning charts. In particular, people
most often consume charts and visualizations in
order to make data-driven decisions (Keim et al.,
2008) — for instance, about whether to evacuate
ahead of a hurricane (Padilla et al., 2018), or health
& safety during the pandemic (Shneiderman, 2020).
Moreover, recent results have shown that readersnot only fixate for longer and are more likely to
recall the textual content of and around visualiza-
tions (Borkin et al., 2015) but this textual content
can strongly influence the takeaway message read-
ers leave with even when it is at odds with the
depicted data (Kong et al., 2018, 2019). Finally,
these issues are exacerbated by the persuasive and
rhetorical force of data and charts (Kennedy et al.,
2016; Hullman and Diakopoulos, 2011), that often
project a sense of authority and certainty (Correll,
2019). As a result, readers may not think to double
check the accuracy of chart captions, and inaccu-
rate statements that models may produce could lead
to harmful downstream decisions.
To proceed ethically with this line of research,
we believe that advances in data and modeling need
to be closely followed by attention devoted to miti-
gating the risks of incorrect statements. At base, au-
tomatically generated captions should be identified
as such at the forefront to raise readers’ awareness
about the potential for incorrect statements. And,
interactive visual linking strategies (such as those
explored by Kong and Agrawala (2012); Kim et al.
(2018)) could be deployed to help readers manu-
ally verify the constituent statements of a caption
against the chart. These strategies, however, place
the burden of harm mitigation on readers. Thus,
an alternate approach might never surface automat-
ically generated captions to readers directly but
instead use them as part of mixed-initiative sys-
tems for jointly authoring visualization and text,
such as Kori (Latif et al., 2021). In such systems,
automated chart captioning models would help to
accelerate the authoring process — combatting the
blank slate problem by providing an initial sum-
mary of the chart — and chart authors would make
any necessary corrections prior to publication.
Besides these human-computer interaction (HCI)
approaches for mitigating harm, an equally impor-
tant direction for future work should leverage inter-
pretability techniques to more deeply study what
the models are learning. To what degree are chart
captioning models stochastic parrots (Bender et al.,
2021), and how much do they understand the infor-
mation charts depict?
Automated Captioning for Accessibility. Al-
though accessibility is a guiding motivation for
the bulk of work in automated captioning (be it im-
age captioning or, as in our case, chart captioning),
studies find mixed reactions, at best, about these
approaches among people with disabilities (PWDs).
For instance, accessibility educator and researcher
Chancey Fleet described Facebook’s automatic im-
age descriptions as “famously useless in the Blind
community” despite “garner[ing] a ton of glow-
ing reviews from mainstream outlets” (Fleet, 2021;
Hanley et al., 2021). This disconnect appears to
stem from a more fundamental mismatch between
what PWDs describe as their captioning needs,
and what the research community — particularly
through its automatic, quantitative evaluations —
prioritizes (Jandrey et al., 2021). In particular,
surveys with PWDs repeatedly surface the con-
textual nature of captions. Bennett et al. (2021)
find that the context of use shapes the degree to
which PWD are comfortable with captions describ-
ing people’s race, gender, and disabilities — for
instance, changing their preferences if they were in
a white, cisgender, nondisabled, and professional
company versus their own community. Similarly,
Jung et al. (2022) find shifting preferences for the
content image descriptions should convey across
different photo activites — for example, when view-
ing or taking photos, participants wished for de-
scriptions that conveyed spatial cues whereas when
searching or reminiscing about photos, participants
hoped for descriptions to connect to personal data
or differentiating details.
In contrast, quantitative metrics of model per-
formance compare generated captions to a single
“ground truth” caption. This framing of success
not only makes it difficult to develop contextually-
varying caption generation but can actively penal-
ize such investigations. For instance, with our work,
we explored how prefix-tuning can be used to de-
velop models that are responsive to users’ pref-
erences about semantic content. However, as de-
scribed in Sec. 5.1, existing quantitative metrics of
model performance (e.g., BLEU, ROUGE, WMD,
and TER) show a drop in model performance de-
spite our qualitative analysis indicating that these
captions are indeed high quality.
Finally, our exploration of semantic prefix-
tuning represents only a very preliminary step to-
wards addressing the contextual captioning needs
of PWDs. In particular, the semantic labels Vis-
Text assigns to captions were derived from prior
work (Lundgard and Satyanarayan, 2022) that only
explored natural language descriptions when con-
suming presentations of visualizations — one task
from a broader palette (Brehmer and Munzner,
2013). Future work might instead extend the Vis-Text dataset — and corresponding models — to con-
sider captions for a broader range of tasks including
consuming visualizations for scientific discovery,
enjoyment or, producing, searching, or querying
visualizations (Brehmer and Munzner, 2013).","faithfulness, misinformation, inaccuracies",,"faithfulness, misinformation, inaccuracies",2023,"statement, concerns, actions, advice, suggestions"
1580,"We have extensively discussed the limitations of
our work in the previous section. We use two exist-
ing datasets, SST (Socher et al., 2013) and IMDB
(Maas et al., 2011), which are publicly available
and commonly used in NLP research. We syntheti-
cally generate datasets of formal languages which
does not require ethical consideration. We have
discussed the experimental details and computa-
tional budget in detail in Appendix G. The research
presented in this paper focuses on analysing the in-
ductive biases of Transformers and LSTMs basedon experiments on formal languages and subse-
quently we believe that our work does not raise any
ethical concerns.
",no ethical concerns,,no ethical concerns,2023,statement